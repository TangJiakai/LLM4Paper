### LLM4Rec-Paper

ä»¥LLM4Recæ–¹å‘ä¸ºä¸»ï¼Œéµå¾ª[How Can Recommender Systems Benefit from Large Language Models: A Survey](https://arxiv.org/pdf/2306.05817v5.pdf)çš„åˆ†ç±»æ ‡å‡†

ç›®å½•å¦‚ä¸‹

1. [LLM &amp; RS](#1)

   ğŸ”·  1.1 [LLM for Feature Engineering](#1.1)

   ğŸ”¸ 1.1.1[User- and Item-level Feature Augmentation](#1.1.1)

   ğŸ”¸ 1.1.2[Instance-level Sample Generation](#1.1.2)

   

   ğŸ”· 1.2 [LLM as Feature Encoder](#1.2)

   ğŸ”¸ 1.2.1[Representation Enhancement](#1.2.1)

   ğŸ”¸ 1.2.2[Unified Cross-domain Recommendation](#1.2.2)

   

   ğŸ”· 1.3 [LLM as Scoring/Ranking Function](#1.3)

   ğŸ”¸ 1.3.1[Item Scoring Task](#1.3.1)

   ğŸ”¸ 1.3.2[Item Generation Task](#1.3.2)

   ğŸ”¸ 1.3.3[Hybrid Task](#1.3.3)

   

   ğŸ”· 1.4 [LLM for User Interaction](#1.4)
   
   ğŸ”¸ 1.4.1[Task-oriented User Interaction](#1.4.1)
   
   ğŸ”¸ 1.4.2[Open-ended User Interaction](#1.4.2)
   
   
   
   ğŸ”· 1.5 [LLM for RS Pipeline Controller](#1.5)
2. [LLM &amp; Graph](#2)
3. [Datasets &amp; Benchmarks](#3)

   ğŸ”· 3.1 [Datasets](#3.1)

   ğŸ”· 3.2 [Benchmarks](#3.2)
4. [Related Repositories](#4)

<h2 id="1">1. LLM & RS</h2>

å¼•å…¥LLMåˆ°æ¨èä¸­çš„å¸¸ç”¨åŠ¨æœºï¼š

- LLMèƒ½ä»¥promptæ–¹å¼ç»Ÿä¸€å„ç§ä¸‹æ¸¸æ¨èä»»åŠ¡
- LLMå°†å„æ¨¡æ€ã€å„ç‰¹å¾ç»Ÿä¸€ä»¥æ–‡æœ¬å‘ˆç°ï¼Œç¼“è§£ä¸åŒæ¨¡æ€/ç‰¹å¾å¼‚è´¨æ€§é—®é¢˜
- LLMå…·æœ‰å¼ºå¤§çš„è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œèƒ½æ›´å¥½çš„æ•è·ç”¨æˆ·çš„åå¥½
- LLMç›¸æ¯”äºä¼ ç»Ÿæ¨èç®—æ³•ï¼Œå…·æœ‰æ›´å¥½çš„å†·å¯åŠ¨å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºæ–‡æœ¬ç‰¹å¾æ˜¯å„ç”¨æˆ·ã€ç‰©å“ã€é¢†åŸŸæ‰€å…±äº«çš„
- åªæœ‰IDä¼šç¼ºä¹ä¸–ç•ŒçŸ¥è¯†ï¼Œåªæœ‰æ–‡æœ¬ä¼šç¼ºä¹ç†è§£æ¨èååŒ/åºåˆ—äº¤äº’æ¨¡å¼ï¼Œç»“åˆäºŒè€…ï¼ˆå¯è§†ä¸ºå¤šä¸ªæ¨¡æ€ï¼‰æ‰èƒ½å……åˆ†å‘æŒ¥ä¸–ç•ŒçŸ¥è¯†å’Œè¡Œä¸ºçŸ¥è¯†çš„ä¼˜åŠ¿

![](where-framework-1.png)

<h3 id="1.1">1.1 LLM for Feature Engineering</h3>


<h4 id="1.1.1">1.1.1 User- and Item-level Feature Augmentation</h4>

| **Name** | **Paper**                                                                                                          | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |                **Link**                | Main Contributions                                                                                                                                                                                                                                                                                                                                                             |
| :------------: | :----------------------------------------------------------------------------------------------------------------------- | :------------------------------: | :----------------------------: | :-------------------: | :------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
|    LLM4KGC    | Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs |       PaLM (540B)/ ChatGPT       |             Frozen             |      Arxiv 2023      |  [[Paper]](https://arxiv.org/abs/2305.09858v1)  |                                                                                                                                                                                                                                                                                                                                                                                |
|     TagGPT     | TagGPT: Large Language Models are Zero-shot Multimodal Taggers                                                           |             ChatGPT             |             Frozen             |      Arxiv 2023      |  [[Paper]](https://arxiv.org/abs/2304.03022v1)  |                                                                                                                                                                                                                                                                                                                                                                                |
|      ICPC      | Large Language Models for User Interest Journeys                                                                         |           LaMDA (137B)           | Full Finetuning/ Prompt Tuning |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2305.15498)   |                                                                                                                                                                                                                                                                                                                                                                                |
|      KAR      | Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models                                 |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2306.10933)   |                                                                                                                                                                                                                                                                                                                                                                                |
|      PIE      | Product Information Extraction using ChatGPT                                                                             |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2306.14921)   |                                                                                                                                                                                                                                                                                                                                                                                |
|      LGIR      | Enhancing Job Recommendation through LLM-based Generative Adversarial Networks                                           |           GhatGLM (6B)           |             Frozen             |       AAAI 2024       |   [[Paper]](https://arxiv.org/abs/2307.10747)   |                                                                                                                                                                                                                                                                                                                                                                                |
|      GIRL      | Generative Job Recommendations with Large Language Model                                                                 |            BELLE (7B)            |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2307.02157)   |                                                                                                                                                                                                                                                                                                                                                                                |
|    LLM-Rec    | LLM-Rec: Personalized Recommendation via Prompting Large Language Models                                                 |         text-davinci-003         |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2307.15780)   |                                                                                                                                                                                                                                                                                                                                                                                |
|     HKFRâœ…     | Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM                                 |    ChatGPT+<br />ChatGLM (6B)    |              LoRA              |      RecSys 2023      |   [[Paper]](https://arxiv.org/abs/2308.03333)   | ä¼ ç»Ÿæ–¹æ³•å°†ç”¨æˆ·å¼‚è´¨ä¿¡æ¯ä¸æ¨¡å‹ç»“åˆï¼Œä¼šå‡ºç°ç‰¹å¾ç¨€ç–å’ŒçŸ¥è¯†ç¢ç‰‡åŒ–ï¼ˆç¼ºä¹ä¸åŒè¡Œä¸ºä¹‹é—´å¼‚è´¨çŸ¥è¯†çš„èåˆï¼‰ã€‚æœ¬æ–‡é€šè¿‡LLMå¸®åŠ©æå–ç”¨æˆ·ä¸åŒè¡Œä¸ºä¸­çš„å¼‚è´¨çŸ¥è¯†ï¼Œå¹¶ä½¿ç”¨instruct tuningçš„æ–¹å¼èåˆè¿™äº›çŸ¥è¯†å’Œæ¨èä»»åŠ¡ã€‚                                                                                                                                                                               |
|    LLaMA-E    | LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following                                         |           LLaMA (30B)           |              LoRA              |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2308.04913)   |                                                                                                                                                                                                                                                                                                                                                                                |
|    EcomGPT    | EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce                                |          BLOOMZ (7.1B)          |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2308.06966)   |                                                                                                                                                                                                                                                                                                                                                                                |
|    TF-DCon    | Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation   |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2310.09874)   |                                                                                                                                                                                                                                                                                                                                                                                |
|    RLMRecâœ…    | Representation Learning with Large Language Models for Recommendation                                                    |             ChatGPT             |             Frozen             |       WWW 2024       |    [[Code]](https://github.com/HKUDS/RLMRec)    | æ¨èéšå¼æ•°æ®åŒ…å«å™ªå£°å’Œåå·®ï¼›ä½¿ç”¨LLMæ¨èå­˜åœ¨æ‰©å±•æ€§ä¸è¶³ã€è¾“å…¥é•¿åº¦é™åˆ¶ç­‰ä¸è¶³ã€‚æœ¬æ–‡ç›®çš„æ˜¯é€šè¿‡LLMå¼ºå¤§è¯­è¨€ç†è§£èƒ½åŠ›æŒ–æ˜ç”¨æˆ·è¡Œä¸ºå’Œåå¥½ï¼ŒåŒæ—¶ä¿ç•™å·²æœ‰æ¨èå™¨çš„æ•ˆç‡å’Œå‡†ç¡®åº¦ã€‚è´¡çŒ®æœ‰1. åŸºäºæ¨ç†çš„ç”¨æˆ·å’Œç‰©å“æç¤ºç”Ÿæˆï¼›2. æå‡ºå¯¹æ¯”å¼å’Œæ©ç ç”Ÿæˆå¼ä¸¤ä¸ªæ–¹æ³•ï¼›3.ä»äº’ä¿¡æ¯è§’åº¦åˆ†æå¼•å…¥æ–‡æœ¬ä¿¡å·å¯¹äºæå‡è¡¨ç¤ºè´¨é‡çš„æœ‰æ•ˆæ€§                                                                             |
|    LLMRecâœ…    | LLMRec: Large Language Models with Graph Augmentation for Recommendation                                                 |             ChatGPT             |             Frozen             |       WSDM 2024       |    [[Code]](https://github.com/HKUDS/LLMRec)    | æ•°æ®ç¨€ç–å¯ä»¥é€šè¿‡side informationè§£å†³ï¼Œä½†æ˜¯è¿™äº›ä¿¡æ¯ä¹Ÿä¼šåŒ…å«å™ªå£°ã€å¯ç”¨æ€§ä½å’Œæ•°æ®è´¨é‡ä½ç­‰é—®é¢˜ï¼Œåè¿‡æ¥ä¼¤å®³äº†æ¨èå‡†ç¡®ç‡ã€‚æœ¬æ–‡æå‡ºæ–¹æ³•ä»ç”¨æˆ·-ç‰©å“äº¤äº’è¾¹ã€ç”¨æˆ·ç”»åƒã€ç‰©å“èŠ‚ç‚¹å±æ€§ä¸‰è§’åº¦å¢å¼ºæ•°æ®ï¼›å¹¶æå‡ºå¯¹å¢å¼ºçš„äº¤äº’å’Œç‰¹å¾åˆ†åˆ«è®¾è®¡å‰ªæã€MAEæ–¹æ³•å»å™ªã€‚                                                                                                                                   |
|     LLMRG     | Enhancing Recommender Systems with Large Language Model Reasoning Graphs                                                 |               GPT4               |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2308.10835)   |                                                                                                                                                                                                                                                                                                                                                                                |
|      CUP      | Recommendations by Concise User Profiles from Review Text                                                                |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2311.01314)   |                                                                                                                                                                                                                                                                                                                                                                                |
|     SINGLE     | Modeling User Viewing Flow using Large Language Models for Article Recommendation                                        |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2311.07619)   |                                                                                                                                                                                                                                                                                                                                                                                |
|     SAGCN     | Understanding Before Recommendation: Semantic Aspect-Aware Review Exploitation via Large Language Models                 |           Vicuna (13B)           |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2312.16275)   |                                                                                                                                                                                                                                                                                                                                                                                |
|      UEM      | User Embedding Model for Personalized Language Prompting                                                                 |       FLAN-T5-base (250M)       |        Full Finetuning        |      Arxiv 2024      |   [[Paper]](https://arxiv.org/abs/2401.04858)   |                                                                                                                                                                                                                                                                                                                                                                                |
|     LLMHG     | LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation                                   |               GPT4               |             Frozen             |      Arxiv 2024      |   [[Paper]](https://arxiv.org/abs/2401.08217)   |                                                                                                                                                                                                                                                                                                                                                                                |
|   Llama4Rec   | Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation                   |           LLaMA2 (7B)           |        Full Finetuning        |      Arxiv 2024      |   [[Paper]](https://arxiv.org/abs/2401.13870)   |                                                                                                                                                                                                                                                                                                                                                                                |
|    LLMHGâœ…    | LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation                                   |               GPT4               |             Frozen             |      Arxiv 2024      | [[Paper]](https://arxiv.org/pdf/2401.08217.pdf) | ä»…ä¾èµ–äº¤äº’å†å²ä¸èƒ½å®Œå…¨æ•æ‰äººç±»å…´è¶£çš„å¤šé¢åå¥½ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨LLMä»ç”¨æˆ·åºåˆ—ä¸­æ¨å¯¼å‡ºå¤šä¸ªInterest Angles(IAs)ï¼Œå†åŸºäºIAå°†ç‰©å“åˆ’åˆ†å¤šä¸ªç»„ï¼ˆå½¢æˆè¶…è¾¹ï¼‰ã€‚æ­¤å¤–ï¼Œç”±äºLLMå¯èƒ½æ— æ³•è€ƒè™‘åˆ°æ‰€æœ‰ç‰©å“çš„ç‰¹æ€§ï¼Œä»¥åŠç”±äºæ¦‚ç‡æ¨ç†çš„æœ¬è´¨ï¼Œå¯èƒ½äº§ç”Ÿä¸æ­£ç¡®çš„æ¨ç†å¯¼è‡´æ¬¡ä¼˜çš„IAæå–ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥è®¾è®¡åå¤„ç†æ–¹æ³•ï¼Œåˆ©ç”¨è¶…è¾¹å†…å®ä½“é è¿‘å’Œè¶…è¾¹ä¹‹é—´æ‹‰è¿œçš„æ–¹æ³•è¿›è¡Œè¶…å›¾ç»“æ„åŒ–å­¦ä¹ ã€‚æœ€åç»“åˆè¶…å›¾ç¥ç»ç½‘ç»œç®—æ³•é¢„æµ‹åˆ†æ•°ã€‚ |

<h4 id="1.1.2">1.1.2 Instance-level Sample Generation</h4>

| **Name** | **Paper**                                                                                                           |   **LLM Backbone (Largest)**   | **LLM Tuning Strategy** | **Publication** |              **Link**              | Main Contributions                                                                                                                                                                                                                                                                      |
| :------------: | :------------------------------------------------------------------------------------------------------------------------ | :----------------------------------: | :---------------------------: | :-------------------: | :--------------------------------------: | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|     GReaT     | Language Models are Realistic Tabular Data Generators                                                                     |          GPT2-medium (355M)          |        Full Finetuning        |       ICLR 2023       | [[Paper]](https://arxiv.org/abs/2210.06280) |                                                                                                                                                                                                                                                                                         |
|     ONCEâœ…     | ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models                       | ChatGPT+<br />LLaMA2(7B)/LLaMA2(13B) |             LoRA             |       WSDM 2024       |   [[Code]](https://github.com/Jyonn/ONCE)   | ä¼ ç»Ÿcontent-basedæ¨èç®—æ³•åœç•™åœ¨æ•æ‰wordçº§åˆ«çš„ç›¸ä¼¼åº¦ï¼Œè€Œæ— æ³•å®ç°contentçº§åˆ«ã€‚æœ¬æ–‡æå‡ºåŒæ—¶åˆ©ç”¨å¼€æºå’Œé—­æºçš„LLMå¸®åŠ©å¢å¼ºå¯¹ç‰©å“å’Œç”¨æˆ·ä¿¡æ¯å†…å®¹çš„ç†è§£ã€‚å¼€æºLLMç”¨äºç¼–ç ç”¨æˆ·å’Œç‰©å“è¡¨ç¤ºï¼›é—­æºLLMç”¨äºç‰©å“å†…å®¹ä¸°å¯Œã€ç”¨æˆ·å…´è¶£æŠ½è±¡ã€ï¼ˆå†·å¯åŠ¨ï¼‰ç”¨æˆ·å†…å®¹ç”Ÿæˆã€‚å¹¶ä½¿ç”¨LoRAã€å†»ç»“ä½å±‚ï¼Œç¼“å­˜æœºåˆ¶ç­‰æé«˜æ•ˆç‡ã€‚ONCEæ–¹æ³•ç»“åˆå¼€æºå’Œé—­æºLLMçš„æ–¹æ³• |
|   AnyPredict   | AnyPredict: Foundation Model for Tabular Prediction                                                                       |               ChatGPT               |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2305.12081) |                                                                                                                                                                                                                                                                                         |
|     DPLLM     | Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models |              T5-XL (3B)              |        Full Finetuning        |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2305.05973) |                                                                                                                                                                                                                                                                                         |
|      MINT      | Large Language Model Augmented Narrative Driven Recommendations                                                           |           text-davinci-003           |            Frozen            |      RecSys 2023      | [[Paper]](https://arxiv.org/abs/2306.02250) |                                                                                                                                                                                                                                                                                         |
|   Agent4Rec   | On Generative Agents in Recommendation                                                                                    |               ChatGPT               |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2310.10108) |                                                                                                                                                                                                                                                                                         |
|   RecPrompt   | RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models                                  |                 GPT4                 |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2312.10463) |                                                                                                                                                                                                                                                                                         |
|     BEQUE     | Large Language Model based Long-tail Query Rewriting in Taobao Search                                                     |             ChatGLM (6B)             |        Full Finetuning        |      WWW 2024      | [[Paper]](https://arxiv.org/abs/2311.03758) |                                                                                                                                                                                                                                                                                         |
| Agent4Ranking | Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM                             |               ChatGPT               |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2312.15450) |                                                                                                                                                                                                                                                                                         |
| OpenGraphâœ… | OpenGraph: Towards Open Graph Foundation Models | ChatGPT | Frozen | Arxiv 2024 | [[Code]](https://github.com/HKUDS/OpenGraph/tree/main) | OpenGraphç®—æ³•æä¾›ä¸€ä¸ªé€šç”¨ä¸”å¯æ‰©å±•çš„å›¾æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°é›¶æ ·æœ¬å­¦ä¹ å¹¶é€‚åº”ä¸åŒé¢†åŸŸçš„å›¾ç»“æ„ã€‚å—é™æå‡ºä¸€ä¸ªé€šç”¨tokenizerå°†è¾“å…¥å›¾å˜æ¢ä¸ºä¸€ä¸ªç»Ÿä¸€tokenåºåˆ—ï¼Œä½¿å¾—èƒ½å¤Ÿåœ¨ä¸åŒå›¾ä¸Šè¿›è¡Œè‰¯å¥½çš„æ³›åŒ–ï¼›è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„å›¾transformerä½œä¸ºencoderï¼Œæ•æ‰å…¨å±€æ‹“æ‰‘ä¸Šä¸‹æ–‡ä¸­çš„èŠ‚ç‚¹çš„ä¾èµ–å…³ç³»ï¼›å¼•å…¥LLMçš„æ•°æ®å¢å¼ºæœºåˆ¶ï¼Œä½¿ç”¨Tree-of-Promptè¿­ä»£åœ°å°†ä¸€èˆ¬èŠ‚ç‚¹åˆ’åˆ†ä¸ºå…·æœ‰æ›´ç²¾ç»†è¯­ä¹‰ç²’åº¦çš„å­ç±»åˆ«å®ä½“ä½œä¸ºå¶å­ç»“ç‚¹ï¼Œå¹¶åˆ©ç”¨Bibbsé‡‡æ ·å¾—åˆ°ç”ŸæˆèŠ‚ç‚¹é›†ï¼Œä»¥åŠè®¾è®¡Node-wise connectionæ¦‚ç‡é¢„æµ‹æ–¹æ³•ï¼›å¼•å…¥äº†ä¸€ç§å°†å±€éƒ¨æ€§æ¦‚å¿µçº³å…¥è¾¹ç¼˜ç”Ÿæˆè¿‡ç¨‹çš„æ–¹æ³•é˜²æ­¢åˆ›é€ è¿‡å¤šçš„è¿æ¥ã€‚ |

<h3 id="1.2">1.2 LLM as Feature Encoder</h3>
<h4 id="1.2.1">1.2.1 Representation Enhancement</h4>


| **Name** | **Paper**                                                                                                                      | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |                                     **Link**                                     | Main Contributions                                                                                                                                                                                                                                                                                                                                                                                     |
| :------------: | :----------------------------------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :-------------------: | :------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
|     U-BERT     | U-BERT: Pre-training User Representations for Improved Recommendation                                                                |         BERT-base (110M)         |        Full Finetuning        |       AAAI 2021       |             [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16557)             |                                                                                                                                                                                                                                                                                                                                                                                                        |
|     UNBERT     | UNBERT: User-News Matching BERT for News Recommendation                                                                              |         BERT-base (110M)         |        Full Finetuning        |      IJCAI 2021      |                   [[Paper]](https://www.ijcai.org/proceedings/2021/462)                   |                                                                                                                                                                                                                                                                                                                                                                                                        |
|     PLM-NR     | Empowering News Recommendation with Pre-trained Language Models                                                                      |       RoBERTa-base (125M)       |        Full Finetuning        |      SIGIR 2021      |                        [[Paper]](https://arxiv.org/abs/2104.07413)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
| Pyramid-ERNIE | Pre-trained Language Model based Ranking in Baidu Search                                                                             |           ERNIE (110M)           |        Full Finetuning        |       KDD 2021       |                        [[Paper]](https://arxiv.org/abs/2105.11108)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|    ERNIE-RS    | Pre-trained Language Model for Web-scale Retrieval in Baidu Search                                                                   |           ERNIE (110M)           |        Full Finetuning        |       KDD 2021       |                        [[Paper]](https://arxiv.org/abs/2106.03373)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|    CTR-BERT    | CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models                                                 |      Customized BERT (1.5B)      |        Full Finetuning        |      ENLSP 2021      | [[Paper]](https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf) |                                                                                                                                                                                                                                                                                                                                                                                                        |
|      SuKD      | Learning Supplementary NLP Features for CTR Prediction in Sponsored Search                                                           |       RoBERTa-large (355M)       |        Full Finetuning        |       KDD 2022       |               [[Paper]](https://dl.acm.org/doi/abs/10.1145/3534678.3539064)               |                                                                                                                                                                                                                                                                                                                                                                                                        |
|      PREC      | Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation                                                |         BERT-base (110M)         |        Full Finetuning        |      COLING 2022      |                  [[Paper]](https://aclanthology.org/2022.coling-1.249/)                  |                                                                                                                                                                                                                                                                                                                                                                                                        |
|     MM-Rec     | MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation                                                               |         BERT-base (110M)         |        Full Finetuning        |      SIGIR 2022      |               [[Paper]](https://dl.acm.org/doi/abs/10.1145/3477495.3531896)               |                                                                                                                                                                                                                                                                                                                                                                                                        |
|  Tiny-NewsRec  | Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation                                                                  |       UniLMv2-base (110M)       |        Full Finetuning        |      EMNLP 2022      |                        [[Paper]](https://arxiv.org/abs/2112.00944)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|    PLM4Tag    | PTM4Tag: Sharpening Tag Recommendation of Stack Overflow Posts with Pre-trained Models                                               |         CodeBERT (125M)         |        Full Finetuning        |       ICPC 2022       |                        [[Paper]](https://arxiv.org/abs/2203.10965)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|   TwHIN-BERT   | TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations                                    |         BERT-base (110M)         |        Full Finetuning        |      Arxiv 2022      |                        [[Paper]](https://arxiv.org/abs/2209.07562)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|      LSH      | Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study                 |         BERT-base (110M)         |        Full Finetuning        |      Arxiv 2023      |                       [[Paper]](https://arxiv.org/abs/2305.03017v1)                       |                                                                                                                                                                                                                                                                                                                                                                                                        |
|  LLM2BERT4Rec  | Leveraging Large Language Models for Sequential Recommendation                                                                       |      text-embedding-ada-002      |            Frozen            |      RecSys 2023      |                        [[Paper]](https://arxiv.org/abs/2309.09261)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|    LLM4ARec    | Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations                                            |           GPT2 (110M)           |         Prompt Tuning         |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2306.01475)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|     TIGER     | Recommender Systems with Generative Retrieval                                                                                        |           Sentence-T5           |            Frozen            |       NIPS 2023       |                        [[Paper]](https://arxiv.org/abs/2305.05065)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|      TBIN      | TBIN: Modeling Long Textual Behavior Data for CTR Prediction                                                                         |         BERT-base (110M)         |            Frozen            |    DLP-RecSys 2023    |                        [[Paper]](https://arxiv.org/abs/2308.08483)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|     LKPNR     | LKPNR: LLM and KG for Personalized News Recommendation Framework                                                                     |           LLaMA2 (7B)           |            Frozen            |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2308.12028)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|      SSNA      | Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation                                    |     DistilRoBERTa-base (83M)     |   Layerwise Adapter Tuning   |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2310.01612)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
| CollabContext | Collaborative Contextualization: Bridging the Gap between Collaborative Filtering and Pre-trained Language Model                     |       Instructor-XL (1.5B)       |            Frozen            |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2310.09400)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|   LMIndexer   | Language Models As Semantic Indexers                                                                                                 |          T5-base (223M)          |        Full Finetuning        |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2310.07815)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|     Stack     | A BERT based Ensemble Approach for Sentiment Classification of Customer Reviews and its Application to Nudge Marketing in e-Commerce |         BERT-base (110M)         |            Frozen            |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2311.10782)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|      N/A      | Utilizing Language Models for Tour Itinerary Recommendation                                                                          |         BERT-base (110M)         |        Full Finetuning        |    PMAI@IJCAI 2023    |                        [[Paper]](https://arxiv.org/abs/2311.12355)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|      UEM      | User Embedding Model for Personalized Language Prompting                                                                             |     Sentence-T5-base (223M)     |            Frozen            |      Arxiv 2024      |                        [[Paper]](https://arxiv.org/abs/2401.04858)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|   Social-LLM   | Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data                                            |     SBERT-MPNet0base (110M)     |            Frozen            |      Arxiv 2024      |                        [[Paper]](https://arxiv.org/abs/2401.00893)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|     LLMRS     | LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase                                                   |           MPNet (110M)           |            Frozen            |      Arxiv 2024      |                        [[Paper]](https://arxiv.org/abs/2401.06676)                        |                                                                                                                                                                                                                                                                                                                                                                                                        |
|    GaCLLMâœ…    | Large Language Model with Graph Convolution for Recommendation                                                                       |           ChatGLM (6B)           |             LoRA             |      Arxiv 2024      |                      [[Paper]](https://arxiv.org/pdf/2402.08859.pdf)                      | ç°æœ‰LLMæ¨èå¿½ç•¥äº†ç”¨æˆ·-ç‰©å“äº¤äº’çš„ç»“æ„åŒ–çŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨LLMæ•æ‰é«˜é˜¶çš„ç”¨æˆ·-ç‰©å“å…³ç³»ã€‚é¦–å…ˆï¼Œåœ¨å…·ä½“domainä¸Šæ‰§è¡ŒSFTè®­ç»ƒï¼Œå¯¹é½ç”¨æˆ·å’Œç‰©å“çš„æè¿°ï¼›ç¬¬äºŒï¼Œé€’å½’åœ°ä»å°‘åˆ°å¤šèåˆé‚»å±…ä¿¡æ¯ã€‚æœ€åï¼Œé€šè¿‡æ‹¼æ¥å›¾embeddingè¡¨ç¤ºå’ŒLLMå›¾è¡¨ç¤ºï¼Œå†…ç§¯è®¡ç®—u-iå¾—åˆ†ã€‚                                                                                                                                                               |
|     SLIMâœ…     | Can Small Language Models be Good Reasoners for Sequential Recommendation?                                                           |           LLaMA2 (7B)           |             LoRA             |       WWW 2024       |                      [[Paper]](https://arxiv.org/pdf/2403.04260.pdf)                      | ä¼ ç»Ÿæ¨èæ¨¡å‹ä¾èµ–äºé—­å¼çš„è®­ç»ƒæ•°æ®é›†ï¼Œä¸å¯é¿å…å¼•å…¥æ›å…‰åå·®å’Œæµè¡Œåº¦åå·®ï¼Œè€ŒLLMå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰ä¸°å¯Œçš„å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†ï¼Œç†è§£äº¤äº’è¡Œä¸ºçš„æœ¬è´¨ã€‚æœ¬æ–‡æå‡ºç”¨chatgptçš„cotèƒ½åŠ›è’¸é¦åˆ°å°LLMï¼ˆå¦‚Llama2ï¼‰ï¼Œå†åŸºäºCoTæ¨ç†è¿‡ç¨‹åˆ†åˆ«ä½¿ç”¨åˆ°ID-Basedå’ŒID-Agnosticåœºæ™¯ä¸‹ï¼Œå®ç°æ€§èƒ½å¤§å¹…æå‡ã€‚å…¶ä¸­ï¼ŒCoTæ¨ç†è¾“å‡ºå’Œå†å²äº¤äº’ç‰©å“ä¿¡æ¯éƒ½ç”¨é¢å¤–çš„Bertè¿›è¡Œç¼–ç ï¼ŒID-Basedæ˜¯æ‹¼æ¥ç‰©å“ä¿¡æ¯å’ŒIDè¡¨ç¤ºï¼›ID-Agnosticæ˜¯ç›´æ¥åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ã€‚ |
| LSVCRâœ… | A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation | ChatGLM3 | LoRA | Arxiv 2024 | [[Code]](https://github.com/RUCAIBox/LSVCR/) | åœ¨å¿«æ‰‹å¹³å°ï¼Œæ‰§è¡Œè§†é¢‘æ¨èå’Œè¯„è®ºæ¨èä»»åŠ¡ã€‚ä¸»è¦æ˜¯ç”¨LLMå¯¹æ–‡æœ¬è¿›è¡Œç¼–ç å¾—åˆ°æ–‡æœ¬è¡¨ç¤ºã€‚LLMåªåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼ˆéƒ¨ç½²æ—¶æŠ›å¼ƒï¼‰ï¼Œå¯¹è¯„è®ºåºåˆ—å’Œè§†é¢‘åºåˆ—è¿›è¡Œç¼–ç ã€‚1. åˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œåå¥½å¯¹é½ï¼ˆä¸åŒåºåˆ—ä¹‹é—´ï¼‰ï¼Œå…¶ä¸­LLMè¾“å‡ºä¼šè¿›è¡Œå¢å¼ºï¼ˆå¦‚è§†é¢‘ä¼šç”¨ä¸åŒç«çƒ­è¯„è®ºï¼Œè¯„è®ºä¼šæ··åˆçœŸå®è¯„è®ºå’ŒLLMç”Ÿæˆçš„è™šæ‹Ÿè¯„è®ºï¼‰ï¼›2.å¯¹è§†é¢‘-æ–‡æœ¬ä¹‹é—´çš„è¡¨ç¤ºè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼›3. LLMæœ¬èº«çš„è‡ªå›å½’è®­ç»ƒï¼›4.åºåˆ—æ¨¡å‹CE æŸå¤±ã€‚ç¬¬äºŒé˜¶æ®µå¾®è°ƒï¼Œç›®æ ‡æ˜¯ID embeddingå’Œæ–‡æœ¬embeddingçš„å¯¹æ¯”æŸå¤±ã€‚ |
| BAHEâœ… | Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors | Qwen-1<br />ï¼ˆ8Bï¼‰ | LoRA | SIGIR 2024 | [[Paper]](https://arxiv.org/pdf/2403.19347.pdf) | å°†ç”¨æˆ·è¡Œä¸ºåˆ’åˆ†ä¸ºåŸå­æ“ä½œï¼ˆæ¯”å¦‚ç‚¹å‡»äº†xxxï¼ŒyyyåŠ å…¥è´­ç‰©è½¦ç­‰ï¼‰ï¼Œå…ˆç”¨LLMä½å±‚ï¼ˆä½å±‚å†»ç»“ï¼‰ç¼–ç è¿™äº›åŸå­è¡Œä¸ºï¼Œå¾—åˆ°è¡Œä¸ºæ•° * çº¬åº¦çš„embedding tableï¼Œå³å°†token-levelè½¬åŒ–ä¸ºbehavior-levelï¼ŒLLMé«˜å±‚å¯¹è¡Œä¸ºembeddingè¿›è¡Œç¼–ç å¾—åˆ°è¾“å‡ºï¼Œæœ€åå–‚å…¥CTRæ¨¡å‹é¢„ä¼°ç‚¹å‡»ç‡ã€‚ |
| LEASRâœ… | Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling | Mistral (7B) | LoRA | SIGIR 2024 | [[Paper]](https://arxiv.org/pdf/2403.16948) | å·²æœ‰offline RLæ¨èéš¾ä»¥è·å–æœ‰æ•ˆç”¨æˆ·åé¦ˆï¼Œä¸”å¦‚ä½•å»ºæ¨¡ç”¨æˆ·çŠ¶æ€å’ŒrewardåŒæ ·ä¹Ÿå…·æœ‰æŒ‘æˆ˜ã€‚æ­£å‘æˆ–è€…è´Ÿå‘actionè®¾ç½®ç»Ÿä¸€çš„rewardä¸å…·æœ‰åŒºåˆ†æ€§ï¼›åŸºäºç‰©å“IDç”Ÿæˆçš„ç”¨æˆ·çŠ¶æ€ä¸èƒ½åæ˜ ç‰¹å®šå†…å®¹çš„è¡Œä¸ºï¼›offlineåœ¨å›ºå®šçš„æ•°æ®ä¸Šè®­ç»ƒï¼Œæ­£è´Ÿåé¦ˆä¸è¶³ã€‚æœ¬æ–‡æå‡ºLLMä½œä¸ºç¯å¢ƒï¼ˆLEï¼‰ï¼Œåˆ›æ–°ç‚¹æœ‰ï¼šå¼•å…¥item tokenizationå¾—åˆ°ç‰©å“çš„è¯­ä¹‰embeddingï¼Œåç»­ç‰©å“IDåºåˆ—è¡¨ç¤ºä¼šå’Œè¯­ä¹‰è¡¨ç¤ºconcatèåˆï¼›è®¾è®¡Reward Modelå’ŒState Modelï¼ˆäºŒè€…éƒ½æœ‰å¯¹åº”ä¸“é—¨è®¾ç½®çš„lossï¼‰ï¼›åˆ©ç”¨LEå¢å¼ºæ•°æ®ï¼Œå¯¹æœªè§ç”¨æˆ·åé¦ˆçš„ä¸‹ä¸€æ¬¡ç‰©å“ï¼ˆæ¥è‡ªIDåºåˆ—çš„top-5ç‰©å“ï¼‰é€‰å–æ¦‚ç‡æœ€é«˜è€…ä½œä¸ºå¢å¼ºåºåˆ—ã€‚ |
| NoteLLMâœ… | NoteLLM: A Retrievable Large Language Model for Note Recommendation | LLaMA2 (7B) | Full<br />Finetuning | WWW 2024 | [[Paper]](https://arxiv.org/pdf/2403.01744) | NoteLLMé€šè¿‡å­¦ä¹ ç”¨æˆ·è¡Œä¸ºå’Œç”Ÿæˆå…³é”®æ¦‚å¿µæ¥æé«˜ç”¨æˆ·çš„è®ºæ–‡æ¨èå’Œæ ‡ç­¾ç”Ÿæˆä½“éªŒã€‚åŠ¨æœºæ˜¯æ–‡ç« è®¤ä¸ºtag/categoryç”Ÿæˆå’Œembeddingç”Ÿæˆéƒ½å…¶å®æ˜¯å¯¹å…³é”®ä¿¡æ¯çš„å‹ç¼©ï¼Œå› æ­¤å‰è€…å¯¹embeddingç”Ÿæˆæ˜¯æœ‰å¥½å¤„çš„ã€‚å…·ä½“å®ç°æ˜¯é€šè¿‡åœ¨prompté‡Œé¢æ·»åŠ [EMB],\<Output\>æ¥è·å–ç‰©å“embeddingå’Œtagç”Ÿæˆã€‚äºŒè€…åˆ†åˆ«å¯¹åº”ç›®æ ‡å‡½æ•°çš„å¯¹æ¯”å­¦ä¹ å’ŒCEå­¦ä¹ éƒ¨åˆ†ã€‚ |
| LRDâœ… | Sequential Recommendation with Latent Relations based on Large Language Model | GPT-3<br />(175B) | Frozen | SIGIR 2024 | [[Code]](https://github.com/ysh-1998/LRD) | ç°æœ‰å…³ç³»æ„ŸçŸ¥åºåˆ—æ¨¡å‹ä½¿ç”¨é¢„å®šä¹‰çš„å…³ç³»ï¼Œå®¹æ˜“é‡åˆ°ç¨€ç–æ€§é—®é¢˜ã€‚æœ¬æ–‡é‡‡ç”¨LLMæŒ–æ˜ç‰©å“ä¹‹é—´çš„éšè—å…³ç³»ã€‚é¦–å…ˆå¯¹ç‰©å“æè¿°ç”¨LLMæ¥æŠ½å–è¡¨ç¤ºï¼Œåˆ©ç”¨DVAEæ€æƒ³æå–ç‰©å“å¯¹èƒŒåçš„å…³ç³»ï¼Œå†åŸºäºæå–å‡ºè¡¨ç¤ºå’Œè¿æ¥ç‰©å“é‡æ„ç›®æ ‡ç‰©å“ï¼›å¯¹äºå·²æœ‰å®šä¹‰çš„å…³ç³»ï¼Œä¹Ÿä¼šå¼•å…¥çŸ¥è¯†å›¾è°±çš„ä¼˜åŒ–ç›®æ ‡ï¼›å†åŠ ä¸Šæ¨èæœ¬èº«çš„ç›®æ ‡è¿›è¡Œå¤šä»»åŠ¡è”åˆå­¦ä¹ ã€‚ |
| CTRLâœ… | CTRL: Connect Collaborative and Language Model for CTR Prediction | RoBERTa | Frozen | Arxiv 2023 | [[Paper]](https://arxiv.org/pdf/2306.02841) | æå‡ºCTRLå°†ååŒä¿¡å·å’Œè¯­ä¹‰ä¿¡å·ç»“åˆèµ·æ¥ã€‚å…·ä½“æ˜¯æå‡ºä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šä¸€é˜¶æ®µæ˜¯æ‰§è¡Œæ–‡æœ¬è¡¨ç¤ºå’ŒCTRååŒæ¨¡å‹è¡¨ç¤ºè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼›äºŒé˜¶æ®µæ˜¯å¯¹ååŒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡ŒSFTå¾®è°ƒã€‚ |
| EAGERâœ… | EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration | Sentence-T5 | Frozen | KDD 2024 | [[Code]](https://github.com/yewzz/EAGER) | ç°æœ‰ç”Ÿæˆå¼æ¨èè¦ä¹ˆåªå…³æ³¨è¯­ä¹‰ï¼Œè¦ä¹ˆåªå…³æ³¨è¡Œä¸ºçŸ¥è¯†ï¼Œå¿½ç•¥äº†äºŒè€…çš„äº’è¡¥æ€§ã€‚æ•…æå‡ºEAGERï¼Œä½œä¸ºä¸€ç§åŒæµç”Ÿæˆæ¶æ„ï¼Œåˆ©ç”¨å…±äº«encoderå’Œåˆ†ç¦»decoderï¼Œè¾…ä»¥åŸºäºç½®ä¿¡åº¦çš„æ’åºç­–ç•¥ã€‚æ­¤å¤–ï¼Œæå‡ºä½¿ç”¨summary tokençš„å…¨å±€å¯¹æ¯”ä»»åŠ¡å®ç°æ¯ç§ç±»å‹ï¼ˆè¡Œä¸ºå’Œè¯­ä¹‰ï¼‰çš„åŒºåˆ†æ€§è§£ç ï¼Œä»¥åŠè¯­ä¹‰å¼•å¯¼çš„è¿ç§»ä»»åŠ¡ï¼Œé€šè¿‡è®¾è®¡çš„é‡æ„å’Œè¯†åˆ«ç›®æ ‡æå‡è¡Œä¸ºå’Œè¯­ä¹‰çš„äº¤å‰èƒ½åŠ›ã€‚ |

<h4 id="1.2.2">1.2.2 Unified Cross-domain Recommendation</h4>

|  **Name**  | **Paper**                                                                                                        |    **LLM Backbone (Largest)**    | **LLM Tuning Strategy** | **Publication** |                   **Link**                   | Main Contributions                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| :--------------: | :--------------------------------------------------------------------------------------------------------------------- | :-------------------------------------: | :---------------------------: | :-------------------: | :------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|      ZESRec      | Zero-Shot Recommender Systems                                                                                          |            BERT-base (110M)            |            Frozen            |      Arxiv 2021      |      [[Paper]](https://arxiv.org/abs/2105.08318)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|    UniSRecâœ…    | Towards Universal Sequence Representation Learning for Recommender Systems                                             |            BERT-base (110M)            |            Frozen            |       KDD 2022       |     [[Code]](https://github.com/RUCAIBox/UniSRec)     | åŸºäºç‰©å“IDåºåˆ—æ¨èéš¾ä»¥å®ç°è·¨åŸŸè¿ç§»ã€‚UniSRecæå‡ºä½¿ç”¨ç‰©å“æ–‡æœ¬ä¿¡æ¯è¡¨ç¤ºç‰©å“ï¼Œå­¦ä¹ é€šç”¨ç‰©å“è¡¨ç¤ºå’Œé€šç”¨åºåˆ—è¡¨ç¤ºã€‚ä¸ºäº†å­¦ä¹ é€šç”¨ç‰©å“è¡¨ç¤ºï¼Œä½¿ç”¨å‚æ•°ç™½åŒ–ï¼ˆå› ä¸ºBERTå®¹æ˜“å‡ºç°éå¹³æ»‘å„å‘å¼‚æ€§è¡¨å¾ç©ºé—´ï¼‰å’ŒMOEç»“æ„ï¼ˆä¸åŒåŸŸä¹‹é—´å­˜åœ¨è¯­ä¹‰gapï¼Œä½¿ç”¨MOEè‡ªé€‚åº”ç»„åˆç‰©å“è¡¨ç¤ºå®ç°è¿ç§»å’Œèåˆä¸åŒåŸŸçš„ä¿¡æ¯ï¼‰ã€‚ä¸ºäº†å®ç°é€šç”¨åºåˆ—è¡¨ç¤ºï¼Œæå‡ºåºåˆ—-ç‰©å“å’Œåºåˆ—-åºåˆ—çš„å¯¹æ¯”ä»»åŠ¡ï¼ˆåºåˆ—æ˜¯é€šè¿‡word dropå’Œitem dropå®ç°ï¼‰ã€‚å¯¹äºfine-tuneé˜¶æ®µï¼Œæ ¹æ®inductiveå’Œtransductiveè®¾ç½®ï¼Œåˆ†åˆ«è€ƒè™‘æ˜¯å¦èåˆIDä¿¡æ¯ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒåªè®­ç»ƒMOEå‚æ•°ã€‚ |
|     TransRec     | TransRec: Learning Transferable Recommendation from Mixture-of-Modality Feedback                                       |            BERT-base (110M)            |        Full Finetuning        |      Arxiv 2022      |      [[Paper]](https://arxiv.org/abs/2206.06190)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|      VQ-Rec      | Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders                                 |            BERT-base (110M)            |            Frozen            |       WWW 2023       |      [[Paper]](https://arxiv.org/abs/2210.12316)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| IDRec vs MoRecâœ… | Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited                          |            BERT-base (110M)            |        Full Finetuning        |      SIGIR 2023      | [[Code]](https://github.com/westlake-repl/IDvs.MoRec) | ç ”ç©¶çº¯ID-basedæ¨èå’Œçº¯modality-basedæ¨èï¼Œå‘ç°ä½¿ç”¨é¢„è®­ç»ƒçš„encoderè¿›è¡Œend2endè®­ç»ƒï¼Œèƒ½å¤ŸæŒå¹³ç”šè‡³è¶…è¿‡ID-basedæ¨èç®—æ³•ï¼ˆç”šè‡³å¯¹äºéå†·å¯åŠ¨ç‰©å“ï¼‰ã€‚                                                                                                                                                                                                                                                                                                                                     |
|     TransRec     | Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights            |           RoBERTa-base (125M)           |   Layerwise Adapter Tuning   |      Arxiv 2023      |      [[Paper]](https://arxiv.org/abs/2305.15036)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|       TCF       | Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights |             OPT-175B (175B)             |    Frozen/ Full Finetuning    |      Arxiv 2023      |      [[Paper]](https://arxiv.org/abs/2305.11700)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| S&R Foundationâœ… | An Unified Search and Recommendation Foundation Model for Cold-Start Scenario                                          | ChatGLM (6B) |            Frozen            |       CIKM 2023       |      [[Paper]](https://arxiv.org/abs/2309.08939)      | æå‡ºè”åˆæœç´¢æ¨èçš„å¤šåŸŸåŸºçŸ³æ¨¡å‹ï¼Œåˆ©ç”¨LLMä»æœç´¢æ–‡æœ¬å’Œæ¨èå•†å“æ–‡æœ¬ä¿¡æ¯ä¸­æå–åŸŸä¸å˜ç‰¹å¾ï¼Œåˆ©ç”¨ï¼ˆä¸‰ç§ï¼‰Aspect Gating FusionæŠ€æœ¯åˆå¹¶IDã€åŸŸä¸å˜ç‰¹å¾ã€ä»»åŠ¡ç‰¹å®šç¨€ç–ç‰¹å¾è·å¾—æœ€ç»ˆæœç´¢å’Œç‰©å“çš„è¡¨ç¤ºï¼ŒåŒæ—¶ä½¿ç”¨åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼ˆJSæ•£åº¦ï¼‰é™åˆ¶åŸŸåˆ†å¸ƒçš„å‘æ•£ç¨‹åº¦ã€‚ä½¿ç”¨æ¥è‡ªæœç´¢å’Œæ¨èä»»åŠ¡çš„æ ·æœ¬è”åˆè®­ç»ƒï¼Œéµå¾ªpretrain-finetuningèŒƒå¼ã€‚                                                                                                                                                                       |
|     MISSRec     | MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation           |            CLIP-B/32 (400M)            |        Full Finetuning        |        MM 2023        |      [[Paper]](https://arxiv.org/abs/2308.11175)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|      UFINâœ…      | UFIN: Universal Feature Interaction Network for Multi-Domain Click-Through Rate Prediction                             |           FLAN-T5-base (250M)           |            Frozen            |      Arxiv 2023      |      [[Code]](https://github.com/Ethan-TZ/UFIN)      | å·²æœ‰è·¨åŸŸæ–¹æ³•ç”±äºä¾èµ–äºIDç‰¹å¾ï¼Œéš¾ä»¥å¾ˆå¥½åœ°å®ç°ç‰¹å¾è¿ç§»ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨æ–‡æœ¬æ•°æ®å­¦ä¹ é€šç”¨ç‰¹å¾äº¤äº’ï¼Œä»è€Œæœ‰æ•ˆåœ¨å„ä¸ªåŸŸä¸Šè¿ç§»ã€‚å…·ä½“åœ°ï¼Œå°†æ–‡æœ¬å’Œç‰¹å¾è§†ä½œä¸¤ä¸ªæ¨¡æ€ï¼Œåˆ©ç”¨åŸºäºLLMçš„encoder-decoderç½‘ç»œç»“æ„åŠ å¼ºä»æ–‡æœ¬åˆ°ç‰¹å¾æ¨¡æ€çš„è¿ç§»æ•ˆæœï¼ˆåŸå§‹ç‰¹å¾->æ–‡æœ¬->é€šç”¨ç‰¹å¾ï¼‰ï¼›å¹¶åˆ©ç”¨MOEå¢å¼ºçš„è‡ªé€‚åº”ç‰¹å¾äº¤äº’æ¨¡å‹å­¦ä¹ å¯è¿ç§»çš„ç‰¹å¾äº¤äº’ï¼›æœ€åæå‡ºå¤šåŸŸçŸ¥è¯†è’¸é¦æ¡†æ¶è®­ç»ƒæ¨¡å‹ã€‚                                                                                                                                       |
|      PMMRec      | Multi-Modality is All You Need for Transferable Recommender Systems                                                    |          RoBERTa-large (355M)          |    Top-2-layer Finetuning    |       ICDE 2024       |      [[Paper]](https://arxiv.org/abs/2312.09602)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|     Uni-CTR     | A Unified Framework for Multi-Domain CTR Prediction via Large Language Models                                          |          Sheared-LLaMA (1.3B)          |             LoRA             |      Arxiv 2023      |      [[Paper]](https://arxiv.org/abs/2312.10743)      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |



<h3 id="1.3"> 1.3 LLM as Scoring/Ranking Function </h3>

<p>

<h4 id="1.3.1">1.3.1 Item Scoring Task</h4>

| **Name** | **Paper**                                                                                                     | **LLM Backbone (Largest)** |     **LLM Tuning Strategy**     | **Publication** |                         **Link**                         | Main Contributions                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| :------------: | :------------------------------------------------------------------------------------------------------------------ | :------------------------------: | :------------------------------------: | :-------------------: | :-------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
|    LMRecSys    | Language Models as Recommender Systems: Evaluations and Limitations                                                 |          GPT2-XL (1.5B)          |            Full Finetuning            |      ICBINB 2021      |       [[Paper]](https://openreview.net/forum?id=hFx3fY7-m9b)       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|      PTab      | PTab: Using the Pre-trained Language Model for Modeling Tabular Data                                                |         BERT-base (110M)         |            Full Finetuning            |      Arxiv 2022      |            [[Paper]](https://arxiv.org/abs/2209.08060)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|   UniTRecâœ…   | UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation  |           BART (406M)           |            Full Finetuning            |       ACL 2023       |      [[Code]](https://github.com/Veason-silverbullet/UniTRec)      | PLMå¤„ç†ç”¨æˆ·å†å²ç‰©å“çš„æ–‡æœ¬ä¿¡æ¯å­˜åœ¨ä¸¤ç§æ–¹æ³•ï¼Œä½†éƒ½æœ‰ä¸è¶³ï¼š(a). ç›´æ¥å°†å…¨éƒ¨ç‰©å“å†å²ä½œä¸ºè¾“å…¥æ–‡æœ¬ä½œä¸ºç”¨æˆ·å…¨å±€è¡¨ç¤ºï¼Œå¿½ç•¥äº†ä¸åŒç‰©å“æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œè€Œæ˜¯æŠŠæ‰€æœ‰è¯éƒ½è§†ä½œåŒç­‰å‚ä¸ï¼›(b).å¼•å…¥é¢å¤–èšåˆç½‘ç»œï¼Œè¿™å‰Šå¼±äº†PLM ç¼–ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚æœ¬æ–‡è´¡çŒ®ï¼š(a). å¯¹æ³¨æ„åŠ›æ©ç æ“ä½œè®¾è®¡word-levelå’Œturn-levelä¸¤ç§æœºåˆ¶ (b).è”åˆè®­ç»ƒåŸºäºDiscriminative Scoreså’ŒCandidate Text Perplexityçš„å¯¹æ¯”æŸå¤±ã€‚                                                                                                                         |
|   Prompt4NR   | Prompt Learning for News Recommendation                                                                             |         BERT-base (110M)         |            Full Finetuning            |      SIGIR 2023      |            [[Paper]](https://arxiv.org/abs/2304.05263)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|  RecFormerâœ…  | Text Is All You Need: Learning Language Representations for Sequential Recommendation                               |        LongFormer (149M)        |            Full Finetuning            |       KDD 2023       |          [[Code]](https://github.com/AaronHeee/RecFormer)          | ID-basedæ¨èéš¾ä»¥è§£å†³å†·å¯åŠ¨é—®é¢˜ï¼›å·²æœ‰è·¨åŸŸæ¨èå·¥ä½œé€šå¸¸å‡è®¾æœ‰é‡å ç‰¹å¾ã€ç‰©å“æˆ–è€…ç”¨æˆ·ç­‰ï¼Œè¿™é™åˆ¶äº†ç®—æ³•åº”ç”¨ã€‚åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åº”ç”¨å­˜åœ¨é—®é¢˜:(a)é¢„è®­ç»ƒæ–‡æœ¬å’Œç‰©å“æ–‡æœ¬å­˜åœ¨é¢†åŸŸå·®å¼‚;(b)æ— æ³•å»ºæ¨¡ç»†ç²’åº¦ç”¨æˆ·åå¥½ã€‚æœ¬æ–‡æå‡ºä»¥key-valueå½¢å¼è¡¨è¾¾ç‰©å“æ–‡æœ¬ï¼Œä¿®æ”¹LongFormeré€šè¿‡åŠ å…¥ç‰¹æ®Šè®¾è®¡Token Embeddingï¼Œä»¥åŠé¢„è®­ç»ƒ+ä¸¤é˜¶æ®µå¾®è°ƒã€‚                                                                                                                                                                                |
|     TabLLM     | TabLLM: Few-shot Classification of Tabular Data with Large Language Models                                          |             T0 (11B)             | Few-shot Parameter-effiecnt Finetuning |     AISTATS 2023     |            [[Paper]](https://arxiv.org/abs/2210.10723)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Zero-shot GPT | Zero-Shot Recommendation as Language Modeling                                                                       |        GPT2-medium (355M)        |                 Frozen                 |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2112.04184)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|   FLAN-T5âœ…   | Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction                                      |         FLAN-5-XXL (11B)         |            Full Finetuning            |    PGAI@CIKM 2023    |          [[Paper]](https://arxiv.org/pdf/2305.06474.pdf)          | 1. è¶…è¿‡100Bçš„æ¨¡å‹åœ¨å†·å¯åŠ¨æƒ…å†µä¸‹ä¸å¯å‘å¼æ–¹æ³•æ¥è¿‘çš„æ€§èƒ½ï¼›2. zero-shotæƒ…å†µä¸‹ä»ç„¶æ— æ³•æŠ—è¡¡ç›‘ç£å­¦ä¹ ä¸‹çš„ä¼ ç»Ÿæ¨èæ¨¡å‹ï¼›3. Fine-Tuned çš„æ¨¡å‹ä»…éœ€è¦å°‘é‡æ•°æ®å°±èƒ½æŒå¹³ç”šè‡³è¶…è¿‡ä¼ ç»Ÿæ¨¡å‹ï¼›                                                                                                                                                                                                                                                                                                    |
|    BookGPT    | BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model                              |             ChatGPT             |                 Frozen                 |      Arxiv 2023      |           [[Paper]](https://arxiv.org/abs/2305.15673v1)           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|   TALLRecâœ…   | TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation              |            LLaMA (7B)            |                  LoRA                  |      RecSys 2023      |           [[Code]](https://github.com/SAI990323/TALLRec)           | LLMå’Œæ¨èä»»åŠ¡å­˜åœ¨gapï¼Œé¢å‘æ¨èçš„è¯­æ–™åº“ä¸è¶³ï¼›æ­¤å¤–ï¼ŒLLMå­˜åœ¨ä¸€å®šé™åˆ¶æ— æ³•ä¿è¯è¾“å‡ºåŒ…å«ç›®æ ‡ç‰©å“ï¼ˆå¦‚æ‹’ç»å›ç­”ï¼Œæˆ–å…¨éƒ¨positiveå›ç­”ï¼‰ã€‚å…·ä½“åŒ…æ‹¬ä¸¤é˜¶æ®µå¾®è°ƒï¼Œä¸€é˜¶æ®µæ˜¯ä½¿ç”¨ self-instructæ•°æ®å¾®è°ƒLLMï¼ŒäºŒé˜¶æ®µæ˜¯åˆ©ç”¨instructionæ•°æ®å¾®è°ƒLLMã€‚                                                                                                                                                                                                                                                                     |
|      PBNR      | PBNR: Prompt-based News Recommender System                                                                          |          T5-small (60M)          |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2304.07862)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|    CR-SoRec    | CR-SoRec: BERT driven Consistency Regularization for Social Recommendation                                          |         BERT-base (110M)         |            Full Finetuning            |      RecSys 2023      | [[Paper]](https://dl.acm.org/doi/fullHtml/10.1145/3604915.3608844) |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|     GLRec     | Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations                           |         BELLE-LLaMA (7B)         |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2307.05722)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|   BERT4CTRâœ…   | BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction |       RoBERTa-large (355M)       |            Full Finetuning            |       KDD 2023       |   [[Paper]](https://dl.acm.org/doi/abs/10.1145/3580305.3599780)   | èåˆéæ–‡æœ¬ç‰¹å¾ï¼ˆsparseå’Œdense featureï¼‰åˆ°LMæ˜¯æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ¬æ–‡æå‡ºUni-Attentionæ–¹æ³•ï¼Œéæ–‡æœ¬ç‰¹å¾å–æ¶ˆä½ç½®ç¼–ç ï¼ŒåŒæ—¶æ”¹å˜å…¶Attentionæ–¹å¼ï¼Œéæ–‡æœ¬ç‰¹å¾ä½œä¸ºQï¼Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºVå’ŒKã€‚                                                                                                                                                                                                                                                                                                                         |
|    ReLLaâœ…    | ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation    |           Vicuna (13B)           |                  LoRA                  |       WWW 2024       |            [[Code]](https://github.com/LaVieEnRose365/ReLLa)            | å®éªŒå‘ç°æ¨èåœºæ™¯ä¸­ï¼Œå¹¶éç”¨æˆ·å†å²åºåˆ—è¶Šé•¿æ•ˆæœè¶Šå¥½ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨semantic user behavior retrieval (SUBR)ä»£æ›¿æœ€è¿‘Kä¸ªäº¤äº’å†å²æ¥æå‡æ•°æ®è´¨é‡ã€‚å¯¹äºzero-shotï¼Œä¸tuneæ¨¡å‹è€Œç›´æ¥ä½¿ç”¨SUBRï¼›å¯¹äºfew-shotï¼Œé™¤äº†ä½¿ç”¨SUBRï¼Œè¿˜è¦ç”¨åŸå§‹å’ŒSUBRå¢å¼ºçš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚SUBRå…·ä½“æ˜¯å¯¹å†å²ç‰©å“å’Œç›®æ ‡ç‰©å“åšPCAé™ç»´ï¼ˆå®ç°å»å™ªï¼‰ï¼Œç„¶åcosineè®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ›¿æ¢åŸå…ˆçš„æœ€è¿‘Kä¸ªäº¤äº’å†å²ã€‚                                                                                                                                        |
|    TASTEâœ…    | Text Matching Improves Sequential Recommendation by Reducing Popularity Biases                                      |          T5-base (223M)          |            Full Finetuning            |       CIKM 2023       |            [[Code]](https://github.com/OpenMatch/TASTE)            | å°†ç”¨æˆ·äº¤äº’å†å²å’Œç‰©å“ä¿¡æ¯è¡¨ç¤ºä¸ºIDå’Œæ–‡æœ¬ä¿¡æ¯çš„æ‹¼æ¥å½¢å¼ï¼Œåˆ©ç”¨æ–‡æœ¬è¡¨ç¤ºåŒ¹é…è¿›è¡Œæ¨èã€‚æ­¤å¤–ï¼ŒTASTEå»ºæ¨¡ç”¨æˆ·é•¿åºåˆ—é€šè¿‡æ³¨æ„åŠ›ç¨€ç–åŒ–æ–¹æ³•ï¼Œå³å°†ç”¨æˆ·å†å²ç‰©å“åˆ†ä¸ºå¤šä¸ªå­åºåˆ—åˆ†åˆ«è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå†å°†è®¡ç®—å‡ºæ¥çš„è¡¨ç¤ºæ‹¼æ¥ã€‚ä½¿ç”¨in-batchå’Œéšæœºé‡‡æ ·çš„ç‰©å“è¿›è¡Œå¯¹æ¯”æŸå¤±è®¡ç®—ã€‚                                                                                                                                                                                                                                      |
|      N/A      | Unveiling Challenging Cases in Text-based Recommender Systems                                                       |         BERT-base (110M)         |            Full Finetuning            | RecSys Workshop 2023 |         [[Paper]](https://ceur-ws.org/Vol-3476/paper5.pdf)         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ClickPromptâœ… | ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction                 |       RoBERTa-large (355M)       |            Full Finetuning            |       WWW 2024       |            [[Paper]](https://arxiv.org/abs/2310.09234)            | ä¼ ç»ŸCTRå»ºæ¨¡IDä¿¡æ¯çš„æ–¹å¼ä¼šå¤±å»æ–‡æœ¬å†…åœ¨çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€ŒPLMæ— æ³•å»ºæ¨¡ååŒçŸ¥è¯†ï¼ˆIDç‰¹å¾æ— è¯­ä¹‰ã€ç‰¹å¾é€šè¿‡æ¨¡ç‰ˆå¡«å……çº¿æ€§ç»„åˆä¸ºæ–‡æœ¬è€Œä¸å…·å¤‡ç‰¹å¾äº¤å‰èƒ½åŠ›ï¼‰ã€‚æœ¬æ–‡æå‡ºå°†IDç‰¹å¾å˜æ¢ä¸ºäº¤äº’æ„ŸçŸ¥çš„soft promptï¼Œå†é€šè¿‡ä¼ ç»ŸCTRæ¨¡å‹å’ŒPLMçš„pretrain-finetuneå­¦ä¹ æœºåˆ¶ï¼Œå®ç°å¯¹è¯­ä¹‰çŸ¥è¯†å’Œåä½œçŸ¥è¯†çš„å»ºæ¨¡ã€‚                                                                                                                                                                                                                     |
|  SetwiseRank  | A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models                  |        FLAN-T5-XXL (11B)        |                 Frozen                 |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.09497)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|      UPSR      | Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language                                             |          T5-base (223M)          |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.13540)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|    LLM-Rec    | One Model for All: Large Language Models are Domain-Agnostic Recommendation Systems                                 |            OPT (6.7B)            |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.14304)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|   LLMRanker   | Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels                        |           FLAN PaLM2 S           |                 Frozen                 |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.14122)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|     CoLLM     | CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation                           |           Vicuna (7B)           |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.19488)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|      FLIP      | FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction      |       RoBERTa-large (355M)       |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.19453)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|     BTRec     | BTRec: BERT-Based Trajectory Recommendation for Personalized Tours                                                  |         BERT-base (110M)         |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.19886)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|   CLLM4Recâœ…   | Collaborative Large Language Model for Recommender Systems                                                          |           GPT2 (110M)           |            Full Finetuning            |       WWW 2024       |          [[Code]](https://github.com/yaochenzhu/LLM4Rec)          | è§£å†³LLMå’Œæ¨èä»»åŠ¡ä¹‹é—´çš„è¯­ä¹‰gapï¼Œå¦‚è¯­è¨€å»ºæ¨¡å¼•å…¥ç”¨æˆ·/ç‰©å“çš„è™šå‡å…³è”ï¼ˆå¦‚user_4332å’Œuser_43,user_43ï¼Œæˆ–è€…è¯­è¨€å…·æœ‰é¡ºåºï¼Œè€Œäº¤äº’å†å²ç‰©å“ä¸éœ€è¦è€ƒè™‘é¡ºåºï¼‰ï¼›ç›´æ¥è¿›è¡Œè‡ªå›å½’çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡å¯¹äºæ¨èä»»åŠ¡æ˜¯ä¸é«˜æ•ˆã€ä¸ç¨³å®šçš„ï¼›ä½¿ç”¨å†…å®¹è¡¨ç¤ºç‰©å“å’Œç”¨æˆ·ï¼Œå®¹æ˜“å¼•å…¥å™ªå£°ã€‚æœ¬æ–‡æå‡ºç»“åˆLLMèŒƒå¼å’Œæ¨èIDèŒƒå¼ï¼Œè®¾è®¡soft+hard promptingç­–ç•¥ï¼ˆå«ååŒå’Œå†…å®¹çš„åŒLLMï¼‰ï¼Œäº’æ­£åˆ™åŒ–æœºåˆ¶é™åˆ¶æ¨¡å‹å­¦ä¹ æ›´ç›¸å…³çš„æ¨èä¿¡æ¯ï¼Œä½¿ç”¨å¤šé¡¹ä¼¼ç„¶é¢„æµ‹å¤´è¿›è¡Œé«˜æ•ˆæ¨æ–­ã€‚                                                                             |
|      CUP      | Recommendations by Concise User Profiles from Review Text                                                           |         BERT-base (110M)         |         Last-layer Finetuning         |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2311.01314)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|      N/A      | Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers                                    |         FLAN-T5-XL (3B)         |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2311.01555)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|    CoWPiRec    | Collaborative Word-based Pre-trained Item Representation for Transferable Recommendation                            |         BERT-base (110M)         |            Full Finetuning            |       ICDM 2023       |            [[Paper]](https://arxiv.org/abs/2311.10501)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|     E4SRec     | E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation   |           LLaMA2 (13B)           |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2312.02443)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|      CER      | The Problem of Coherence in Natural Language Explanations of Recommendations                                        |           GPT2 (110M)           |            Full Finetuning            |       ECAI 2023       |            [[Paper]](https://arxiv.org/abs/2312.11356)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|      LSAT      | Preliminary Study on Incremental Learning for Large Language Model-based Recommender Systems                        |            LLaMA (7B)            |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2312.15599)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|   Llama4Rec   | Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation              |           LLaMA2 (7B)           |            Full Finetuning            |      Arxiv 2024      |            [[Paper]](https://arxiv.org/abs/2401.13870)            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|     PFCRâœ…     | Prompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation                           |                                  |            Full Finetuning            |       WWW 2024       |              [[Code]](https://github.com/Ckano/PFCR)              | å·²æœ‰è·¨åŸŸæ¨èè”é‚¦ç®—æ³•å­˜åœ¨ç¼ºé™·ï¼š(1)ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯ä¸Šä¼ åˆ°ä¸­å¿ƒæœåŠ¡å™¨ï¼Œå®¹æ˜“æ³„æ¼ç”¨æˆ·éšç§ï¼›(2)ä¾èµ–äºIDè¡¨ç¤ºï¼Œä¸åˆ©äºä¸åŒåŸŸä¹‹é—´è¿ç§»ï¼›(3)åŸºäºè·¨åŸŸä¹‹é—´å­˜åœ¨é‡å ç”¨æˆ·å‡è®¾ã€‚æœ¬æ–‡æå‡ºï¼š(1) å°†æ¯ä¸ªåŸŸè§†ä½œclientï¼Œç”¨æˆ·éšç§æ•°æ®è¿›è¡Œå±€éƒ¨æ›´æ–°ï¼Œåªä¸Šä¼ æ¯ä¸ªclientçš„ç‰©å“ç›¸å…³æ¢¯åº¦ï¼›(2)ä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºç‰©å“ä¿¡æ¯ï¼ˆä¸ºé˜²æ­¢recommenderè¿‡äºå¼ºè°ƒæ–‡æœ¬ç‰¹å¾ï¼Œä½¿ç”¨text->code->representationæœºåˆ¶ï¼‰ï¼›(3)æå‡ºFull promptingæœºåˆ¶ï¼ˆåºåˆ—promptã€ç”¨æˆ·promptã€domain promptï¼‰å’ŒLight Promptingæœºåˆ¶ï¼ˆåºåˆ—promptingã€åŸŸpromptingï¼‰å®ç°åŸŸè‡ªé€‚åº”å¾®è°ƒã€‚ |
| E2URecâœ… | Towards Efficient and Effective Unlearning of Large Language Models for Recommendation | T5-base (223M) | LoRA | Arxiv 2024 | [[Code]](https://github.com/justarter/E2URec) | ä¸ºéšç§ä¿æŠ¤å’Œæ•ˆç‡è€ƒè™‘ï¼Œéœ€è¦å®ç°å·²æœ‰æ¨¡å‹å¯¹éƒ¨åˆ†æ•°æ®çš„é—å¿˜ã€‚æœ¬æ–‡åšæ³•æ˜¯è®¾è®¡ä¸¤ä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹æ˜¯åªç”¨é—å¿˜æ•°æ®è®­ç»ƒï¼Œç¬¬äºŒä¸ªæ¨¡å‹æ˜¯åŸå§‹æ¨¡å‹ã€‚ä½¿ç”¨v-\alpha ReLU(v_f-v)å¯ä»¥è¡¨ç¤ºé—å¿˜äº†æ•°æ®çš„logitsä¿¡æ¯ï¼Œå¯¹äºä¸¤ä¸ªæ•™å¸ˆæ¨¡å‹éƒ½é€‚ç”¨KLæ•£åº¦ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼ŒåŒæ—¶å¼•å…¥è¯­è¨€æœ¬èº«NLL lossã€‚ |
| PromptRecâœ… | Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendations | BERT<br />GPT2<br />T5<br />LLaMA | Full Finetuning | WWW 2024 | [[Code]](https://github.com/JacksonWuxs/PromptRec) | è§£å†³ç³»ç»Ÿå†·å¯åŠ¨é—®é¢˜ã€‚é¦–å…ˆæå‡ºPromptRecï¼ŒåŸºäºLLMå¯¹ç”¨æˆ·-ç‰©å“ç‰¹å¾æè¿°ï¼Œå°†CTRè½¬åŒ–ä¸ºæƒ…æ„Ÿé¢„æµ‹ä»»åŠ¡ã€‚æ¥ç€ï¼Œç”±äºLLMå»¶è¿Ÿé«˜ï¼Œæ‰€ä»¥è¦è½¬ç§»In-Context-Learningèƒ½åŠ›åˆ°å°LMä¸Šï¼šï¼ˆ1ï¼‰ä½¿ç”¨Refined Corpuså®Œæˆæ¨¡å‹é¢„è®­ç»ƒï¼Œè¿™ä¸€æ­¥æ˜¯å› ä¸ºé¢„è®­ç»ƒå°æ¨¡å‹æ˜¯åœ¨é€šç”¨æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œä¼šæµªè´¹ä¸€äº›å‚æ•°åœ¨ä»»åŠ¡ä¸ç›¸å…³çš„æ–‡æ¡£ä¸Šï¼›ï¼ˆ2ï¼‰Transferable Prompt Pre-Trainingï¼Œåˆ©ç”¨éšé©¬å°”å¯å¤«HMMè®­ç»ƒlearnable promptï¼Œè¿™é‡Œä¼šå°†prefix promptåˆ’åˆ†ä¸ºtaskå’Œdomain promptï¼Œè¿™é‡Œåªä¼šè®­ç»ƒtask promptã€‚ |
| RATâœ… | RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction | Transformer | Full Finetuning | WWW 2024 | [[Code]](https://github.com/YushenLi807/WWW24-RAT) | å½“å‰CTRæ–¹æ³•ä¸»è¦é›†ä¸­äºå¯¹å•ä¸ªæ ·æœ¬å†…çš„ç‰¹å¾äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œè€Œå¿½ç•¥äº†å¯ä»¥ä½œä¸ºå¢å¼ºé¢„æµ‹çš„å‚è€ƒä¸Šä¸‹æ–‡çš„æ½œåœ¨è·¨æ ·æœ¬å…³ç³»ã€‚RATé€šè¿‡æ£€ç´¢ç›¸ä¼¼æ ·æœ¬å¹¶åˆ©ç”¨è¿™äº›å¤–éƒ¨ç¤ºä¾‹æ¥å¢å¼ºæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚æ£€ç´¢å¢å¼ºæ˜¯ä»æ ·æœ¬(x,y)é€šè¿‡queryçš„xå’Œkeyçš„xè®¡ç®—BM25ç›¸ä¼¼åº¦ï¼›æ¥ç€target å’Œ æ£€ç´¢åˆ°çš„sample è¿›è¡Œconcatï¼Œå¾—åˆ°(K+1,F+1,D)çš„ç‰¹å¾ï¼ŒKæ˜¯æ£€ç´¢ä¸ªæ•°ï¼ŒFæ˜¯ç‰¹å¾æ•°ï¼ˆè¿™é‡ŒæŠŠlabelä¹Ÿä½œä¸ºä¸€ä¸ªç‰¹å¾ï¼Œtargetçš„labelå®šä¹‰ä¸º[UNK]ï¼‰ï¼Œé€šè¿‡åç»­çš„intra-blockï¼Œcross-blockå’ŒMLPã€‚å…¶ä¸­intra-blockå’Œcross-blockåˆ†åˆ«æ˜¯ç»è¿‡LNï¼ˆintraæ˜¯åœ¨ç‰¹å¾çº¬åº¦ï¼Œcrossæ˜¯åœ¨æ ·æœ¬çº¬åº¦ï¼‰å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ |

<h4 id="1.3.2">1.3.2 Item Generation Task</h4>

| **Name** | **Paper**                                                                                                 | **LLM Backbone (Largest)** | **LLM Tuning Strategy** |     **Publication**     |                       **Link**                       | Main Contributions                                                                                                                                                                                                                                                                                                                                                        |
| :------------: | :-------------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :----------------------------: | :--------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|    GPT4Rec    | GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation               |           GPT2 (110M)           |        Full Finetuning        |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2304.03879)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     VIP5âœ…     | VIP5: Towards Multimodal Foundation Models for Recommendation                                                   |          T5-base (223M)          |    Layerwise Adater Tuning    |           EMNLP 2023           |          [[Paper]](https://arxiv.org/abs/2305.14302)          | å¦‚ä½•åœ¨ä¸ä¿®æ”¹åŸå§‹ç»“æ„ä¸‹å°†å¤šæ¨¡æ€æ¨èæ¨¡å‹é€‚ç”¨äºå„ç§ä»»åŠ¡å’Œæ¨¡æ€ç¼ºä¹ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºå°†ç‰©å“æ–‡æœ¬ã€å›¾åƒç¼–ç ä¸ºkä¸ªtokenï¼Œå¹¶å†»ç»“P5 backboneï¼Œåªè®­ç»ƒadapterï¼ˆAttentionå’ŒFFNä¹‹åçš„æ–°çŸ©é˜µï¼‰å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚ä¸‹æœ‰ä»»åŠ¡åŒ…æ‹¬ç›´æ¥æ¨èã€åºåˆ—ã€å¯è§£é‡Šã€‚                                                                                                                                          |
|    P5-IDâœ…    | How to Index Item IDs for Recommendation Foundation Models                                                      |          T5-small (60M)          |        Full Finetuning        |         SIGIR-AP 2023         |      [[Code]](https://github.com/Wenyueh/LLM-RecSys-ID)      | å…³æ³¨ç ”ç©¶LLMæ¨èçš„Item ID indexæ–¹å¼ã€‚æå‡ºåºåˆ—Indexã€ååŒIndexã€è¯­ä¹‰Indexå’Œæ··åˆIndexæ–¹å¼ã€‚                                                                                                                                                                                                                                                                                  |
|    FaiRLLM    | Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation                  |             ChatGPT             |            Frozen            |          RecSys 2023          |          [[Paper]](https://arxiv.org/abs/2305.07609)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      PALR      | PALR: Personalization Aware LLMs for Recommendation                                                             |            LLaMA (7B)            |        Full Finetuning        |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2305.07622)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   ChatGPTâœ…   | Large Language Models are Zero-Shot Rankers for Recommender Systems                                             |             ChatGPT             |            Frozen            |           ECIR 2024           |         [[Code]](https://github.com/RUCAIBox/LLMRank)         | æ¢ç´¢LLMä½œä¸ºæ¨èæ’åºæ¨¡å‹çš„èƒ½åŠ›ã€‚å‘ç°ï¼ŒLLMå¯¹å†å²å€™é€‰ç‰©å“é¡ºåºä¸æ•æ„Ÿï¼Œä½¿ç”¨recency-focused promptingå’ŒICLå¯ä»¥æ”¹å–„ï¼›å¢åŠ å†å²ç‰©å“æ•°é‡ä¸ä¸€å®šå¢å¼ºè¡¨ç°ï¼Œå¯èƒ½åè€Œè´Ÿé¢ä½œç”¨ï¼›LLMä¼šå—åˆ°å€™é€‰ç‰©å“ä½ç½®åå·®ã€æµè¡Œåº¦åå·®ã€æ–‡æœ¬ç›¸ä¼¼åº¦å½±å“ã€‚æå‡ºboostrappingï¼ˆæ‰“ä¹±å€™é€‰ç‰©å“ï¼Œæ’åºBæ¬¡ï¼‰å¾—åˆ°æœ€ç»ˆæ’åºç»“æœã€‚Instruction Tuningè¿‡çš„LLMå’Œæ›´å¤§çš„LLMå¯¹äºæå‡æ¨èæ•ˆæœæœ‰å¸®åŠ©ã€‚                            |
|      AGR      | Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT                                  |             ChatGPT             |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2305.04518)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      NIR      | Zero-Shot Next-Item Recommendation using Large Pretrained Language Models                                       |           GPT3 (175B)           |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2304.03153)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     GPTRec     | Generative Sequential Recommendation with GPTRec                                                                |        GPT2-medium (355M)        |        Full Finetuning        |       Gen-IR@SIGIR 2023       |          [[Paper]](https://arxiv.org/abs/2306.11114)          |                                                                                                                                                                                                                                                                                                                                                                           |
|    ChatNews    | A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News            |             ChatGPT             |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2306.10702)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      N/A      | Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences     |            PaLM (62B)            |            Frozen            |          RecSys 2023          |          [[Paper]](https://arxiv.org/abs/2307.14225)          |                                                                                                                                                                                                                                                                                                                                                                           |
|  LLMSeqPrompt  | Leveraging Large Language Models for Sequential Recommendation                                                  |         OpenAI ada model         |           Finetune           |          RecSys 2023          |          [[Paper]](https://arxiv.org/abs/2309.09261)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     GenRec     | GenRec: Large Language Model for Generative Recommendation                                                      |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2307.00457)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     UP5âœ…     | UP5: Unbiased Foundation Model for Fairness-aware Recommendation                                                |          T5-base (223M)          |         Prefix Tuning         |           EACL 2024           |          [[Paper]](https://arxiv.org/abs/2305.12090)          | ç ”ç©¶LLMæ¨èçš„ç”¨æˆ·å…¬å¹³æ€§ã€‚æå‡ºprefix promptå’Œåˆ†ç±»å™¨ä¹‹é—´å¯¹æŠ—è®­ç»ƒå®ç°å…¬å¹³æ€§ï¼›å…¶ä¸­prefix soft promptåˆ†åˆ«åŠ åœ¨encoderå’Œdecoderå‰é¢ï¼Œå¯¹äºå¤šä¸ªæ•æ„Ÿå±æ€§çš„æ··åˆï¼Œä½¿ç”¨target-attentionæœºåˆ¶ç”Ÿæˆå•ä¸€prefix promptã€‚                                                                                                                                                                     |
|     HKFRâœ…     | Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM                        |    ChatGPT+<br />ChatGLM (6B)    |             LoRA             |          RecSys 2023          |          [[Paper]](https://arxiv.org/abs/2308.03333)          | ä¼ ç»Ÿæ–¹æ³•å°†ç”¨æˆ·å¼‚è´¨ä¿¡æ¯ä¸æ¨¡å‹ç»“åˆï¼Œä¼šå‡ºç°ç‰¹å¾ç¨€ç–å’ŒçŸ¥è¯†ç¢ç‰‡åŒ–ï¼ˆç¼ºä¹ä¸åŒè¡Œä¸ºä¹‹é—´å¼‚è´¨çŸ¥è¯†çš„èåˆï¼‰ã€‚æœ¬æ–‡é€šè¿‡LLMå¸®åŠ©æå–ç”¨æˆ·ä¸åŒè¡Œä¸ºä¸­çš„å¼‚è´¨çŸ¥è¯†ï¼Œå¹¶ä½¿ç”¨instruct tuningçš„æ–¹å¼èåˆè¿™äº›çŸ¥è¯†å’Œæ¨èä»»åŠ¡ã€‚                                                                                                                                                                          |
| RecExplainerâœ… | RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability                          |         Vicuna-v1.3 (7B)         |             LoRA             |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2311.10947)          | åˆ©ç”¨å¾®è°ƒçš„LLMä½œä¸ºä»£ç†æ¨¡å‹ï¼Œå­¦ä¹ æ¨¡ä»¿å’Œç†è§£ç›®æ ‡æ¨èæ¨¡å‹ã€‚è®¾è®¡è¡Œä¸ºå¯¹é½å’Œæ„å›¾å¯¹é½ã€‚å‰è€…åŠ¨æœºæ˜¯å¦‚æœLLMèƒ½å¯¹é½ç›®æ ‡æ¨¡å‹ï¼Œé‚£ä¹ˆå°±èƒ½å¤Ÿæ¨¡ä»¿ç›®æ ‡æ¨¡å‹çš„é€»è¾‘åšå‡ºé¢„æµ‹ï¼›åè€…åŠ¨æœºæ˜¯å¦‚æœLLMèƒ½å¤Ÿä¿ç•™å…¶å¤šæ­¥æ¨ç†èƒ½åŠ›çš„åŒæ—¶ç†è§£ç›®æ ‡æ¨¡å‹çš„ç¥ç»å…ƒï¼Œåˆ™æœ‰å¯èƒ½é˜æ˜ç›®æ ‡æ¨¡å‹çš„å†³ç­–é€»è¾‘ã€‚ä¸‹æ¸¸ä»»åŠ¡åŒ…æ‹¬next item retrieval, item ranking, interest classification, and history reconstructionä»¥åŠè§£é‡Šç”Ÿæˆã€‚ |
|      N/A      | The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations      |             ChatGPT             |            Frozen            |           EAAMO 2023           |          [[Paper]](https://arxiv.org/abs/2308.02053)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     BIGRecâœ…     | A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems                                |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |          [[Code]](https://github.com/SAI990323/Grounding4Rec)          | å·²æœ‰æ–¹æ³•ç¼ºä¹æ¢ç´¢LLM-basedæ¨èç®—æ³•åœ¨all-rankingä¸‹çš„è¡¨ç°ã€‚æœ¬æ–‡æå‡ºä¸¤æ­¥åŸºç¡€å¤§æ¨¡å‹ï¼Œé¦–å…ˆåœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œinstruction tuningï¼Œç¬¬äºŒæ­¥æ ¹æ®LLMè¾“å‡ºembeddingå’Œç‰©å“embeddingçš„æµè¡Œåº¦åŠ æƒæ¬§å¼è·ç¦»è¿›è¡Œæ’åºã€‚ |
|     KP4SR     | Knowledge Prompt-tuning for Sequential Recommendation                                                           |          T5-small (60M)          |        Full Finetuning        |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2308.08459)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   RecSysLLM   | Leveraging Large Language Models for Pre-trained Recommender Systems                                            |            GLM (10B)            |             LoRA             |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2308.10837)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     PODâœ…     | Prompt Distillation for Efficient LLM-based Recommendation                                                      |          T5-small (60M)          |        Full Finetuning        |           CIKM 2023           |         [[Code]](https://github.com/lileipisces/POD)         | å·²æœ‰LLM4Recéœ€è¦å°†ç”¨æˆ·å’Œç‰©å“ä¿¡æ¯åµŒå…¥åˆ°ç»™å®šçš„ç¦»æ•£æ¨¡ç‰ˆä¸­ï¼Œä½†ç”±äºå¤§é‡çš„ä»»åŠ¡ä¿¡æ¯æè¿°å¯èƒ½å¯¼è‡´å¦‚LLMè¯¯è§£ä¸ºå¦ä¸€ä¸ªä»»åŠ¡ï¼Œæˆ–è€…æ©ç›–äº†å…³é”®ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸ºæ¯ä¸ªä»»åŠ¡è®¾è®¡è¿ç»­promptï¼Œè®­ç»ƒæ—¶è¿ç»­å’Œç¦»æ•£promptåŒæ—¶å‡ºç°ï¼Œç›®çš„æ˜¯å°†ä»»åŠ¡çŸ¥è¯†è’¸é¦åˆ°è¿ç»­promptä¸­ï¼ˆæ¨æ–­æ—¶åªä¿ç•™è¿ç»­promptï¼‰ï¼›æ­¤å¤–è®¾è®¡ä»»åŠ¡äº¤æ›¿è®­ç»ƒæ–¹æ³•ï¼Œé˜²æ­¢ä¸åŒä»»åŠ¡å¥å­é•¿åº¦ä¸åŒå¯¼è‡´padæ—¶é—´è¿‡é•¿ã€‚                                     |
|      N/A      | Evaluating ChatGPT as a Recommender System: A Rigorous Approach                                                 |             ChatGPT             |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2309.03613)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      RaRS      | Retrieval-augmented Recommender System: Enhancing Recommender Systems with Large Language Models                |             ChatGPT             |            Frozen            | RecSys Doctoral Symposium 2023 | [[Paper]](https://dl.acm.org/doi/abs/10.1145/3604915.3608889) |                                                                                                                                                                                                                                                                                                                                                                           |
|   JobRecoGPT   | JobRecoGPT -- Explainable job recommendations using LLMs                                                        |               GPT4               |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2309.11805)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     LANCER     | Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling |           GPT2 (110M)           |         Prefix Tuning         |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2309.10435)          |                                                                                                                                                                                                                                                                                                                                                                           |
|    TransRec    | A Multi-facet Paradigm to Bridge Large Language Model and Recommendation                                        |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2310.06491)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   AgentCFâœ…   | AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems                         | text-davinci-003 & gpt-3.5-turbo |            Frozen            |            WWW 2024            |          [[Paper]](https://arxiv.org/abs/2310.09233)          | å°†ç”¨æˆ·å’Œç‰©å“éƒ½ä½œä¸ºagentå»ºæ¨¡åŒè¾¹å…³ç³»å¹¶å®ç°ååŒä¼˜åŒ–å„è‡ªçš„memoryï¼Œreflectionç­‰å†…éƒ¨ç»“æ„                                                                                                                                                                                                                                                                                       |
|      P4LM      | Factual and Personalized Recommendations using Language Models and Reinforcement Learning                       |             PaLM2-XS             |        Full Finetuning        |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2310.06176)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   InstructMK   | Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model                        |            LLaMA (7B)            |        Full Finetuning        |        CIKM GenRec 2023        |          [[Paper]](https://arxiv.org/abs/2310.16409)          |                                                                                                                                                                                                                                                                                                                                                                           |
|    LightLM    | LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation                             |          T5-small (60M)          |        Full Finetuning        |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2310.17488)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   LlamaRecâœ…   | LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking                                      |           LLaMA2 (7B)           |             QLoRA             |         PGAI@CIKM 2023         |          [[Paper]](https://arxiv.org/abs/2311.02089)          | LLMç”±äºè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆå¯¼è‡´æ¨æ–­å»¶æ—¶é«˜ã€‚æœ¬æ–‡æå‡ºä¸¤é˜¶æ®µæ’åºæ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µæ˜¯ç”¨å°è§„æ¨¡æ¨¡å‹ç”Ÿæˆå€™é€‰ç‰©å“ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯ç”¨LLMæ ¹æ®è¯­è¨€å™¨çš„æ–¹æ³•å°†è¾“å‡ºlogitså˜ä¸ºå€™é€‰ç‰©å“æ¦‚ç‡ã€‚LLMä½¿ç”¨æŒ‡ä»¤æ•°æ®å¾®è°ƒï¼Œåªå¾®è°ƒindex letterå’ŒEOS tokenã€‚è¿™é‡Œindex letteræ˜¯æŒ‡promptä¸­ä¼šå½¢æˆå¦‚"(A) item title"ä¸­çš„Aã€‚                                                                                             |
|      N/A      | Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study                                |              GPT-4V              |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2311.04199)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      N/A      | Exploring Fine-tuning ChatGPT for News Recommendation                                                           |             ChatGPT             |  gpt-3.5-urbo finetuning API  |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2311.05850)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      N/A      | Do LLMs Implicitly Exhibit User Discrimination in Recommendation? An Empirical Study                            |             ChatGPT             |            Frozen            |           Arxiv 2023           |          [[Code]](https://github.com/ljy0ustc/LLaRA)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     LC-Rec     | Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation                        |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2311.09049)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      DOKE      | Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations                          |             ChatGPT             |            Frozen            |           Arxiv 2023           |          [[Code]](https://github.com/ljy0ustc/LLaRA)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   ControlRec   | ControlRec: Bridging the Semantic Gap between Language Model and Personalized Recommendation                    |          T5-base (223M)          |        Full Finetuning        |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2311.16441)          |                                                                                                                                                                                                                                                                                                                                                                           |
|    LLaRAâœ…    | LLaRA: Large Language-Recommendation Assistant       |           LLaMA2 (7B)           |             LoRA             |           SIGIR 2024           |          [[Paper]](https://arxiv.org/abs/2312.02445)          | ä»…åˆ©ç”¨IDæˆ–è€…æ–‡æœ¬ï¼Œæ— æ³•å……åˆ†å‘æŒ¥ä¸–ç•ŒçŸ¥è¯†å’Œè¡Œä¸ºåºåˆ—æ¨¡å¼ã€‚æœ¬æ–‡å°†ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¾—åˆ°çš„embeddingå’Œæ–‡æœ¬æè¿°embeddingéƒ½æ”¾åœ¨promptä¸­ï¼›æ­¤å¤–ï¼Œè®¾è®¡è¯¾ç¨‹å­¦ä¹ ä»ç®€å•ä»»åŠ¡ï¼ˆçº¯æ–‡æœ¬ï¼‰åˆ°éš¾ä»»åŠ¡ï¼ˆæ–‡æœ¬+IDæ··åˆï¼‰å®ç°åºåˆ—è¡Œä¸ºçŸ¥è¯†è’¸é¦åˆ°LLM                                                                                                                                                            |
|     PO4ISRâœ…     | Large Language Models for Intent-Driven Session Recommendations                                                 |             ChatGPT             |            Frozen            |           SIGIR 2024           |          [[Code]](https://github.com/llm4sr/PO4ISR)          | ä¼ ç»Ÿæ–¹æ³•å‡è®¾æ‰€æœ‰ä¼šè¯ä¸­çš„æ„å›¾æ•°é‡å‡åŒ€ã€‚è¿™ä¸€å‡è®¾å¿½ç•¥äº†ç”¨æˆ·ä¼šè¯çš„åŠ¨æ€æ€§è´¨ï¼Œå…¶ä¸­æ„å›¾çš„æ•°é‡å’Œç±»å‹å¯ä»¥æ˜¾è‘—å˜åŒ–ï¼Œå…¶æ¬¡åœ¨éšç©ºé—´ä¸­å­¦ä¹ æ„å›¾è¡¨ç¤ºï¼Œè¿™ä¸å…·å¤‡å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡æå‡ºé€šè¿‡Prompt Initialization->Prompt Optimization->Prompt Selectionæ–¹æ³•åœ¨ä¼šè¯å†…åŠ¨æ€ç†è§£ä¼šè¯çº§åˆ«çš„è¯­ä¹‰ç”¨æˆ·æ„å›¾ï¼Œå®ç°Promptçš„è‡ªåŠ¨ä¼˜åŒ–ï¼Œå¹¶åˆ©ç”¨UCB-Banditsæ–¹å¼æ‹©ä¼˜ã€‚ |
|      DRDT      | DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation                        |             ChatGPT             |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.11336)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   RecPrompt   | RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models                        |               GPT4               |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.10463)          |                                                                                                                                                                                                                                                                                                                                                                           |
|      LiT5      | Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models            |            T5-XL (3B)            |        Full Finetuning        |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.16098)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     STELLA     | Large Language Models are Not Stable Recommender Systems                                                        |             ChatGPT             |            Frozen            |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.15746)          |                                                                                                                                                                                                                                                                                                                                                                           |
|   Llama4Rec   | Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation          |           LLaMA2 (7B)           |        Full Finetuning        |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.13870)          |                                                                                                                                                                                                                                                                                                                                                                           |
|     VSTâœ…     | Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models                                          |      GPT4<br />LLaVA (13B)      |            Frozen            |           Arxiv 2024           |        [[Paper]](https://arxiv.org/pdf/2402.08670.pdf)        | large vision-language model (LVLM)ç¼ºä¹ç”¨æˆ·åå¥½çŸ¥è¯†ï¼Œéš¾ä»¥è§£å†³åŒ…å«å¤§é‡ç¦»æ•£ã€å™ªå£°ã€å†—ä½™çš„å›¾åƒåºåˆ—ã€‚æœ¬æ–‡æå‡ºVSTæ¨¡å‹é€šè¿‡ä»ç‰©å“çš„å…³é”®å›¾åƒä¸­æå–æ€»ç»“ä¿¡æ¯ï¼ˆåˆ©ç”¨LVLMï¼‰ï¼Œå†å°†è¿™äº›ä¿¡æ¯å’Œç‰©å“titleæ‹¼æ¥ä½œä¸ºç”¨å“æ–‡æœ¬è¡¨ç¤ºã€‚                                                                                                                                                              |
| IDGenRecâœ… | Towards LLM-RecSys Alignment with Textual ID Learning | T5-base (223M) | Full Finetuning | SIGIR 2024 | [[Code]](https://github.com/agiresearch/IDGenRec) | ç°æœ‰æ–¹æ³•éš¾ä»¥ç¼–ç ç®€ä»‹ä¸”æœ‰æ„ä¹‰çš„ç‰©å“è¡¨ç¤ºåˆ°text-to-textæ¡†æ¶ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨ID generatorï¼ˆä½¿ç”¨åœ¨article tag generation taskä¸Šè®­ç»ƒçš„T5 smallä½œä¸ºåˆå§‹åŒ–ï¼‰å¯¹ç‰©å“æè¿°ç”Ÿæˆæœ‰æ„ä¹‰çš„åºåˆ—IDï¼›è€Œå¯¹äºbase modelï¼Œé‡‡ç”¨P5ï¼›è®­ç»ƒé‡‡ç”¨äº¤æ›¿è®­ç»ƒæ–¹å¼ã€‚æ­¤å¤–ï¼Œä¸ºäº†é˜²æ­¢IDé‡å¤å’Œç¡®ä¿ç”ŸæˆIDå¯¹åº”åˆ°å­˜åœ¨çš„ç‰©å“ï¼Œåˆ†åˆ«è®¾è®¡å¤šæ ·åŒ–IDç”Ÿæˆæœºåˆ¶ï¼ˆé‡‡ç”¨diverse beam searchæŠ€å·§ï¼‰å’Œå—é™å‰ç¼€æ•°å®ç°ã€‚ |

<h4 id="1.3.3">1.3.3 Hybrid Task</h4>

| **Name** | **Paper**                                                                                              | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |                **Link**                | Main Contributions                                                                                                                                                                                           |
| :------------: | :----------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :-------------------: | :------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
|      P5âœ…      | Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5) |          T5-base (223M)          |        Full Finetuning        |      RecSys 2022      |    [[Code]](https://github.com/jeykigung/P5)    | ä½¿ç”¨ç›¸åŒçš„è¯­è¨€å»ºæ¨¡ç›®æ ‡å®ç°ç»Ÿä¸€çš„æ¨èå¼•æ“ï¼ˆåºåˆ—æ¨è+ç›´æ¥æ¨è+è§£é‡Šç”Ÿæˆ+è¯„è®ºç›¸å…³+è¯„åˆ†é¢„æµ‹ï¼‰ï¼Œå®ç°åŸºäºpromptçš„æŒ‡ä»¤æ¨è                                                                                           |
|    M6-Recâœ…    | M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems                             |          M6-base (300M)          |         Option Tuning         |      Arxiv 2022      |   [[Paper]](https://arxiv.org/abs/2205.08084)   | ç”¨æˆ·è¡Œä¸ºè½¬åŒ–ä¸ºçº¯æ–‡æœ¬ï¼Œè®­ç»ƒä»»åŠ¡åŒ…æ‹¬CTR/CVR+è§£é‡Šç”Ÿæˆ+queryç”Ÿæˆ+å¯¹è¯æ¨è+äº§å“ç”Ÿæˆ+æ£€ç´¢ä»»åŠ¡ç­‰ï¼ˆæ— IDï¼‰ã€‚åŒæ—¶è®¾è®¡å¤šä¸ªä¼˜åŒ–æ“ä½œï¼Œå¦‚option tuningç­‰                                                                   |
| InstructRecâœ… | Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach            |         FLAN-T5-XL (3B)         |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2305.07001)   | è®¾è®¡39ç²—ç²’åº¦instruction promptï¼ˆPreference+Intention+Task Form+Contextï¼‰ç”¨äºä¸åŒåœºæ™¯ï¼Œå¹¶åŸºäºæ­¤è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªç»†ç²’åº¦ä¸ªæ€§åŒ–instructionï¼ˆæ— IDï¼‰ï¼Œå°†æ¨èä»»åŠ¡è½¬åŒ–ä¸ºinstruction followingèŒƒå¼ï¼Œå®ç°ç”¨æˆ·è‡ªç”±åœ°è¡¨è¾¾éœ€æ±‚ |
|   ChatGPTâœ…   | Is ChatGPT a Good Recommender? A Preliminary Study                                                           |             ChatGPT             |            Frozen            |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2304.10149)   | æ¢æµ‹ChatGPTåœ¨Aè¯„åˆ†é¢„æµ‹ã€Bç›´æ¥æ¨èã€Cåºåˆ—æ¨èã€Dè§£é‡Šç”Ÿæˆå’ŒEè¯„è®ºæ€»ç»“äº”æ–¹é¢çš„èƒ½åŠ›ã€‚ç»“è®ºæ˜¯Aã€Då’ŒEè¡¨ç°è¾ƒä½³ï¼Œä½†æ˜¯Bå’ŒCè¡¨ç°å·®                                                                                        |
|   ChatGPTâœ…   | Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent                           |             ChatGPT             |            Frozen            |      Arxiv 2023      | [[Code]](https://github.com/sunnweiwei/RankGPT) | å·²æœ‰å·¥ä½œç¼ºä¹å¯¹LLMå¯¹äºæ–‡æ¡£é‡æ’èƒ½åŠ›çš„æ¢ç´¢ï¼Œæœ¬æ–‡éªŒè¯äº†GPT4ç›¸æ¯”äºSOTAçš„ä¼˜è¶Šæ€§ï¼Œæ­¤å¤–æœ¬æ–‡ä¹Ÿå°†ChatGPTæ’åºèƒ½åŠ›è’¸é¦åˆ°å°æ¨¡å‹ï¼Œå®ç°æ€§èƒ½çš„æå‡                                                                          |
|   ChatGPTâœ…   | Uncovering ChatGPT's Capabilities in Recommender Systems                                                     |             ChatGPT             |            Frozen            |      RecSys 2023      |  [[Code]](https://github.com/rainym00d/LLM4RS)  | æ¢æµ‹ChatGPTåœ¨point-wiseï¼Œpair-wiseå’Œlist-wiseä¸‹çš„æ¨èæ€§èƒ½                                                                                                                                                    |
|     BDLMâœ…     | Bridging the Information Gap Between Domain-Specific Model and General LLM for Personalized Recommendation   |           Vicuna (7B)           |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2311.03778)   | å¤§æ¨¡å‹ä»¥æ–‡æœ¬å½¢å¼è¡¨è¾¾éš¾ä»¥åŒºåˆ†ç›¸ä¼¼ä½†ä»æœ‰å¾®å°åŒºåˆ«çš„å•†å“ï¼Œä¸”éš¾ä»¥è¡¨ç¤ºå¤æ‚çš„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼›ä¼ ç»Ÿdomain-specificæ¨¡å‹éš¾ä»¥åœ¨æ•°æ®ç¨€ç–åœºæ™¯è¡¨ç°å¥½ã€‚æå‡ºä¿¡æ¯å…±äº«æ¨¡å—ï¼Œç”¨æˆ·/ç‰©å“ID tokenæ‰©å……è¯è¡¨ï¼Œè”åˆæ¨¡å‹è®­ç»ƒç­‰æ–¹æ³•ã€‚       |
|  RecRankerâœ…  | RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation                        |           LLaMA2 (13B)           |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2312.16018)   | æ··åˆæ’åºï¼ˆpoint+pair+listï¼‰æŒ‡ä»¤æ„å»ºï¼Œpromptä½ç½®æ¶ˆåï¼Œè‡ªé€‚åº”ç”¨æˆ·é‡‡æ ·ï¼ˆé‡è¦æ€§ã€cluster-basedï¼Œé‡å¤æƒ©ç½šï¼‰ï¼Œæ¨æ–­æ—¶ä½¿ç”¨ä¸‰ç§æ’åºä»»åŠ¡çš„ï¼ˆè°ƒæ•´åï¼‰åˆ†æ•°ä¹‹å’Œã€‚                                                         |

<h3 id="1.4">1.4 LLM for User Interaction</h3>


<h4 id="1.4.1">1.4.1 Task-oriented User Interaction</h4>

| **Name** | **Paper**                                                                                                     | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |              **Link**              | Main Contributions |
| :------------: | :------------------------------------------------------------------------------------------------------------------ | :------------------------------: | :---------------------------: | :-------------------: | :--------------------------------------: | ------------------ |
|   TG-ReDial   | Towards Topic-Guided Conversational Recommender System                                                              |  BERT-base (110M) & GPT2 (110M)  |            Unknown            |      COLING 2020      | [[Paper]](https://arxiv.org/abs/2010.04125) |                    |
|      TCP      | Follow Me: Conversation Planning for Target-driven Recommendation Dialogue Systems                                  |         BERT-base (110M)         |        Full Finetuning        |      Arxiv 2022      | [[Paper]](https://arxiv.org/abs/2208.03516) |                    |
|      MESE      | Improving Conversational Recommendation Systems' Quality with Context-Aware Item Meta-Information                   |  DistilBERT (67M) & GPT2 (110M)  |        Full Finetuning        |       ACL 2022       | [[Paper]](https://arxiv.org/abs/2112.08140) |                    |
|    UniMIND    | A Unified Multi-task Learning Framework for Multi-goal Conversational Recommender Systems                           |         BART-base (139M)         |        Full Finetuning        |     ACM TOIS 2023     | [[Paper]](https://arxiv.org/abs/2204.06923) |                    |
|     VRICR     | Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation                            |         BERT-base (110M)         |        Full Finetuning        |       WSDM 2023       | [[Paper]](https://arxiv.org/abs/2212.11868) |                    |
|      KECR      | Explicit Knowledge Graph Reasoning for Conversational Recommendation                                                |  BERT-base (110M) & GPT2 (110M)  |            Frozen            |     ACM TIST 2023     | [[Paper]](https://arxiv.org/abs/2305.00783) |                    |
|      N/A      | Large Language Models as Zero-Shot Conversational Recommenders                                                      |               GPT4               |            Frozen            |       CIKM 2023       | [[Paper]](https://arxiv.org/abs/2308.10053) |                    |
|    MuseChat    | MuseChat: A Conversational Music Recommendation System for Videos                                                   |           Vicuna (7B)           |             LoRA             |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2310.06282) |                    |
|      N/A      | Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue |       Chinese-Alpaca (7B)       |             LoRA             |  EMNLP 2023 Findings  | [[Paper]](https://arxiv.org/abs/2310.14626) |                    |

<h4 id="1.4.2">1.4.2 Open-ended User Interaction</h4>

| **Name** | **Paper**                                                                                  | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |              **Link**              | Main Contributions |
| :------------: | :----------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :-------------------: | :--------------------------------------: | ------------------ |
|     BARCOR     | BARCOR: Towards A Unified Framework for Conversational Recommendation Systems                    |         BART-base (139M)         |  Selective-layer Finetuning  |      Arxiv 2022      | [[Paper]](https://arxiv.org/abs/2203.14257) |                    |
|   RecInDial   | RecInDial: A Unified Framework for Conversational Recommendation with Pretrained Language Models |         DialoGPT (110M)         |        Full Finetuning        |       AACL 2022       | [[Paper]](https://arxiv.org/abs/2110.07477) |                    |
|     UniCRS     | Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning        |      DialoGPT-small (176M)      |            Frozen            |       KDD 2022       | [[Paper]](https://arxiv.org/abs/2206.09363) |                    |
|     T5-CR     | Multi-Task End-to-End Training Improves Conversational Recommendation                            |          T5-base (223M)          |        Full Finetuning        |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2305.06218) |                    |
|      TtW      | Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation                 |  T5-base (223M) & T5-XXL (11B)  |   Full Finetuning & Frozen   |      Arxiv 2023      |                                          |                    |
|      N/A      | Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models  |             ChatGPT             |            Frozen            |      EMNLP 2023      | [[Paper]](https://arxiv.org/abs/2305.13112) |                    |

<h3 id="1.5">1.5 LLM for RS Pipeline Controller</h3>


| **Name** | **Paper**                                                                                  | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |              **Link**              | Main Contributions                                                                                                                                                                                                                                                                                                                                |
| :------------: | :----------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :-------------------: | :--------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|    Chat-REC    | Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System                  |             ChatGPT             |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2303.14524) |                                                                                                                                                                                                                                                                                                                                                   |
|     RecLLM     | Leveraging Large Language Models in Conversational Recommender Systems                           |            LLaMA (7B)            |        Full Finetuning        |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2305.07961) |                                                                                                                                                                                                                                                                                                                                                   |
|      RAHâœ…      | RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models |               GPT4               |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2308.09904) | å°†æ•´ä¸ªç³»ç»Ÿåˆ†ä¸ºæ¨èç³»ç»Ÿ(R)ã€åŠ©æ‰‹(A)å’Œäºº(H)ã€‚æœ¬æ–‡æ ¸å¿ƒåœ¨äºè®¾è®¡åŠ©æ‰‹å®ç°ä»¥äººä¸ºä¸­å¿ƒã€‚æˆ‘ä»¬é€šè¿‡learn-action-criticå’Œ reflectionæœºåˆ¶æ¥å®ç°åŠ©æ‰‹ï¼Œä»¥æ”¹å–„å­¦ä¹ åˆ°çš„ä¸ªæ€§åŒ–ã€‚æ¯ä¸ªåŠ©æ‰‹ç”±å¤šä¸ªåŸºäºLLMçš„agentç»„æˆï¼Œä¸»è¦åŒ…æ‹¬ï¼ˆ1ï¼‰learn agentï¼šä»ç”¨æˆ·äº¤äº’ä¸­å­¦ä¹ ç”¨æˆ·ä¸ªæ€§å¹¶ä¿å­˜åˆ°ä¸ªæ€§åº“ä¸­ï¼ˆ2ï¼‰action agentï¼šæ ¹æ®å­¦ä¹ åˆ°çš„ä¸ªæ€§åšå‡ºåŠ¨ä½œï¼Œä¾‹å¦‚è·¨å¹³å°è¿‡æ»¤é¡¹ç›®ï¼ˆ3ï¼‰ ) Critic Agentï¼šéªŒè¯æ‰€åšçš„åŠ¨ä½œæ˜¯å¦ä¸ç”¨æˆ·ä¸€è‡´ï¼Œå¹¶åˆ†æå¦‚ä½•æ ¡å‡†ã€‚ (4) Reflect Agentï¼šæ£€æŸ¥å’Œä¼˜åŒ–ç´¯ç§¯å­¦ä¹ çš„ä¸ªæ€§ï¼Œä¾‹å¦‚è§£å†³é‡å¤å’Œå†²çªã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†learnã€actionå’Œcritic agentï¼Œä»¥ç¡®ä¿å­¦ä¹ åˆ°çš„ä¸ªæ€§ä¸ç”¨æˆ·æ›´åŠ ä¸€è‡´ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä¸‰ä¸ªä»£ç†åº”è¯¥é€’å½’åœ°å·¥ä½œï¼Œç›´åˆ°åŠ¨ä½œä»£ç†å–ä»£å­¦ä¹ åˆ°çš„ä¸ªæ€§å¹¶åå‘æ¨æ–­å‡ºæ­£ç¡®çš„ç”¨æˆ·äº¤äº’1 |
|   RecMindâœ…   | RecMind: Large Language Model Powered Agent For Recommendation                                   |             ChatGPT             |            Frozen            |    PGAI@CIKM 2023    | [[Paper]](https://arxiv.org/abs/2308.14296) | ç¼ºä¹å…³äºLLMå¦‚ä½•å›åº”ä¸ªæ€§åŒ–æŸ¥è¯¢ï¼ˆå¦‚æ¨èè¯·æ±‚ï¼‰çš„ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºLLMèµ‹èƒ½çš„æ¨èæ™ºèƒ½ä½“ï¼Œé€šè¿‡è®¡åˆ’ï¼Œåˆ©ç”¨å·¥å…·è·å–å¤–éƒ¨çŸ¥è¯†ï¼Œåˆ©ç”¨ç§äººæ•°æ®å®ç°ç²¾å‡†ä¸ªæ€§åŒ–æ¨èã€‚ä¸»è¦æå‡ºç®—æ³•ä¸ºâ€œSelf- Inspiringâ€ï¼Œè€ƒè™‘ä¹‹å‰æ¢ç´¢è¿‡çš„çŠ¶æ€æŒ‡å¯¼ä¸‹ä¸€æ­¥ï¼ˆä¸åŒäºToTå’ŒCoTé—å¼ƒäº†ä¹‹å‰æ¢ç´¢è¿‡çš„åˆ†æ”¯ï¼‰ã€‚åœ¨ä¸‹æ¸¸å¤šä¸ªä»»åŠ¡ï¼ˆè¯„åˆ†é¢„æµ‹ã€åºåˆ—æ¨èã€ç›´æ¥æ¨èã€è§£é‡Šç”Ÿæˆã€è¯„è®ºæ€»ç»“ï¼‰å®ç°ä¸P5ç›¸å½“çš„æ€§èƒ½ã€‚ |
|  InteRecAgent  | Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations          |               GPT4               |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2308.16505) |                                                                                                                                                                                                                                                                                                                                                   |
|      CORE      | Lending Interaction Wings to Recommender Systems with Conversational Agents                      |               N/A               |              N/A              |       NIPS 2023       | [[Paper]](https://arxiv.org/abs/2310.04230) |                                                                                                                                                                                                                                                                                                                                                   |
| LLMCRSâœ… | A Large Language Model Enhanced Conversational Recommender System | LLaMA2 (7B) | Full Finetuning | Arxiv 2023 | [[Paper]](https://arxiv.org/abs/2308.06212) | å¯¹è¯æ¨èç”±å¤šä¸ªå­ä»»åŠ¡æ„æˆï¼Œå¦‚ç”¨æˆ·åå¥½è¯±å¼•ã€æ¨èã€è§£é‡Šã€ç‰©å“æœç´¢ã€‚ç°æœ‰CRSæ— æ³•é«˜æ•ˆåœ°ç®¡ç†å¤šä¸ªå­ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨LLMç®¡ç†å„ä¸ªå­ä»»åŠ¡ï¼Œå¯¹äºå­ä»»åŠ¡çš„è§£å†³ï¼Œæ˜¯é€šè¿‡LLMè°ƒç”¨ä¸“å®¶æ¨¡å‹ï¼Œæœ€åä½¿ç”¨LLMä½œä¸ºè¯­è¨€æ¥å£ä¸ç”¨æˆ·å¯¹è¯ã€‚å…·ä½“å®ç°åŒ…æ‹¬åŸºäºè§„åˆ’çš„æŒ‡ä»¤ã€åŸºäºæ ·ä¾‹çš„æŒ‡ä»¤ã€åŠ¨æ€å­ä»»åŠ¡å’Œæ¨¡å‹åŒ¹é…ã€åŸºäºæ€»ç»“çš„å¯¹è¯ç”Ÿæˆã€‚ä½¿ç”¨åŸºäºæ¨¡å‹è¡¨ç°åé¦ˆçš„RLæœºåˆ¶ä¼˜åŒ–LLMã€‚ |
| MACRecâœ… | Multi-Agent Collaboration Framework for Recommender Systems |  | Frozen | SIGIR 2024 | [[Code]](https://github.com/wzf2000/MACRec) | ä»‹ç»äº†ä¸€ä¸ªåä¸ºMACRecçš„å¤šæ™ºèƒ½ä½“åä½œæ¨èç³»ç»Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸åŒæ™ºèƒ½ä½“çš„èƒ½åŠ›å’Œåä½œæ¥è§£å†³æ¨èä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯„åˆ†é¢„æµ‹ã€é¡ºåºæ¨èã€å¯¹è¯æ¨èå’Œè§£é‡Šç”Ÿæˆç­‰ã€‚æ¯ä¸ªæ™ºèƒ½ä½“æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¦‚ä»»åŠ¡è§£é‡Šå™¨ã€ç»ç†ã€åæ€è€…ã€æœç´¢è€…å’Œç”¨æˆ·/ç‰©å“åˆ†æå¸ˆï¼Œä»¥ååŒå®Œæˆä»»åŠ¡ã€‚æ¡†æ¶æä¾›äº†å¯å®šåˆ¶çš„æ™ºèƒ½ä½“å’Œæœ‰ç”¨çš„å·¥å…·ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è¿›è¡Œæ¨èä»»åŠ¡ã€‚ |
| ToolRecâœ… | Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning | ChatGPT | Frozen | SIGIR 2024 | [[Code]](https://github.com/Go0day/ToolRec-Code) | ç°æœ‰çš„åŸºäºLLMçš„RSä¼šå‡ºç°å¹»è§‰ã€ç‰©å“è¯­ä¹‰ç©ºé—´å’Œç”¨æˆ·è¡Œä¸ºç©ºé—´ä¹‹é—´çš„é”™ä½ï¼Œæˆ–è€…è¿‡äºç®€å•çš„æ§åˆ¶ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºToolRecï¼Œé€šè¿‡å·¥å…·å­¦ä¹ çš„LLMèµ‹èƒ½æ¨èçš„æ¡†æ¶ï¼Œå®ƒä½¿ç”¨LLMä½œä¸ºä»£ç†ç”¨æˆ·ï¼Œä»è€ŒæŒ‡å¯¼æ¨èè¿‡ç¨‹å¹¶è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥ç”Ÿæˆä¸ç”¨æˆ·ç»†å¾®åå¥½å¯†åˆ‡ç›¸å…³çš„æ¨èåˆ—è¡¨ã€‚ä½¿ç”¨CoTæŠ€æœ¯å®ç°ç”¨æˆ·å†³ç­–æ¨¡æ‹Ÿï¼Œå¼•å…¥é¢å‘ç‰©å“å±æ€§çš„æ£€ç´¢å’Œæ’åºå·¥å…·ï¼Œå‰è€…ä¼šåŒ…æ‹¬attribute-specific encoderå’Œdense layerçš„å¯è®­ç»ƒå‚æ•°ï¼ˆèåˆè¯­ä¹‰å’Œè¡Œä¸ºçŸ¥è¯†ï¼‰ï¼Œåè€…æ˜¯promptå®ç°ã€‚ |

<h3 id="1.6">1.6 Other Related Papers</h3>


<h4 id="1.6.1">1.6.1 Related Survey Papers</h4>

| **Paper**                                                                                                                |        **Publication**        |              **Link**              | Main Contributions |
| :----------------------------------------------------------------------------------------------------------------------------- | :---------------------------------: | :--------------------------------------: | ------------------ |
| Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis                      |             Arixv 2024             | [[Paper]](https://arxiv.org/abs/2401.04997) |                    |
| User Modeling in the Era of Large Language Models: Current Research and Future Directions                                      | IEEE Data Engineering Bulletin 2023 | [[Paper]](https://arxiv.org/abs/2312.11518) |                    |
| A Survey on Large Language Models for Personalized and Explainable Recommendations                                             |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2311.12338) |                    |
| Large Language Models for Generative Recommendation: A Survey and Visionary Discussions                                        |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2309.01157) |                    |
| Large Language Models for Information Retrieval: A Survey                                                                      |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2308.07107) |                    |
| When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities                                  |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2307.16376) |                    |
| Recommender Systems in the Era of Large Language Models (LLMs)                                                                 |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2307.02046) |                    |
| A Survey on Large Language Models for Recommendation                                                                           |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2305.19860) |                    |
| Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems |              TACL 2023              | [[Paper]](https://arxiv.org/abs/2302.03735) |                    |
| Self-Supervised Learning for Recommender Systems: A Survey                                                                     |              TKDE 2022              | [[Paper]](https://arxiv.org/abs/2203.15876) |                    |

<h4 id="1.6.2">1.6.2 Other Papers</h4>

| **Name**                                                                 | **Paper**                                                    | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** | **Link**                                                     | Main Contributions                                           |
| :-------------------------------------------------------------------------------------- | :-------------------: | :--------------------------------------------------------: | ------------------ | --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
|                | Large Language Model Can Interpret Latent Space of Sequential Recommender               |                |                |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2310.20487)          |                    |
|  | Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging |  |  |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2309.01026)          |                    |
|   | INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning  |   |   |      Arxiv 2024      |          [[Paper]](https://arxiv.org/abs/2401.06532)          |                    |
|                  | Evaluation of Synthetic Datasets for Conversational Recommender Systems                 |                  |                  |      Arxiv 2023      |         [[Paper]](https://arxiv.org/abs/2212.08167v1)         |                    |
|                  | Generative Recommendation: Towards Next-generation Recommender Paradigm                 |                  |                  |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2304.03516)          |                    |
|                | Towards Personalized Prompt-Model Retrieval for Generative Recommendation               |                |                |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2308.02205)          |                    |
|                                                    | Generative Next-Basket Recommendation                                                   |                                                    |                                                    |      RecSys 2023      | [[Paper]](https://dl.acm.org/doi/abs/10.1145/3604915.3608823) |                    |
| PPTODâœ… | Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System | ChatGPT | Frozen | CIKM 2023 | [[Paper]](https://arxiv.org/abs/2306.09821) | å¯¹ç›‘ç£å¾®è°ƒåçš„å¯¹è¯æ¨¡å‹ï¼Œä½¿ç”¨LLMå¯¹è¾“å‡ºè¿›è¡Œæ»¡æ„åº¦è¯„åˆ†ï¼Œç„¶åå¥—ç”¨PPOä¼˜åŒ–æ–¹æ³•æ‰§è¡Œæ¨¡å‹å‚æ•°æ›´æ–°ï¼Œæœ¬è´¨æ˜¯å°†LLMä½œä¸ºannotation-free user simulator |
| LoRecâœ… | LoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks | Llama2 <br />(7B) | Frozen | SIGIR 2024 | [[Code]](https://anonymous.4open.science/r/LoRec/README.md) | å·²æœ‰æ¨èé˜²å¾¡ç®—æ³•ä¾èµ–äºå¯¹æŸç§æ”»å‡»ç‰¹å®šçš„å‡è®¾å’Œè§„åˆ™ï¼Œé™åˆ¶äº†å¯¹æœªçŸ¥æ”»å‡»ç±»å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§æ¨¡å‹é€šç”¨çŸ¥è¯†å’Œåºåˆ—æ¨¡å‹ä¸“æœ‰çŸ¥è¯†èåˆå¢å¼ºæ¨èæ¨¡å‹å¯¹æ¨èæ”»å‡»çš„é˜²å¾¡èƒ½åŠ›ã€‚å…·ä½“åœ°ï¼Œä¸€æ–¹é¢é€šè¿‡è®¾è®¡prompt å¾—åˆ°åŒ…å«é€šç”¨çŸ¥è¯†çš„LLM embeddingï¼Œå¦ä¸€æ–¹é¢é€šè¿‡åºåˆ—æ¨¡å‹å¾—åˆ°åŒ…å«å…·ä½“çŸ¥è¯†çš„embeddingã€‚promptçš„LLM è¾“å‡ºä¼šé€šè¿‡MLPï¼Œé¢„æµ‹æ˜¯æ¬ºè¯ˆç”¨æˆ·çš„æ¦‚ç‡ï¼ŒåŒæ—¶prompt ä¼šç»è¿‡å¦å¤–ä¸€ä¸ªçº¬åº¦é™ç»´å±‚ä¸åºåˆ—embeddingè¿›è¡Œç›¸ä¼¼åº¦æœ€å¤§åŒ–ï¼›æ¥ç€ä¼šæ ¹æ®prompt æ¬ºè¯ˆç”¨æˆ·çš„æ¦‚ç‡é€šè¿‡è®¾è®¡çš„è‡ªé€‚åº”é˜ˆå€¼å’Œè¿­ä»£æƒé‡è¡¥å¿æœºåˆ¶ï¼Œæœ€ç»ˆå¯¹ä¸åŒç”¨æˆ·è¿›è¡ŒåŠ æƒè®­ç»ƒã€‚ |
| BiLLPâœ… | Large Language Models are Learnable Planners for Long-Term Recommendation | ChatGPT | Frozen | SIGIR 2024 | [[Code]](https://github.com/jizhi-zhang/BiLLP) | ç”¨äºé•¿æœŸæ¨èçš„å­¦ä¹ å‹è§„åˆ’æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºå®è§‚å­¦ä¹ å’Œå¾®è§‚å­¦ä¹ ä¸¤ä¸ªå±‚æ¬¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„åˆ’èƒ½åŠ›æ¥ä¼˜åŒ–é•¿æœŸæ¨èçš„æ•ˆæœã€‚å®è§‚å­¦ä¹ éƒ¨åˆ†åŒ…æ‹¬Reflectorå’ŒPlannerï¼Œç”¨äºè·å–é«˜çº§æŒ‡å¯¼åŸåˆ™Reelectionï¼Œå’Œç”Ÿæˆå‰ç»æ€§è®¡åˆ’å¹¶ä»¥Thoughtå½¢å¼è¾“å‡ºã€‚å¾®è§‚å­¦ä¹ éƒ¨åˆ†åŒ…æ‹¬åŸºäºLLMçš„Actorå’ŒCriticï¼Œç”¨äºä¸ªæ€§åŒ–æ¨èå’Œè¯„ä¼°ç”¨æˆ·æ»¡æ„åº¦ã€‚æ¯ä¸ªæˆåˆ†éƒ½æœ‰å„è‡ªçš„memoryï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹åŠ¨æ€æ›´æ–°ï¼Œä¸”æ¯æ¬¡é€‰å–éƒ½ä¼šä¾æ®è®¾è®¡çš„è§„åˆ™é€‰å–æœ€ç›¸ä¼¼çš„Kä¸ªæ’å…¥åˆ°promptä¸­ï¼Œç±»æ¯”gradient updateå¢åŠ åŠ¨ä½œæ¦‚ç‡çš„æ–¹æ³•ã€‚ |
| DEALRecâœ… | Data-efficient Fine-tuning for LLM-based Recommendation | Llama2 <br />(7B) | LoRA | SIGIR 2024 | [[Paper]](https://arxiv.org/pdf/2401.17197) | å¯¹å¤§æ¨¡å‹è®­ç»ƒæ•°æ®è¿›è¡Œä¿®å‰ªã€‚è®¾è®¡influence scoreå’Œeffor scoreï¼Œå‰è€…è®¡ç®—æ¯ä¸ªè®­ç»ƒæ ·æœ¬å¯¹ç»éªŒåˆ†é™©çš„å½±å“åˆ†æ•°ï¼ˆä½¿ç”¨å°çš„ä¼ ç»Ÿæ¨¡å‹ä½œä¸ºä»£ç†ï¼‰ï¼Œåè€…ä¸ºäº†å¼¥è¡¥ä¼ ç»Ÿä»£ç†æ¨¡å‹å’ŒLLMä¹‹é—´çš„å·®è·ï¼ˆå› ä¸ºLLMå…·æœ‰ä¸åŒçš„å­¦ä¹ èƒ½åŠ›ï¼‰ï¼Œè®¡ç®—æ¯ä¸ªæ ·æœ¬å¯¼è‡´æ¨¡å‹å‚æ•°æ”¹å˜çš„èŒƒæ•°ã€‚ä»¥äºŒè€…çš„åŠ æƒå’Œä½œä¸ºæ ·æœ¬åˆ†æ•°ï¼Œä½†å¦‚æœè´ªå¿ƒé€‰æ‹©è¿™äº›é«˜åˆ†æ ·æœ¬ä¼šå¯¼è‡´ä½è¦†ç›–ç‡ï¼Œä¸”è¿™äº›æ ·æœ¬å¯èƒ½å…·æœ‰ç›¸ä¼¼æ¨¡å¼å¯¼è‡´æ¬¡ä¼˜çš„æ ·æœ¬é€‰æ‹©ã€‚å› æ­¤ï¼Œæå‡ºåˆ†å±‚æŠ½æ ·çš„è¦†ç›–åº¦å¢å¼ºæ ·æœ¬é€‰æ‹©æ–¹æ³•ã€‚ |
| âœ… | Source Echo Chamber: Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop | ChatGPT | Frozen | Arxiv 2024 | [[Paper]](https://arxiv.org/abs/2405.17998) | ç ”ç©¶å‘ç°ï¼Œå½“äººå·¥ç”Ÿæˆå†…å®¹ï¼ˆHGCï¼‰å’Œäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åŒæ—¶å­˜åœ¨äºæ¨èç³»ç»Ÿä¸­æ—¶ï¼Œæ¨èæ¨¡å‹æ›´å€¾å‘äºæ¨èAIGCè€Œä¸æ˜¯HGCã€‚ä½œè€…é€šè¿‡å®éªŒè¯æ˜äº†è¿™ç§æ¥æºåè§åœ¨ä¸åŒé˜¶æ®µçš„åé¦ˆå¾ªç¯ä¸­ä¼šè¢«æ”¾å¤§ï¼Œä»è€Œå¯¹ç”¨æˆ·çš„æ¨èç»“æœäº§ç”Ÿå½±å“ã€‚è®ºæ–‡åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šHGC  dominateï¼ŒHGC-AIGC coexistï¼ŒAIGC dominanceè¡¨ç¤ºè¿‡å»ã€ç°åœ¨å’Œæœªæ¥çš„çŠ¶æ€ã€‚å¯¹äºæ–¹æ³•éƒ¨åˆ†ï¼Œè®ºæ–‡æå‡ºå¯¹ç‰©å“æ–¹å’Œuser history encoderåˆ†åˆ«æ‰§è¡ŒL1 lossï¼Œåè€…ä¸ºäº†é˜²æ­¢ä¸åŒhistory sequenceè¡¨ç¤ºè¶‹äºä¸€è‡´ï¼Œå¼•å…¥åºåˆ—è¡¨ç¤ºçš„ç†µæœ€å¤§åŒ–æ­£åˆ™é¡¹å‡è½»æ­¤é—®é¢˜ã€‚ |

<h3 id="1.7">1.7 Paper Pending List: to be Added to Our Survey Paper</h3>


|  **Name**  | **Paper**                                                                                                                         | **LLM Backbone (Largest)** | **LLM Tuning Strategy** |     **Publication**     |                       **Link**                       |
| :---------------: | :-------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :----------------------------: | :--------------------------------------------------------: |
|                  | User-Centric Conversational Recommendation: Adapting the Need of User with Large Language Models                                        |                                  |                              | RecSys Doctoral Symposium 2023 | [[Paper]](https://dl.acm.org/doi/abs/10.1145/3604915.3608885) |
|                  | Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation                         |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2310.16738)          |
|                  | Large Language Model based Long-tail Query Rewriting in Taobao Search                                                                   |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2311.03758)          |
|                  | Knowledge Graphs and Pre-trained Language Models enhanced Representation Learning for Conversational Recommender Systems                |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.10967)          |
|                  | Unlocking the Potential of Large Language Models for Explainable Recommendations                                                        |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.15661)          |
|                  | The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective                                                  |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.15524)          |
|                  | Empowering Few-Shot Recommender Systems with Large Language Models -- Enhanced Representations                                          |                                  |                              |          IEEE Access          |          [[Paper]](https://arxiv.org/abs/2312.13557)          |
|                  | dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models         |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.13264)          |
| Logic-Scaffolding | Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs                                      |           Falcon (40B)           |            Frozen            |           WSDM 2024           |          [[Paper]](https://arxiv.org/abs/2312.14345)          |
|                  | Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.04057)          |
|                  | ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback                                        |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.03605)          |
|                  | Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations in Recommender Systems                                    |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.04474)          |
|                  | Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency                           |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.10545)          |
|                  | LLM4Vis: Explainable Visualization Recommendation using ChatGPT                                                                         |                                  |                              |           EMNLP 2023           |          [[Paper]](https://arxiv.org/abs/2310.07652)          |
|                  | Parameter-Efficient Conversational Recommender System as a Language Processing Task                                                     |                                  |                              |           EACL 2024           |          [[Paper]](https://arxiv.org/abs/2401.14194)          |
|                  | Data-efficient Fine-tuning for LLM-based Recommendation                                                                                 |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.17197)          |
|                  | PAP-REC: Personalized Automatic Prompt for Recommendation Language Model                                                                |                                  |                              |           Arxiv 2024           |                                                            |
|                  | From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models              |                                  |                              |           Arxiv 2024           |                                                            |
|                  | Uncertainty-Aware Explainable Recommendation with Large Language Models                                                                 |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2402.03366)          |
|                  | Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism                                      |                                  |                              |            WWW 2024            |                                                            |
|                  | Item-side Fairness of Large Language Model-based Recommendation System                                                                  |                                  |                              |            WWW 2024            |                                                            |
|                   |                                                              |                            |                         |                                |                                                              |

<h2 id="2">2. LLM & Graph</h2>


| **Name** | **Paper**                                                                                             | **LLM Backbone (Largest)** | **LLM Tuning Strategy** |  **Publication**  |                 **Link**                 |                                                                                                                                                                                                                Main Contributions                                                                                                                                                                                                                |
| :-------------: | :---------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :----------------------: | :---------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|   GraphGPTâœ…   | GraphGPT: Graph Instruction Tuning for Large Language Models                                                |           Vicuna (7B)           |            Frozen            |        SIGIR 2024        |    [[Code]](https://github.com/HKUDS/GraphGPT)    |                                                                    å·²æœ‰é¢„è®­ç»ƒå›¾Embeddingä¾èµ–äºä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒï¼Œé™åˆ¶äº†å…¶åœ¨å°‘é‡æ•°æ®æˆ–æ•°æ®ä¸å¯ç”¨çš„åœºæ™¯ã€‚æœ¬æ–‡æå‡ºGraphGPTæ¡†æ¶å¯¹é½LLMå’Œå›¾ç»“æ„çŸ¥è¯†é€šè¿‡ä¸¤é˜¶æ®µçš„æŒ‡ä»¤å¾®è°ƒï¼ŒåŒ…æ‹¬SSLæŒ‡ä»¤ï¼ˆæ–‡æœ¬å’Œå›¾è¡¨ç¤ºåŒ¹é…ï¼‰+å…·ä½“ä»»åŠ¡å›¾æŒ‡ä»¤ï¼ˆèŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ï¼‰ï¼ŒåŒæ—¶åˆ©ç”¨ChatGPTçš„CoTæŠ€æœ¯è’¸é¦å¼€æºLLMï¼›å°†å…·ä½“ä»»åŠ¡æŒ‡ä»¤å’ŒCoTæŒ‡ä»¤æ•°æ®æ··åˆä½œä¸ºæ¨¡å‹å¾®è°ƒæ•°æ®ã€‚                                                                    |
| InstructGraphâœ… | InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment |            LLaMA (7B)            |             LoRA             |        Arxiv 2024        |  [[Paper]](https://arxiv.org/pdf/2402.08785.pdf)  |                                                                                                 èµ‹äºˆLLMå›¾æ¨ç†å’Œå›¾ç”Ÿæˆçš„èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨åå¥½å¯¹é½è§£å†³ç”Ÿæˆå¹»è±¡é—®é¢˜ã€‚ç¬¬ä¸€æ­¥ï¼Œå°†å›¾ç¼–ç ä¸ºcode_likeçš„åŸºæœ¬å˜é‡ï¼Œå¹¶è®¾è®¡å›¾ç»“æ„å»ºæ¨¡ã€å›¾è¯­è¨€å»ºæ¨¡ã€å›¾ç”Ÿæˆå»ºæ¨¡å’Œå›¾æ€ç»´å»ºæ¨¡ä½œä¸ºæŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚ç¬¬äºŒæ­¥ï¼Œä¸ºäº†è§£å†³å›¾æ¨ç†å’Œå›¾ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è±¡ï¼Œåˆ©ç”¨DPOå¯¹é½æ–¹æ³•å‡è½»ã€‚                                                                                                 |
| GraphAdapterâœ… | Can GNN be Good Adapter for LLMs?                                                                           |           LLaMA (13B)           |            Frozen            |         WWW 2024         | [[Paper]](https://arxiv.org/pdf/2402.12984v1.pdf) | Text-attributed Graph(TAG)å¾ˆéš¾æ‰©å±•åˆ°åäº¿çº§åˆ«çš„LLMï¼Œå¿½ç•¥äº†LLMçš„zero-shotèƒ½åŠ›ï¼›ä¸€äº›è‡ªç›‘ç£GNN-LMæ–¹æ³•åˆ†å¼€è®­ç»ƒLMå’ŒGNNï¼Œå¯¼è‡´ç»“æœæ˜¯æ¬¡ä¼˜çš„ã€‚æœ¬æ–‡æå‡ºGraphAdapterå†»ç»“LMï¼Œå°†LMè¾“å‡ºå±‚(Headå‰é¢çš„)æ”¹å˜ä¸ºå¯è®­ç»ƒçš„adapter GNNï¼Œè¡¨ç¤ºå±‚é¢èåˆPLMè¾“å‡ºçš„æ–‡æœ¬è¡¨ç¤ºå’ŒGNNçš„ç»“æ„è¡¨ç¤ºï¼Œåœ¨è¾“å‡ºå±‚é¢ä¹ŸåŠ å…¥çº¯PLMçš„next tokené¢„æµ‹æ¦‚ç‡è¾“å‡ºã€‚ä½¿ç”¨é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ï¼ˆå‚æ•°åŒ…æ‹¬GNN+Fusionæ¨¡å—ï¼‰ï¼Œå¾®è°ƒæ—¶åœ¨PLM-headä¹‹ååŠ å…¥New-Headï¼ˆå‚æ•°åŒ…æ‹¬GNN+Fusion+new-headï¼‰ã€‚ä¸‹æ¸¸ä»»åŠ¡ä¸ºèŠ‚ç‚¹åˆ†ç±» |
|      OFAâœ…      | One for All: Towards Training One Graph Model for All Classification Tasks                                  |           LLaMA (13B)           |            Frozen            | ICLR 2024<br />Spotlight | [[Code]](https://github.com/LechengKong/OneForAll) |                                                                    æŒ‘æˆ˜1. ä¸åŒåŸŸå›¾æ•°æ®åˆ†å¸ƒä¸åŒï¼Œæœ‰ä¸åŒçš„å±æ€§ï¼Œå¾ˆéš¾åœ¨å•ä¸€è¡¨ç¤ºç©ºé—´è¡¨ç¤ºå„ç§å›¾ï¼›2.å›¾ä»»åŠ¡å¤šæ ·åŒ–ï¼ˆèŠ‚ç‚¹ã€è¾¹ã€å›¾ï¼‰ï¼Œéœ€è¦ä¸åŒçš„embeddingç­–ç•¥ï¼›3.å¦‚ä½•ä¸ºå›¾promptèŒƒå¼å®ç°ICLã€‚æœ¬æ–‡æå‡ºOne for All(OFA)æ–¹æ³•ï¼Œä½¿ç”¨ä¸€ä¸ªå›¾æ¨¡å‹è§£å†³æ‰€æœ‰é—®é¢˜ã€‚ä½¿ç”¨TAGè¡¨ç¤ºå›¾æ•°æ®ï¼Œå¼•å…¥nodes-of-interest(NOI)æ ‡å‡†åŒ–å„ç§å›¾ä»»åŠ¡ï¼Œä»¥åŠå®ç°ICLèƒ½åŠ›ã€‚                                                                    |
|       âœ…       | All in One: Multi-Task Prompting for Graph Neural Networks                                                  |                                  |                              | KDD 2023<br />Best Paper | [[Code]](https://github.com/sheldonresearch/ProG) |                                             é¢„è®­ç»ƒpretexté€šå¸¸ä¸ä¸‹æ¸¸å›¾ä»»åŠ¡ä¸å…¼å®¹ï¼Œä»¥åŠå¯èƒ½å‡ºç°è´Ÿè¿ç§»ã€‚æœ¬æ–‡æå‡ºå¤šä»»åŠ¡promptingæ–¹æ³•ï¼Œé€šè¿‡prompt tokenï¼Œtoken structureå’Œinserting patternå°†graph promptå’Œlanguage promptç»Ÿä¸€ï¼Œä¸ºäº†ç¼©å°å„ä¸‹æ¸¸ä»»åŠ¡å’Œé¢„è®­ç»ƒçš„gapï¼Œå°†ä¸‹æ¸¸ä»»åŠ¡ï¼ˆèŠ‚ç‚¹ã€è¾¹ã€å›¾ï¼‰ç»Ÿä¸€çœ‹ä½œå›¾çº§ä»»åŠ¡ï¼ˆèŠ‚ç‚¹å’Œè¾¹å¯ä»¥ç”¨tau-hop å­å›¾é¢„æµ‹ï¼‰ï¼›ä¸ºäº†å­¦ä¹ å¯é çš„æç¤ºï¼Œä½¿ç”¨å…ƒå­¦ä¹ æ–¹æ³•å¾®è°ƒpromptå’Œä»»åŠ¡å¤´å‚æ•°ã€‚                                             |
| OpenGraphâœ… | OpenGraph: Towards Open Graph Foundation Models | ChatGPT | Frozen | Arxiv 2024 | [[Code]](https://github.com/HKUDS/OpenGraph/tree/main) | OpenGraphç®—æ³•æä¾›ä¸€ä¸ªé€šç”¨ä¸”å¯æ‰©å±•çš„å›¾æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°é›¶æ ·æœ¬å­¦ä¹ å¹¶é€‚åº”ä¸åŒé¢†åŸŸçš„å›¾ç»“æ„ã€‚å—é™æå‡ºä¸€ä¸ªé€šç”¨tokenizerå°†è¾“å…¥å›¾å˜æ¢ä¸ºä¸€ä¸ªç»Ÿä¸€tokenåºåˆ—ï¼Œä½¿å¾—èƒ½å¤Ÿåœ¨ä¸åŒå›¾ä¸Šè¿›è¡Œè‰¯å¥½çš„æ³›åŒ–ï¼›è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„å›¾transformerä½œä¸ºencoderï¼Œæ•æ‰å…¨å±€æ‹“æ‰‘ä¸Šä¸‹æ–‡ä¸­çš„èŠ‚ç‚¹çš„ä¾èµ–å…³ç³»ï¼›å¼•å…¥LLMçš„æ•°æ®å¢å¼ºæœºåˆ¶ï¼Œä½¿ç”¨Tree-of-Promptè¿­ä»£åœ°å°†ä¸€èˆ¬èŠ‚ç‚¹åˆ’åˆ†ä¸ºå…·æœ‰æ›´ç²¾ç»†è¯­ä¹‰ç²’åº¦çš„å­ç±»åˆ«å®ä½“ä½œä¸ºå¶å­ç»“ç‚¹ï¼Œå¹¶åˆ©ç”¨Bibbsé‡‡æ ·å¾—åˆ°ç”ŸæˆèŠ‚ç‚¹é›†ï¼Œä»¥åŠè®¾è®¡Node-wise connectionæ¦‚ç‡é¢„æµ‹æ–¹æ³•ï¼›å¼•å…¥äº†ä¸€ç§å°†å±€éƒ¨æ€§æ¦‚å¿µçº³å…¥è¾¹ç¼˜ç”Ÿæˆè¿‡ç¨‹çš„æ–¹æ³•é˜²æ­¢åˆ›é€ è¿‡å¤šçš„è¿æ¥ã€‚ |

<h2 id="3"> 3. Datasets & Benchmarks </h2>

The datasets & benchmarks for LLM-related RS topics should maintain the original semantic/textual features, instead of anonymous feature IDs.

<h3 id="3.1">3.1 Datasets</h3>

| **Dataset** | **RS Scenario** |                                                               **Link**                                                               | Main Contributions |
| :---------------: | :--------------------: | :----------------------------------------------------------------------------------------------------------------------------------------: | ------------------ |
|   Reddit-Movie   | Conversational & Movie | [[Link]](https://github.com/AaronHeee/LLMs-as-Zero-Shot-Conversational-RecSys#large-language-models-as-zero-shot-conversational-recommenders) |                    |
|     Amazon-M2     |       E-commerce       |                                                  [[Link]](https://arxiv.org/abs/2307.09688)                                                  |                    |
|     MovieLens     |         Movie         |                                            [[Link]](https://grouplens.org/datasets/movielens/1m/)                                            |                    |
|      Amazon      |       E-commerce       |                                   [[Link]](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews)                                   |                    |
|   BookCrossing   |          Book          |                                        [[Link]](http://www2.informatik.uni-freiburg.de/~cziegler/BX/)                                        |                    |
|     GoodReads     |          Book          |                                          [[Link]](https://mengtingwan.github.io/data/goodreads.html)                                          |                    |
|       Anime       |         Anime         |                             [[Link]](https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database)                             |                    |
|     PixelRec     |      Short Video      |                                              [[Link]](https://github.com/westlake-repl/PixelRec)                                              |                    |
|      Netflix      |         Movie         |                                                   [[Link]](https://github.com/HKUDS/LLMRec)                                                   |                    |

<h3 id="3.2">3.2 Benchmarks</h3>

|   **Benchmarks**   |                                      **Webcite Link**                                      |             **Paper**             | Main Contributions |
| :----------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------: | ------------------ |
| Amazon-M2 (KDD Cup 2023) | [[Link]](https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendation-challenge) | [[Paper]](https://arxiv.org/abs/2307.09688) |                    |
|          LLMRec          |                           [[Link]](https://github.com/williamliujl/LLMRec)                           | [[Paper]](https://arxiv.org/abs/2308.12241) |                    |
|          OpenP5          |                           [[Link]](https://github.com/agiresearch/OpenP5)                           | [[Paper]](https://arxiv.org/abs/2306.11134) |                    |
|          TABLET          |                             [[Link]](https://dylanslacks.website/Tablet)                             | [[Paper]](https://arxiv.org/abs/2304.13188) |                    |

<h2 id="4">Related Repositories</h2>

|                                                     **Repo Name**                                                     |           **Maintainer**           |                            Link                            |
| :-------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------: | :---------------------------------------------------------: |
|                               [rs-llm-paper-list](https://github.com/wwliu555/rs-llm-paper-list)                               |   [wwliu555](https://github.com/wwliu555)   |                                                            |
|    [awesome-recommend-system-pretraining-papers](https://github.com/archersama/awesome-recommend-system-pretraining-papers)    | [archersama](https://github.com/archersama) |                                                            |
|                                           [LLM4Rec](https://github.com/WLiK/LLM4Rec)                                           |       [WLiK](https://github.com/WLiK)       |                                                            |
|                          [Awesome-LLM4RS-Papers](https://github.com/nancheng58/Awesome-LLM4RS-Papers)                          | [nancheng58](https://github.com/nancheng58) |                                                            |
|                                  [LLM4IR-Survey](https://github.com/RUC-NLPIR/LLM4IR-Survey)                                  |  [RUC-NLPIR](https://github.com/RUC-NLPIR)  |                                                            |
|                          [Awesome-LLM-for-RecSys](https://github.com/CHIANGEL/Awesome-LLM-for-RecSys)                          | [Jianghao Lin](https://github.com/CHIANGEL) | [[Survey]](https://github.com/CHIANGEL/Awesome-LLM-for-RecSys) |
| [Awesome-LLM-Uncertainty-Reliability-Robustness](https://github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustness) | [jxzhangjhu](https://github.com/jxzhangjhu) |                                                            |
