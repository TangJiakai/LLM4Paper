#! https://zhuanlan.zhihu.com/p/678404083

### LLM-Paper

ä»¥LLM4Recæ–¹å‘ä¸ºä¸»

ç›®å½•å¦‚ä¸‹

1. [LLM & RS](#1)

   ğŸ”·  1.1 [LLM for Feature Engineering](#1.1)

   â€‹		ğŸ”¸ 1.1.1 [User- and Item-level Feature Augmentation](#1.1.1)

   â€‹		ğŸ”¸ 1.1.2 [Instance-level Sample Generation](#1.1.2)

   ğŸ”· 1.2 [LLM as Feature Encoder](#1.2)

   â€‹		ğŸ”¸ 1.2.1 [Representation Enhancement](#1.2.1)

   â€‹		ğŸ”¸ 1.2.2 [Unified Cross-domain Recommendation](#1.2.2)
   
   ğŸ”· 1.3 [LLM as Scoring/Ranking Function](#1.3)
   
   â€‹		ğŸ”¸ 1.3.1 [Item Scoring Task](#1.3.1)
   
   â€‹		ğŸ”¸ 1.3.2 [Item Generation Task](#1.3.2)
   
   â€‹		ğŸ”¸ 1.3.3 [Hybrid Task](#1.3.3)
   
   ğŸ”· 1.4 [LLM for User Interaction](#1.4)
   
   â€‹		ğŸ”¸ 1.4.1 [Task-oriented User Interaction](#1.4.1)
   
   â€‹		ğŸ”¸ 1.4.2 [Open-ended User Interaction](#1.4.2)
   
   ğŸ”· 1.5 [LLM for RS Pipeline Controller](#1.5)

2. [LLM & Graph](#2)

3. [Datasets & Benchmarks](#3)

   ğŸ”· 3.1 [Datasets](#3.1)

   ğŸ”· 3.2 [Benchmarks](#3.2)

4. [Related Repositories](#4)

<h2 id="1">1. LLM & RS</h2>

å¼•å…¥LLMåˆ°æ¨èä¸­çš„å¸¸ç”¨åŠ¨æœºï¼š

- LLMèƒ½ä»¥promptæ–¹å¼ç»Ÿä¸€å„ç§ä¸‹æ¸¸æ¨èä»»åŠ¡
- LLMå°†å„æ¨¡æ€ã€å„ç‰¹å¾ç»Ÿä¸€ä»¥æ–‡æœ¬å‘ˆç°ï¼Œç¼“è§£ä¸åŒæ¨¡æ€/ç‰¹å¾å¼‚è´¨æ€§é—®é¢˜
- LLMå…·æœ‰å¼ºå¤§çš„è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œèƒ½æ›´å¥½çš„æ•è·ç”¨æˆ·çš„åå¥½
- LLMç›¸æ¯”äºä¼ ç»Ÿæ¨èç®—æ³•ï¼Œå…·æœ‰æ›´å¥½çš„å†·å¯åŠ¨å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºæ–‡æœ¬ç‰¹å¾æ˜¯å„ç”¨æˆ·ã€ç‰©å“ã€é¢†åŸŸæ‰€å…±äº«çš„
- åªæœ‰IDä¼šç¼ºä¹ä¸–ç•ŒçŸ¥è¯†ï¼Œåªæœ‰æ–‡æœ¬ä¼šç¼ºä¹ç†è§£æ¨èååŒ/åºåˆ—äº¤äº’æ¨¡å¼ï¼Œç»“åˆäºŒè€…ï¼ˆå¯è§†ä¸ºå¤šä¸ªæ¨¡æ€ï¼‰æ‰èƒ½å……åˆ†å‘æŒ¥ä¸–ç•ŒçŸ¥è¯†å’Œè¡Œä¸ºçŸ¥è¯†çš„ä¼˜åŠ¿

![](where-framework-1.png)

<details><summary><h3 id="1.1">1.1 LLM for Feature Engineering</h3></summary>
<p>
<h4 id="1.1.1">1.1.1 User- and Item-level Feature Augmentation</h4>

| **Name** | **Paper**                                                                                                           | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |                **Link**                | Main Contributions |
| :------------: | :------------------------------------------------------------------------------------------------------------------------ | :------------------------------: | :----------------------------: | :-------------------: | :------------------------------------------: | ------------------ |
|    LLM4KGC    | Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs  |       PaLM (540B)/ ChatGPT       |             Frozen             |      Arxiv 2023      |  [[Paper]](https://arxiv.org/abs/2305.09858v1)  |                    |
|     TagGPT     | TagGPT: Large Language Models are Zero-shot Multimodal Taggers                                                            |             ChatGPT             |             Frozen             |      Arxiv 2023      |  [[Paper]](https://arxiv.org/abs/2304.03022v1)  |                    |
|      ICPC      | Large Language Models for User Interest Journeys                                                                          |           LaMDA (137B)           | Full Finetuning/ Prompt Tuning |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2305.15498)   |                    |
|      KAR      | Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models                                  |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2306.10933)   |                    |
|      PIE      | Product Information Extraction using ChatGPT                                                                              |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2306.14921)   |                    |
|      LGIR      | Enhancing Job Recommendation through LLM-based Generative Adversarial Networks                                            |           GhatGLM (6B)           |             Frozen             |       AAAI 2024       |   [[Paper]](https://arxiv.org/abs/2307.10747)   |                    |
|      GIRL      | Generative Job Recommendations with Large Language Model                                                                  |            BELLE (7B)            |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2307.02157)   |                    |
|    LLM-Rec    | LLM-Rec: Personalized Recommendation via Prompting Large Language Models                                                  |         text-davinci-003         |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2307.15780)   |                    |
|      HKFRâœ…      | Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM                                  |             ChatGPT+<br />ChatGLM (6B)             |             LoRA             |      RecSys 2023      |   [[Paper]](https://arxiv.org/abs/2308.03333)   | ä¼ ç»Ÿæ–¹æ³•å°†ç”¨æˆ·å¼‚è´¨ä¿¡æ¯ä¸æ¨¡å‹ç»“åˆï¼Œä¼šå‡ºç°ç‰¹å¾ç¨€ç–å’ŒçŸ¥è¯†ç¢ç‰‡åŒ–ï¼ˆç¼ºä¹ä¸åŒè¡Œä¸ºä¹‹é—´å¼‚è´¨çŸ¥è¯†çš„èåˆï¼‰ã€‚æœ¬æ–‡é€šè¿‡LLMå¸®åŠ©æå–ç”¨æˆ·ä¸åŒè¡Œä¸ºä¸­çš„å¼‚è´¨çŸ¥è¯†ï¼Œå¹¶ä½¿ç”¨instruct tuningçš„æ–¹å¼èåˆè¿™äº›çŸ¥è¯†å’Œæ¨èä»»åŠ¡ã€‚ |
|    LLaMA-E    | LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following                                          |           LLaMA (30B)           |              LoRA              |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2308.04913)   |                    |
|    EcomGPT    | EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce                                 |          BLOOMZ (7.1B)          |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2308.06966)   |                    |
|    TF-DCon    | Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation    |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2310.09874)   |                    |
|     RLMRecâœ…     | Representation Learning with Large Language Models for Recommendation                                                     |             ChatGPT             |             Frozen             |       WWW 2024       |   [[Code]](https://github.com/HKUDS/RLMRec)   | æ¨èéšå¼æ•°æ®åŒ…å«å™ªå£°å’Œåå·®ï¼›ä½¿ç”¨LLMæ¨èå­˜åœ¨æ‰©å±•æ€§ä¸è¶³ã€è¾“å…¥é•¿åº¦é™åˆ¶ç­‰ä¸è¶³ã€‚æœ¬æ–‡ç›®çš„æ˜¯é€šè¿‡LLMå¼ºå¤§è¯­è¨€ç†è§£èƒ½åŠ›æŒ–æ˜ç”¨æˆ·è¡Œä¸ºå’Œåå¥½ï¼ŒåŒæ—¶ä¿ç•™å·²æœ‰æ¨èå™¨çš„æ•ˆç‡å’Œå‡†ç¡®åº¦ã€‚è´¡çŒ®æœ‰1. åŸºäºæ¨ç†çš„ç”¨æˆ·å’Œç‰©å“æç¤ºç”Ÿæˆï¼›2. æå‡ºå¯¹æ¯”å¼å’Œæ©ç ç”Ÿæˆå¼ä¸¤ä¸ªæ–¹æ³•ï¼›3.ä»äº’ä¿¡æ¯è§’åº¦åˆ†æå¼•å…¥æ–‡æœ¬ä¿¡å·å¯¹äºæå‡è¡¨ç¤ºè´¨é‡çš„æœ‰æ•ˆæ€§ |
|     LLMRecâœ…     | LLMRec: Large Language Models with Graph Augmentation for Recommendation                                                  |             ChatGPT             |             Frozen             |       WSDM 2024       | [[Code]](https://github.com/HKUDS/LLMRec) | æ•°æ®ç¨€ç–å¯ä»¥é€šè¿‡side informationè§£å†³ï¼Œä½†æ˜¯è¿™äº›ä¿¡æ¯ä¹Ÿä¼šåŒ…å«å™ªå£°ã€å¯ç”¨æ€§ä½å’Œæ•°æ®è´¨é‡ä½ç­‰é—®é¢˜ï¼Œåè¿‡æ¥ä¼¤å®³äº†æ¨èå‡†ç¡®ç‡ã€‚æœ¬æ–‡æå‡ºæ–¹æ³•ä»ç”¨æˆ·-ç‰©å“äº¤äº’è¾¹ã€ç”¨æˆ·ç”»åƒã€ç‰©å“èŠ‚ç‚¹å±æ€§ä¸‰è§’åº¦å¢å¼ºæ•°æ®ï¼›å¹¶æå‡ºå¯¹å¢å¼ºçš„äº¤äº’å’Œç‰¹å¾åˆ†åˆ«è®¾è®¡å‰ªæã€MAEæ–¹æ³•å»å™ªã€‚ |
|     LLMRG     | Enhancing Recommender Systems with Large Language Model Reasoning Graphs                                                  |              GPT4              |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2308.10835)   |                    |
|      CUP      | Recommendations by Concise User Profiles from Review Text                                                                 |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2311.01314)   |                    |
|     SINGLE     | Modeling User Viewing Flow using Large Language Models for Article Recommendation                                         |             ChatGPT             |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2311.07619)   |                    |
|     SAGCN     | Understanding Before Recommendation: Semantic Aspect-Aware Review Exploitation via Large Language Models                  |           Vicuna (13B)           |             Frozen             |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2312.16275)   |                    |
|      UEM      | User Embedding Model for Personalized Language Prompting                                                                  |       FLAN-T5-base (250M)       |        Full Finetuning        |      Arxiv 2024      |   [[Paper]](https://arxiv.org/abs/2401.04858)   |                    |
|     LLMHG     | LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation                                    |               GPT4               |             Frozen             |      Arxiv 2024      |   [[Paper]](https://arxiv.org/abs/2401.08217)   |                    |
|   Llama4Rec   | Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation                    |           LLaMA2 (7B)           |        Full Finetuning        |      Arxiv 2024      |   [[Paper]](https://arxiv.org/abs/2401.13870)   |                    |

<h4 id="1.1.2">1.1.2 Instance-level Sample Generation</h4>

|   **Name**    | **Paper**                                                    |      **LLM Backbone (Largest)**      | **LLM Tuning Strategy** | **Publication** |                  **Link**                   | Main Contributions                                           |
| :-----------: | :----------------------------------------------------------- | :----------------------------------: | :---------------------: | :-------------: | :-----------------------------------------: | ------------------------------------------------------------ |
|     GReaT     | Language Models are Realistic Tabular Data Generators        |          GPT2-medium (355M)          |     Full Finetuning     |    ICLR 2023    | [[Paper]](https://arxiv.org/abs/2210.06280) |                                                              |
|     ONCEâœ…     | ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models | ChatGPT+<br />LLaMA2(7B)/LLaMA2(13B) |          LoRA           |    WSDM 2024    |   [[Code]](https://github.com/Jyonn/ONCE)   | ä¼ ç»Ÿcontent-basedæ¨èç®—æ³•åœç•™åœ¨æ•æ‰wordçº§åˆ«çš„ç›¸ä¼¼åº¦ï¼Œè€Œæ— æ³•å®ç°contentçº§åˆ«ã€‚æœ¬æ–‡æå‡ºåŒæ—¶åˆ©ç”¨å¼€æºå’Œé—­æºçš„LLMå¸®åŠ©å¢å¼ºå¯¹ç‰©å“å’Œç”¨æˆ·ä¿¡æ¯å†…å®¹çš„ç†è§£ã€‚å¼€æºLLMç”¨äºç¼–ç ç”¨æˆ·å’Œç‰©å“è¡¨ç¤ºï¼›é—­æºLLMç”¨äºç‰©å“å†…å®¹ä¸°å¯Œã€ç”¨æˆ·å…´è¶£æŠ½è±¡ã€ï¼ˆå†·å¯åŠ¨ï¼‰ç”¨æˆ·å†…å®¹ç”Ÿæˆã€‚å¹¶ä½¿ç”¨LoRAã€å†»ç»“ä½å±‚ï¼Œç¼“å­˜æœºåˆ¶ç­‰æé«˜æ•ˆç‡ã€‚ |
|  AnyPredict   | AnyPredict: Foundation Model for Tabular Prediction          |               ChatGPT                |         Frozen          |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2305.12081) |                                                              |
|     DPLLM     | Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models |              T5-XL (3B)              |     Full Finetuning     |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2305.05973) |                                                              |
|     MINT      | Large Language Model Augmented Narrative Driven Recommendations |           text-davinci-003           |         Frozen          |   RecSys 2023   | [[Paper]](https://arxiv.org/abs/2306.02250) |                                                              |
|   Agent4Rec   | On Generative Agents in Recommendation                       |               ChatGPT                |         Frozen          |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2310.10108) |                                                              |
|   RecPrompt   | RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models |                 GPT4                 |         Frozen          |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2312.10463) |                                                              |
|    PO4ISR     | Large Language Models for Intent-Driven Session Recommendations |               ChatGPT                |         Frozen          |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2312.07552) |                                                              |
|     BEQUE     | Large Language Model based Long-tail Query Rewriting in Taobao Search |             ChatGLM (6B)             |     Full Finetuning     |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2311.03758) |                                                              |
| Agent4Ranking | Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM |               ChatGPT                |         Frozen          |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2312.15450) |                                                              |

</p>
</details>

<details><summary><h3 id="1.2">1.2 LLM as Feature Encoder</h3></summary>
<p>
<h4 id="1.2.1">1.2.1 Representation Enhancement</h4>

| **Name** | **Paper**                                                                                                                      | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |                                     **Link**                                     | Main Contributions |
| :------------: | :----------------------------------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :-------------------: | :------------------------------------------------------------------------------------: | ------------------ |
|     U-BERT     | U-BERT: Pre-training User Representations for Improved Recommendation                                                                |         BERT-base (110M)         |        Full Finetuning        |       AAAI 2021       |             [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/16557)             |                    |
|     UNBERT     | UNBERT: User-News Matching BERT for News Recommendation                                                                              |         BERT-base (110M)         |        Full Finetuning        |      IJCAI 2021      |                   [[Paper]](https://www.ijcai.org/proceedings/2021/462)                   |                    |
|     PLM-NR     | Empowering News Recommendation with Pre-trained Language Models                                                                      |       RoBERTa-base (125M)       |        Full Finetuning        |      SIGIR 2021      |                        [[Paper]](https://arxiv.org/abs/2104.07413)                        |                    |
| Pyramid-ERNIE | Pre-trained Language Model based Ranking in Baidu Search                                                                             |           ERNIE (110M)           |        Full Finetuning        |       KDD 2021       |                        [[Paper]](https://arxiv.org/abs/2105.11108)                        |                    |
|    ERNIE-RS    | Pre-trained Language Model for Web-scale Retrieval in Baidu Search                                                                   |           ERNIE (110M)           |        Full Finetuning        |       KDD 2021       |                        [[Paper]](https://arxiv.org/abs/2106.03373)                        |                    |
|    CTR-BERT    | CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models                                                 |      Customized BERT (1.5B)      |        Full Finetuning        |      ENLSP 2021      | [[Paper]](https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf) |                    |
|      SuKD      | Learning Supplementary NLP Features for CTR Prediction in Sponsored Search                                                           |       RoBERTa-large (355M)       |        Full Finetuning        |       KDD 2022       |               [[Paper]](https://dl.acm.org/doi/abs/10.1145/3534678.3539064)               |                    |
|      PREC      | Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation                                                |         BERT-base (110M)         |        Full Finetuning        |      COLING 2022      |                  [[Paper]](https://aclanthology.org/2022.coling-1.249/)                  |                    |
|     MM-Rec     | MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation                                                               |         BERT-base (110M)         |        Full Finetuning        |      SIGIR 2022      |               [[Paper]](https://dl.acm.org/doi/abs/10.1145/3477495.3531896)               |                    |
|  Tiny-NewsRec  | Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation                                                                  |       UniLMv2-base (110M)       |        Full Finetuning        |      EMNLP 2022      |                        [[Paper]](https://arxiv.org/abs/2112.00944)                        |                    |
|    PLM4Tag    | PTM4Tag: Sharpening Tag Recommendation of Stack Overflow Posts with Pre-trained Models                                               |         CodeBERT (125M)         |        Full Finetuning        |       ICPC 2022       |                        [[Paper]](https://arxiv.org/abs/2203.10965)                        |                    |
|   TwHIN-BERT   | TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations                                    |         BERT-base (110M)         |        Full Finetuning        |      Arxiv 2022      |                        [[Paper]](https://arxiv.org/abs/2209.07562)                        |                    |
|      LSH      | Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study                 |         BERT-base (110M)         |        Full Finetuning        |      Arxiv 2023      |                       [[Paper]](https://arxiv.org/abs/2305.03017v1)                       |                    |
|  LLM2BERT4Rec  | Leveraging Large Language Models for Sequential Recommendation                                                                       |      text-embedding-ada-002      |            Frozen            |      RecSys 2023      |                        [[Paper]](https://arxiv.org/abs/2309.09261)                        |                    |
|    LLM4ARec    | Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations                                            |           GPT2 (110M)           |         Prompt Tuning         |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2306.01475)                        |                    |
|     TIGER     | Recommender Systems with Generative Retrieval                                                                                        |           Sentence-T5           |            Frozen            |       NIPS 2023       |                        [[Paper]](https://arxiv.org/abs/2305.05065)                        |                    |
|      TBIN      | TBIN: Modeling Long Textual Behavior Data for CTR Prediction                                                                         |         BERT-base (110M)         |            Frozen            |    DLP-RecSys 2023    |                        [[Paper]](https://arxiv.org/abs/2308.08483)                        |                    |
|     LKPNR     | LKPNR: LLM and KG for Personalized News Recommendation Framework                                                                     |           LLaMA2 (7B)           |            Frozen            |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2308.12028)                        |                    |
|      SSNA      | Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation                                    |     DistilRoBERTa-base (83M)     |   Layerwise Adapter Tuning   |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2310.01612)                        |                    |
| CollabContext | Collaborative Contextualization: Bridging the Gap between Collaborative Filtering and Pre-trained Language Model                     |       Instructor-XL (1.5B)       |            Frozen            |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2310.09400)                        |                    |
|   LMIndexer   | Language Models As Semantic Indexers                                                                                                 |          T5-base (223M)          |        Full Finetuning        |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2310.07815)                        |                    |
|     Stack     | A BERT based Ensemble Approach for Sentiment Classification of Customer Reviews and its Application to Nudge Marketing in e-Commerce |         BERT-base (110M)         |            Frozen            |      Arxiv 2023      |                        [[Paper]](https://arxiv.org/abs/2311.10782)                        |                    |
|      N/A      | Utilizing Language Models for Tour Itinerary Recommendation                                                                          |         BERT-base (110M)         |        Full Finetuning        |    PMAI@IJCAI 2023    |                        [[Paper]](https://arxiv.org/abs/2311.12355)                        |                    |
|      UEM      | User Embedding Model for Personalized Language Prompting                                                                             |     Sentence-T5-base (223M)     |            Frozen            |      Arxiv 2024      |                        [[Paper]](https://arxiv.org/abs/2401.04858)                        |                    |
|   Social-LLM   | Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data                                            |     SBERT-MPNet0base (110M)     |            Frozen            |      Arxiv 2024      |                        [[Paper]](https://arxiv.org/abs/2401.00893)                        |                    |
|     LLMRS     | LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase                                                   |           MPNet (110M)           |            Frozen            |      Arxiv 2024      |                        [[Paper]](https://arxiv.org/abs/2401.06676)                        |                    |
| GaCLLMâœ… | Large Language Model with Graph Convolution for Recommendation | ChatGLM (6B) | LoRA | Arxiv 2024 | [[Paper]](https://arxiv.org/pdf/2402.08859.pdf) | ç°æœ‰LLMæ¨èå¿½ç•¥äº†ç”¨æˆ·-ç‰©å“äº¤äº’çš„ç»“æ„åŒ–çŸ¥è¯†ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨LLMæ•æ‰é«˜é˜¶çš„ç”¨æˆ·-ç‰©å“å…³ç³»ã€‚é¦–å…ˆï¼Œåœ¨å…·ä½“domainä¸Šæ‰§è¡ŒSFTè®­ç»ƒï¼Œå¯¹é½ç”¨æˆ·å’Œç‰©å“çš„æè¿°ï¼›ç¬¬äºŒï¼Œé€’å½’åœ°ä»å°‘åˆ°å¤šèåˆé‚»å±…ä¿¡æ¯ã€‚æœ€åï¼Œé€šè¿‡æ‹¼æ¥å›¾embeddingè¡¨ç¤ºå’ŒLLMå›¾è¡¨ç¤ºï¼Œå†…ç§¯è®¡ç®—u-iå¾—åˆ†ã€‚ |

<h4 id="1.2.2">1.2.2 Unified Cross-domain Recommendation</h4>

|    **Name**     | **Paper**                                                    |       **LLM Backbone (Largest)**        | **LLM Tuning Strategy**  | **Publication** |                       **Link**                        | Main Contributions                                           |
| :-------------: | :----------------------------------------------------------- | :-------------------------------------: | :----------------------: | :-------------: | :---------------------------------------------------: | ------------------------------------------------------------ |
|     ZESRec      | Zero-Shot Recommender Systems                                |            BERT-base (110M)             |          Frozen          |   Arxiv 2021    |      [[Paper]](https://arxiv.org/abs/2105.08318)      |                                                              |
|    UniSRecâœ…     | Towards Universal Sequence Representation Learning for Recommender Systems |            BERT-base (110M)             |          Frozen          |    KDD 2022     |     [[Code]](https://github.com/RUCAIBox/UniSRec)     | åŸºäºç‰©å“IDåºåˆ—æ¨èéš¾ä»¥å®ç°è·¨åŸŸè¿ç§»ã€‚UniSRecæå‡ºä½¿ç”¨ç‰©å“æ–‡æœ¬ä¿¡æ¯è¡¨ç¤ºç‰©å“ï¼Œå­¦ä¹ é€šç”¨ç‰©å“è¡¨ç¤ºå’Œé€šç”¨åºåˆ—è¡¨ç¤ºã€‚ä¸ºäº†å­¦ä¹ é€šç”¨ç‰©å“è¡¨ç¤ºï¼Œä½¿ç”¨å‚æ•°ç™½åŒ–ï¼ˆå› ä¸ºBERTå®¹æ˜“å‡ºç°éå¹³æ»‘å„å‘å¼‚æ€§è¡¨å¾ç©ºé—´ï¼‰å’ŒMOEç»“æ„ï¼ˆä¸åŒåŸŸä¹‹é—´å­˜åœ¨è¯­ä¹‰gapï¼Œä½¿ç”¨MOEè‡ªé€‚åº”ç»„åˆç‰©å“è¡¨ç¤ºå®ç°è¿ç§»å’Œèåˆä¸åŒåŸŸçš„ä¿¡æ¯ï¼‰ã€‚ä¸ºäº†å®ç°é€šç”¨åºåˆ—è¡¨ç¤ºï¼Œæå‡ºåºåˆ—-ç‰©å“å’Œåºåˆ—-åºåˆ—çš„å¯¹æ¯”ä»»åŠ¡ï¼ˆåºåˆ—æ˜¯é€šè¿‡word dropå’Œitem dropå®ç°ï¼‰ã€‚å¯¹äºfine-tuneé˜¶æ®µï¼Œæ ¹æ®inductiveå’Œtransductiveè®¾ç½®ï¼Œåˆ†åˆ«è€ƒè™‘æ˜¯å¦èåˆIDä¿¡æ¯ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒåªè®­ç»ƒMOEå‚æ•°ã€‚ |
|    TransRec     | TransRec: Learning Transferable Recommendation from Mixture-of-Modality Feedback |            BERT-base (110M)             |     Full Finetuning      |   Arxiv 2022    |      [[Paper]](https://arxiv.org/abs/2206.06190)      |                                                              |
|     VQ-Rec      | Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders |            BERT-base (110M)             |          Frozen          |    WWW 2023     |      [[Paper]](https://arxiv.org/abs/2210.12316)      |                                                              |
| IDRec vs MoRecâœ… | Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited |            BERT-base (110M)             |     Full Finetuning      |   SIGIR 2023    | [[Code]](https://github.com/westlake-repl/IDvs.MoRec) | ç ”ç©¶çº¯ID-basedæ¨èå’Œçº¯modality-basedæ¨èã€‚å®ç°å‘ç°ä½¿ç”¨é¢„è®­ç»ƒçš„encoderè¿›è¡Œend2endè®­ç»ƒï¼Œèƒ½å¤ŸæŒå¹³ç”šè‡³è¶…è¿‡ID-basedæ¨èç®—æ³•ï¼ˆç”šè‡³å¯¹äºéå†·å¯åŠ¨ç‰©å“ï¼‰ã€‚ |
|    TransRec     | Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights |           RoBERTa-base (125M)           | Layerwise Adapter Tuning |   Arxiv 2023    |      [[Paper]](https://arxiv.org/abs/2305.15036)      |                                                              |
|       TCF       | Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights |             OPT-175B (175B)             | Frozen/ Full Finetuning  |   Arxiv 2023    |      [[Paper]](https://arxiv.org/abs/2305.11700)      |                                                              |
| S&R Foundationâœ… | An Unified Search and Recommendation Foundation Model for Cold-Start Scenario | ChatGLM (6B)<br />or<br />ChatGLM2 (6B) |          Frozen          |    CIKM 2023    |      [[Paper]](https://arxiv.org/abs/2309.08939)      | æå‡ºè”åˆæœç´¢æ¨èçš„å¤šåŸŸåŸºçŸ³æ¨¡å‹ï¼Œåˆ©ç”¨LLMä»æœç´¢æ–‡æœ¬å’Œæ¨èå•†å“æ–‡æœ¬ä¿¡æ¯ä¸­æå–åŸŸä¸å˜ç‰¹å¾ï¼Œåˆ©ç”¨ï¼ˆä¸‰ç§ï¼‰Aspect Fating FusionæŠ€æœ¯åˆå¹¶IDã€åŸŸä¸å˜ç‰¹å¾ã€ä»»åŠ¡ç‰¹å®šç¨€ç–ç‰¹å¾è·å¾—æœ€ç»ˆæœç´¢å’Œç‰©å“çš„è¡¨ç¤ºï¼ŒåŒæ—¶ä½¿ç”¨åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼ˆJSæ•£åº¦ï¼‰é™åˆ¶åŸŸåˆ†å¸ƒçš„å‘æ•£ç¨‹åº¦ã€‚ä½¿ç”¨æ¥è‡ªæœç´¢å’Œæ¨èä»»åŠ¡çš„æ ·æœ¬è”åˆè®­ç»ƒï¼Œéµå¾ªpretrain-finetuningèŒƒå¼ã€‚ |
|     MISSRec     | MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation |            CLIP-B/32 (400M)             |     Full Finetuning      |     MM 2023     |      [[Paper]](https://arxiv.org/abs/2308.11175)      |                                                              |
|      UFINâœ…      | UFIN: Universal Feature Interaction Network for Multi-Domain Click-Through Rate Prediction |           FLAN-T5-base (250M)           |          Frozen          |   Arxiv 2023    |      [[Code]](https://github.com/Ethan-TZ/UFIN)       | å·²æœ‰è·¨åŸŸæ–¹æ³•ç”±äºä¾èµ–äºIDç‰¹å¾ï¼Œéš¾ä»¥å¾ˆå¥½åœ°å®ç°ç‰¹å¾è¿ç§»ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨æ–‡æœ¬æ•°æ®å­¦ä¹ é€šç”¨ç‰¹å¾äº¤äº’ï¼Œä»è€Œæœ‰æ•ˆåœ¨å„ä¸ªåŸŸä¸Šè¿ç§»ã€‚å…·ä½“åœ°ï¼Œå°†æ–‡æœ¬å’Œç‰¹å¾è§†ä½œä¸¤ä¸ªæ¨¡æ€ï¼Œåˆ©ç”¨åŸºäºLLMçš„encoder-decoderç½‘ç»œç»“æ„åŠ å¼ºä»æ–‡æœ¬åˆ°ç‰¹å¾æ¨¡æ€çš„è¿ç§»æ•ˆæœï¼ˆåŸå§‹ç‰¹å¾->æ–‡æœ¬->é€šç”¨ç‰¹å¾ï¼‰ï¼›å¹¶åˆ©ç”¨MOEå¢å¼ºçš„è‡ªé€‚åº”ç‰¹å¾äº¤äº’æ¨¡å‹å­¦ä¹ å¯è¿ç§»çš„ç‰¹å¾äº¤äº’ï¼›æœ€åæå‡ºå¤šåŸŸçŸ¥è¯†è’¸é¦æ¡†æ¶è®­ç»ƒæ¨¡å‹ã€‚ |
|     PMMRec      | Multi-Modality is All You Need for Transferable Recommender Systems |          RoBERTa-large (355M)           |  Top-2-layer Finetuning  |    ICDE 2024    |      [[Paper]](https://arxiv.org/abs/2312.09602)      |                                                              |
|     Uni-CTR     | A Unified Framework for Multi-Domain CTR Prediction via Large Language Models |          Sheared-LLaMA (1.3B)           |           LoRA           |   Arxiv 2023    |      [[Paper]](https://arxiv.org/abs/2312.10743)      |                                                              |

</p>
</details>

<details><summary><h3 id="1.3">1.3 LLM as Scoring/Ranking Function</h3></summary>
<p>

<h4 id="1.3.1">1.3.1 Item Scoring Task</h4>


| **Name** | **Paper**                                                                                                     | **LLM Backbone (Largest)** |     **LLM Tuning Strategy**     | **Publication** |                         **Link**                         | Main Contributions |
| :------------: | :------------------------------------------------------------------------------------------------------------------ | :------------------------------: | :------------------------------------: | :-------------------: | :-------------------------------------------------------------: | ------------------ |
|    LMRecSys    | Language Models as Recommender Systems: Evaluations and Limitations                                                 |          GPT2-XL (1.5B)          |            Full Finetuning            |      ICBINB 2021      |       [[Paper]](https://openreview.net/forum?id=hFx3fY7-m9b)       |                    |
|      PTab      | PTab: Using the Pre-trained Language Model for Modeling Tabular Data                                                |         BERT-base (110M)         |            Full Finetuning            |      Arxiv 2022      |            [[Paper]](https://arxiv.org/abs/2209.08060)            |                    |
|    UniTRecâœ…    | UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation  |           BART (406M)           |            Full Finetuning            |       ACL 2023       |            [[Code]](https://github.com/Veason-silverbullet/UniTRec)            | PLMå¤„ç†ç”¨æˆ·å†å²ç‰©å“çš„æ–‡æœ¬ä¿¡æ¯å­˜åœ¨ä¸¤ç§æ–¹æ³•ï¼Œä½†éƒ½æœ‰ä¸è¶³ï¼š(a). ç›´æ¥å°†å…¨éƒ¨ç‰©å“å†å²ä½œä¸ºè¾“å…¥æ–‡æœ¬ä½œä¸ºç”¨æˆ·å…¨å±€è¡¨ç¤ºï¼Œå¿½ç•¥äº†ä¸åŒç‰©å“æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œè€Œæ˜¯æŠŠæ‰€æœ‰è¯éƒ½è§†ä½œåŒç­‰å‚ä¸ï¼›(b).å¼•å…¥é¢å¤–èšåˆç½‘ç»œï¼Œè¿™å‰Šå¼±äº†PLM ç¼–ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚æœ¬æ–‡è´¡çŒ®ï¼š(a). å¯¹æ³¨æ„åŠ›æ©ç æ“ä½œè®¾è®¡word-levelå’Œturn-levelä¸¤ç§æœºåˆ¶ (b).è”åˆè®­ç»ƒåŸºäºDiscriminative Scoreså’ŒCandidate Text Perplexityçš„å¯¹æ¯”æŸå¤±ã€‚ |
|   Prompt4NR   | Prompt Learning for News Recommendation                                                                             |         BERT-base (110M)         |            Full Finetuning            |      SIGIR 2023      |            [[Paper]](https://arxiv.org/abs/2304.05263)            |                    |
|   RecFormerâœ…   | Text Is All You Need: Learning Language Representations for Sequential Recommendation                               |        LongFormer (149M)        |            Full Finetuning            |       KDD 2023       |           [[Code]](https://github.com/AaronHeee/RecFormer)           | ID-basedæ¨èéš¾ä»¥è§£å†³å†·å¯åŠ¨é—®é¢˜ï¼›å·²æœ‰è·¨åŸŸæ¨èå·¥ä½œé€šå¸¸å‡è®¾æœ‰é‡å ç‰¹å¾ã€ç‰©å“æˆ–è€…ç”¨æˆ·ç­‰ï¼Œè¿™é™åˆ¶äº†ç®—æ³•åº”ç”¨ã€‚åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åº”ç”¨å­˜åœ¨é—®é¢˜:(a)é¢„è®­ç»ƒæ–‡æœ¬å’Œç‰©å“æ–‡æœ¬å­˜åœ¨é¢†åŸŸå·®å¼‚;(b)æ— æ³•å»ºæ¨¡ç»†ç²’åº¦ç”¨æˆ·åå¥½ã€‚æœ¬æ–‡æå‡ºä»¥key-valueå½¢å¼è¡¨è¾¾ç‰©å“æ–‡æœ¬ï¼Œä¿®æ”¹LongFormeré€šè¿‡åŠ å…¥ç‰¹æ®Šè®¾è®¡Token Embeddingï¼Œä»¥åŠé¢„è®­ç»ƒ+ä¸¤é˜¶æ®µå¾®è°ƒã€‚ |
|     TabLLM     | TabLLM: Few-shot Classification of Tabular Data with Large Language Models                                          |             T0 (11B)             | Few-shot Parameter-effiecnt Finetuning |     AISTATS 2023     |            [[Paper]](https://arxiv.org/abs/2210.10723)            |                    |
| Zero-shot GPT | Zero-Shot Recommendation as Language Modeling                                                                       |        GPT2-medium (355M)        |                 Frozen                 |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2112.04184)            |                    |
|    FLAN-T5âœ…    | Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction                                      |         FLAN-5-XXL (11B)         |            Full Finetuning            |      PGAI@CIKM 2023      |          [[Paper]](https://arxiv.org/pdf/2305.06474.pdf)          | 1. è¶…è¿‡100Bçš„æ¨¡å‹åœ¨å†·å¯åŠ¨æƒ…å†µä¸‹ä¸å¯å‘å¼æ–¹æ³•æ¥è¿‘çš„æ€§èƒ½ï¼›<br/>2. zero-shotæƒ…å†µä¸‹ä»ç„¶æ— æ³•æŠ—è¡¡ç›‘ç£å­¦ä¹ ä¸‹çš„ä¼ ç»Ÿæ¨èæ¨¡å‹ï¼›<br/>3. Fine-Tuned çš„æ¨¡å‹ä»…éœ€è¦å°‘é‡æ•°æ®å°±èƒ½æŒå¹³ç”šè‡³è¶…è¿‡ä¼ ç»Ÿæ¨¡å‹ï¼› |
|    BookGPT    | BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model                              |             ChatGPT             |                 Frozen                 |      Arxiv 2023      |           [[Paper]](https://arxiv.org/abs/2305.15673v1)           |                    |
|    TALLRecâœ…    | TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation              |            LLaMA (7B)            |                  LoRA                  |      RecSys 2023      |            [[Code]](https://github.com/SAI990323/TALLRec)            | LLMå’Œæ¨èä»»åŠ¡å­˜åœ¨gapï¼Œé¢å‘æ¨èçš„è¯­æ–™åº“ä¸è¶³ï¼›æ­¤å¤–ï¼ŒLLMå­˜åœ¨ä¸€å®šé™åˆ¶æ— æ³•ä¿è¯è¾“å‡ºåŒ…å«ç›®æ ‡ç‰©å“ï¼ˆå¦‚æ‹’ç»å›ç­”ï¼Œæˆ–å…¨éƒ¨positiveå›ç­”ï¼‰ã€‚å…·ä½“åŒ…æ‹¬ä¸¤é˜¶æ®µå¾®è°ƒï¼Œä¸€é˜¶æ®µæ˜¯ä½¿ç”¨ self-instructæ•°æ®å¾®è°ƒLLMï¼ŒäºŒé˜¶æ®µæ˜¯åˆ©ç”¨instructionæ•°æ®å¾®è°ƒLLMã€‚ |
|      PBNR      | PBNR: Prompt-based News Recommender System                                                                          |          T5-small (60M)          |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2304.07862)            |                    |
|    CR-SoRec    | CR-SoRec: BERT driven Consistency Regularization for Social Recommendation                                          |         BERT-base (110M)         |            Full Finetuning            |      RecSys 2023      | [[Paper]](https://dl.acm.org/doi/fullHtml/10.1145/3604915.3608844) |                    |
|   PromptRec   | Towards Personalized Cold-Start Recommendation with Prompts                                                         |            LLaMA (7B)            |                 Frozen                 |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2306.17256)            |                    |
|     GLRec     | Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations                           |         BELLE-LLaMA (7B)         |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2307.05722)            |                    |
|    BERT4CTRâœ…    | BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction |       RoBERTa-large (355M)       |            Full Finetuning            |       KDD 2023       |   [[Paper]](https://dl.acm.org/doi/abs/10.1145/3580305.3599780)   | èåˆéæ–‡æœ¬ç‰¹å¾ï¼ˆsparseå’Œdense featureï¼‰åˆ°LMæ˜¯æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ¬æ–‡æå‡ºUni-Attentionæ–¹æ³•ï¼Œéæ–‡æœ¬ç‰¹å¾å–æ¶ˆä½ç½®ç¼–ç ï¼ŒåŒæ—¶æ”¹å˜å…¶Attentionæ–¹å¼ï¼Œéæ–‡æœ¬ç‰¹å¾ä½œä¸ºQï¼Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºVå’ŒKã€‚ |
|     ReLLaâœ…     | ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation    |           Vicuna (13B)           |                  LoRA                  |       WWW 2024       |            [[Paper]](https://arxiv.org/abs/2308.11131)            | å®éªŒå‘ç°æ¨èåœºæ™¯ä¸­ï¼Œå¹¶éç”¨æˆ·å†å²åºåˆ—è¶Šé•¿æ•ˆæœè¶Šå¥½ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨semantic user behavior retrieval (SUBR)ä»£æ›¿æœ€è¿‘Kä¸ªäº¤äº’å†å²æ¥æå‡æ•°æ®è´¨é‡ã€‚å¯¹äºzero-shotï¼Œä¸tuneæ¨¡å‹è€Œç›´æ¥ä½¿ç”¨SUBRï¼›å¯¹äºfew-shotï¼Œé™¤äº†ä½¿ç”¨SUBRï¼Œè¿˜è¦ç”¨åŸå§‹å’ŒSUBRå¢å¼ºçš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚SUBRå…·ä½“æ˜¯å¯¹å†å²ç‰©å“å’Œç›®æ ‡ç‰©å“åšPCAé™ç»´ï¼ˆå®ç°å»å™ªï¼‰ï¼Œç„¶åcosineè®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ›¿æ¢åŸå…ˆçš„æœ€è¿‘Kä¸ªäº¤äº’å†å²ã€‚ |
|     TASTEâœ…     | Text Matching Improves Sequential Recommendation by Reducing Popularity Biases                                      |          T5-base (223M)          |            Full Finetuning            |       CIKM 2023       |            [[Code]](https://github.com/OpenMatch/TASTE)            | å°†ç”¨æˆ·äº¤äº’å†å²å’Œç‰©å“ä¿¡æ¯è¡¨ç¤ºä¸ºIDå’Œæ–‡æœ¬ä¿¡æ¯çš„æ‹¼æ¥å½¢å¼ï¼Œåˆ©ç”¨æ–‡æœ¬è¡¨ç¤ºåŒ¹é…è¿›è¡Œæ¨èã€‚æ­¤å¤–ï¼ŒTASTEå»ºæ¨¡ç”¨æˆ·é•¿åºåˆ—é€šè¿‡æ³¨æ„åŠ›ç¨€ç–åŒ–æ–¹æ³•ï¼Œå³å°†ç”¨æˆ·å†å²ç‰©å“åˆ†ä¸ºå¤šä¸ªå­åºåˆ—åˆ†åˆ«è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åå†å°†è®¡ç®—å‡ºæ¥çš„è¡¨ç¤ºæ‹¼æ¥ã€‚ä½¿ç”¨in-batchå’Œéšæœºé‡‡æ ·çš„ç‰©å“è¿›è¡Œå¯¹æ¯”æŸå¤±è®¡ç®—ã€‚ |
|      N/A      | Unveiling Challenging Cases in Text-based Recommender Systems                                                       |         BERT-base (110M)         |            Full Finetuning            | RecSys Workshop 2023 |         [[Paper]](https://ceur-ws.org/Vol-3476/paper5.pdf)         |                    |
|  ClickPromptâœ…  | ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction                 |       RoBERTa-large (355M)       |            Full Finetuning            |       WWW 2024       |            [[Paper]](https://arxiv.org/abs/2310.09234)            | ä¼ ç»ŸCTRå»ºæ¨¡IDä¿¡æ¯çš„æ–¹å¼ä¼šå¤±å»æ–‡æœ¬å†…åœ¨çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€ŒPLMæ— æ³•å»ºæ¨¡ååŒçŸ¥è¯†ï¼ˆIDç‰¹å¾æ— è¯­ä¹‰ã€ç‰¹å¾é€šè¿‡æ¨¡ç‰ˆå¡«å……çº¿æ€§ç»„åˆä¸ºæ–‡æœ¬è€Œä¸å…·å¤‡ç‰¹å¾äº¤å‰èƒ½åŠ›ï¼‰ã€‚æœ¬æ–‡æå‡ºå°†IDç‰¹å¾å˜æ¢ä¸ºäº¤äº’æ„ŸçŸ¥çš„soft promptï¼Œå†é€šè¿‡ä¼ ç»ŸCTRæ¨¡å‹å’ŒPLMçš„pretrain-finetuneå­¦ä¹ æœºåˆ¶ï¼Œå®ç°å¯¹è¯­ä¹‰çŸ¥è¯†å’Œåä½œçŸ¥è¯†çš„å»ºæ¨¡ã€‚ |
|  SetwiseRank  | A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models                  |        FLAN-T5-XXL (11B)        |                 Frozen                 |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.09497)            |                    |
|      UPSR      | Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language                                             |          T5-base (223M)          |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.13540)            |                    |
|    LLM-Rec    | One Model for All: Large Language Models are Domain-Agnostic Recommendation Systems                                 |            OPT (6.7B)            |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.14304)            |                    |
|   LLMRanker   | Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels                        |           FLAN PaLM2 S           |                 Frozen                 |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.14122)            |                    |
|     CoLLM     | CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation                           |           Vicuna (7B)           |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.19488)            |                    |
|      FLIP      | FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction      |       RoBERTa-large (355M)       |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2310.19453)            |                    |
| BTRec | BTRec: BERT-Based Trajectory Recommendation for Personalized Tours | BERT-base (110M) | Full Finetuning | Arxiv 2023 | [[Paper]](https://arxiv.org/abs/2310.19886) | |
|    CLLM4Recâœ…    | Collaborative Large Language Model for Recommender Systems                                                          |           GPT2 (110M)           |            Full Finetuning            |      WWW 2024      |            [[Code]](https://github.com/yaochenzhu/LLM4Rec)            | è§£å†³LLMå’Œæ¨èä»»åŠ¡ä¹‹é—´çš„è¯­ä¹‰gapï¼Œå¦‚è¯­è¨€å»ºæ¨¡å¼•å…¥ç”¨æˆ·/ç‰©å“çš„è™šå‡å…³è”ï¼ˆå¦‚user_4332å’Œuser_43,user_43ï¼Œæˆ–è€…è¯­è¨€å…·æœ‰é¡ºåºï¼Œè€Œäº¤äº’å†å²ç‰©å“ä¸éœ€è¦è€ƒè™‘é¡ºåºï¼‰ï¼›ç›´æ¥è¿›è¡Œè‡ªå›å½’çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡å¯¹äºæ¨èä»»åŠ¡æ˜¯ä¸é«˜æ•ˆã€ä¸ç¨³å®šçš„ï¼›ä½¿ç”¨å†…å®¹è¡¨ç¤ºç‰©å“å’Œç”¨æˆ·ï¼Œå®¹æ˜“å¼•å…¥å™ªå£°ã€‚æœ¬æ–‡æå‡ºç»“åˆLLMèŒƒå¼å’Œæ¨èIDèŒƒå¼ï¼Œè®¾è®¡soft+hard promptingç­–ç•¥ï¼ˆå«ååŒå’Œå†…å®¹çš„åŒLLMï¼‰ï¼Œäº’æ­£åˆ™åŒ–æœºåˆ¶é™åˆ¶æ¨¡å‹å­¦ä¹ æ›´ç›¸å…³çš„æ¨èä¿¡æ¯ï¼Œä½¿ç”¨å¤šé¡¹ä¼¼ç„¶é¢„æµ‹å¤´è¿›è¡Œé«˜æ•ˆæ¨æ–­ã€‚ |
|      CUP      | Recommendations by Concise User Profiles from Review Text                                                           |         BERT-base (110M)         |         Last-layer Finetuning         |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2311.01314)            |                    |
|      N/A      | Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers                                    |         FLAN-T5-XL (3B)         |            Full Finetuning            |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2311.01555)            |                    |
|    CoWPiRec    | Collaborative Word-based Pre-trained Item Representation for Transferable Recommendation                            |         BERT-base (110M)         |            Full Finetuning            |       ICDM 2023       |            [[Paper]](https://arxiv.org/abs/2311.10501)            |                    |
|     E4SRec     | E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation   |           LLaMA2 (13B)           |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2312.02443)            |                    |
|      CER      | The Problem of Coherence in Natural Language Explanations of Recommendations                                        |           GPT2 (110M)           |            Full Finetuning            |       ECAI 2023       |            [[Paper]](https://arxiv.org/abs/2312.11356)            |                    |
|      LSAT      | Preliminary Study on Incremental Learning for Large Language Model-based Recommender Systems                        |            LLaMA (7B)            |                  LoRA                  |      Arxiv 2023      |            [[Paper]](https://arxiv.org/abs/2312.15599)            |                    |
|   Llama4Rec   | Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation              |           LLaMA2 (7B)           |            Full Finetuning            |      Arxiv 2024      |            [[Paper]](https://arxiv.org/abs/2401.13870)            |                    |
| PFCRâœ… | Prompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation |  | Full Finetuning | WWW 2024 | [[Code]](https://github.com/Ckano/PFCR) | å·²æœ‰è·¨åŸŸæ¨èè”é‚¦ç®—æ³•å­˜åœ¨ç¼ºé™·ï¼š(1)ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯ä¸Šä¼ åˆ°ä¸­å¿ƒæœåŠ¡å™¨ï¼Œå®¹æ˜“æ³„æ¼ç”¨æˆ·éšç§ï¼›(2)ä¾èµ–äºIDè¡¨ç¤ºï¼Œä¸åˆ©äºä¸åŒåŸŸä¹‹é—´è¿ç§»ï¼›(3)åŸºäºè·¨åŸŸä¹‹é—´å­˜åœ¨é‡å ç”¨æˆ·å‡è®¾ã€‚æœ¬æ–‡æå‡ºï¼š(1) å°†æ¯ä¸ªåŸŸè§†ä½œclientï¼Œç”¨æˆ·éšç§æ•°æ®è¿›è¡Œå±€éƒ¨æ›´æ–°ï¼Œåªä¸Šä¼ æ¯ä¸ªclientçš„ç‰©å“ç›¸å…³æ¢¯åº¦ï¼›(2)ä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºç‰©å“ä¿¡æ¯ï¼ˆä¸ºé˜²æ­¢recommenderè¿‡äºå¼ºè°ƒæ–‡æœ¬ç‰¹å¾ï¼Œä½¿ç”¨text->code->representationæœºåˆ¶ï¼‰ï¼›(3)æå‡ºFull promptingæœºåˆ¶ï¼ˆåºåˆ—promptã€ç”¨æˆ·promptã€domain promptï¼‰å’ŒLight Promptingæœºåˆ¶ï¼ˆåºåˆ—promptingã€åŸŸpromptingï¼‰å®ç°åŸŸè‡ªé€‚åº”å¾®è°ƒã€‚ |

<h4 id="1.3.2">1.3.2 Item Generation Task</h4>

| **Name** | **Paper**                                                                                                 | **LLM Backbone (Largest)** | **LLM Tuning Strategy** |     **Publication**     |                          **Link**                          | Main Contributions |
| :------------: | :-------------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :----------------------------: | :--------------------------------------------------------------: | ------------------ |
|    GPT4Rec    | GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation               |           GPT2 (110M)           |        Full Finetuning        |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2304.03879)             |                    |
|      VIP5âœ…      | VIP5: Towards Multimodal Foundation Models for Recommendation                                                   |          T5-base (223M)          |    Layerwise Adater Tuning    |           EMNLP 2023           |             [[Paper]](https://arxiv.org/abs/2305.14302)             | å¦‚ä½•åœ¨ä¸ä¿®æ”¹åŸå§‹ç»“æ„ä¸‹å°†å¤šæ¨¡æ€æ¨èæ¨¡å‹é€‚ç”¨äºå„ç§ä»»åŠ¡å’Œæ¨¡æ€ç¼ºä¹ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºå°†ç‰©å“æ–‡æœ¬ã€å›¾åƒç¼–ç ä¸ºkä¸ªtokenï¼Œå¹¶å†»ç»“P5 backboneï¼Œåªè®­ç»ƒadapterï¼ˆAttentionå’ŒFFNä¹‹åçš„æ–°çŸ©é˜µï¼‰å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚ä¸‹æœ‰ä»»åŠ¡åŒ…æ‹¬ç›´æ¥æ¨èã€åºåˆ—ã€å¯è§£é‡Šã€‚ |
|     P5-IDâœ…     | How to Index Item IDs for Recommendation Foundation Models                                                      |          T5-small (60M)          |        Full Finetuning        |           SIGIR-AP 2023           |             [[Code]](https://github.com/Wenyueh/LLM-RecSys-ID)             | å…³æ³¨ç ”ç©¶LLMæ¨èçš„Item ID indexæ–¹å¼ã€‚æå‡ºåºåˆ—Indexã€ååŒIndexã€è¯­ä¹‰Indexå’Œæ··åˆIndexæ–¹å¼ã€‚ |
|    FaiRLLM    | Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation                  |             ChatGPT             |            Frozen            |          RecSys 2023          |             [[Paper]](https://arxiv.org/abs/2305.07609)             |                    |
|      PALR      | PALR: Personalization Aware LLMs for Recommendation                                                             |            LLaMA (7B)            |        Full Finetuning        |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2305.07622)             |                    |
|    ChatGPTâœ…    | Large Language Models are Zero-Shot Rankers for Recommender Systems                                             |             ChatGPT             |            Frozen            |           ECIR 2024           |             [[Code]](https://github.com/RUCAIBox/LLMRank)             | æ¢ç´¢LLMä½œä¸ºæ¨èæ’åºæ¨¡å‹çš„èƒ½åŠ›ã€‚å‘ç°ï¼ŒLLMå¯¹å†å²å€™é€‰ç‰©å“é¡ºåºä¸æ•æ„Ÿï¼Œä½¿ç”¨recency-focused promptingå’ŒICLå¯ä»¥æ”¹å–„ï¼›å¢åŠ å†å²ç‰©å“æ•°é‡ä¸ä¸€å®šå¢å¼ºè¡¨ç°ï¼Œå¯èƒ½åè€Œè´Ÿé¢ä½œç”¨ï¼›LLMä¼šå—åˆ°å€™é€‰ç‰©å“ä½ç½®åå·®ã€æµè¡Œåº¦åå·®ã€æ–‡æœ¬ç›¸ä¼¼åº¦å½±å“ã€‚æå‡ºboostrappingï¼ˆæ‰“ä¹±å€™é€‰ç‰©å“ï¼Œæ’åºBæ¬¡ï¼‰å¾—åˆ°æœ€ç»ˆæ’åºç»“æœã€‚Instruction Tuningè¿‡çš„LLMå’Œæ›´å¤§çš„LLMå¯¹äºæå‡æ¨èæ•ˆæœæœ‰å¸®åŠ©ã€‚ |
|      AGR      | Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT                                  |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2305.04518)             |                    |
|      NIR      | Zero-Shot Next-Item Recommendation using Large Pretrained Language Models                                       |           GPT3 (175B)           |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2304.03153)             |                    |
|     GPTRec     | Generative Sequential Recommendation with GPTRec                                                                |        GPT2-medium (355M)        |        Full Finetuning        |       Gen-IR@SIGIR 2023       |             [[Paper]](https://arxiv.org/abs/2306.11114)             |                    |
|    ChatNews    | A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News            |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2306.10702)             |                    |
|      N/A      | Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences     |            PaLM (62B)            |            Frozen            |          RecSys 2023          |             [[Paper]](https://arxiv.org/abs/2307.14225)             |                    |
|  LLMSeqPrompt  | Leveraging Large Language Models for Sequential Recommendation                                                  |         OpenAI ada model         |           Finetune           |          RecSys 2023          |             [[Paper]](https://arxiv.org/abs/2309.09261)             |                    |
|     GenRec     | GenRec: Large Language Model for Generative Recommendation                                                      |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2307.00457)             |                    |
| UP5âœ… | UP5: Unbiased Foundation Model for Fairness-aware Recommendation | T5-base (223M) | Prefix Tuning | Arxiv 2023 | [[Paper]](https://arxiv.org/abs/2305.12090) | ç ”ç©¶LLMæ¨èçš„ç”¨æˆ·å…¬å¹³æ€§ã€‚æå‡ºprefix promptå’Œåˆ†ç±»å™¨ä¹‹é—´å¯¹æŠ—è®­ç»ƒå®ç°å…¬å¹³æ€§ï¼›å…¶ä¸­prefix soft promptåˆ†åˆ«åŠ åœ¨encoderå’Œdecoderå‰é¢ï¼Œå¯¹äºå¤šä¸ªæ•æ„Ÿå±æ€§çš„æ··åˆï¼Œä½¿ç”¨target-attentionæœºåˆ¶ç”Ÿæˆå•ä¸€prefix promptã€‚ |
|     HKFRâœ…     | Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM |    ChatGPT+<br />ChatGLM (6B)    |            LoRA             |          RecSys 2023           |         [[Paper]](https://arxiv.org/abs/2308.03333)          | ä¼ ç»Ÿæ–¹æ³•å°†ç”¨æˆ·å¼‚è´¨ä¿¡æ¯ä¸æ¨¡å‹ç»“åˆï¼Œä¼šå‡ºç°ç‰¹å¾ç¨€ç–å’ŒçŸ¥è¯†ç¢ç‰‡åŒ–ï¼ˆç¼ºä¹ä¸åŒè¡Œä¸ºä¹‹é—´å¼‚è´¨çŸ¥è¯†çš„èåˆï¼‰ã€‚æœ¬æ–‡é€šè¿‡LLMå¸®åŠ©æå–ç”¨æˆ·ä¸åŒè¡Œä¸ºä¸­çš„å¼‚è´¨çŸ¥è¯†ï¼Œå¹¶ä½¿ç”¨instruct tuningçš„æ–¹å¼èåˆè¿™äº›çŸ¥è¯†å’Œæ¨èä»»åŠ¡ã€‚ |
| RecExplainerâœ… | RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability |         Vicuna-v1.3 (7B)         |            LoRA             |           Arxiv 2023           |         [[Paper]](https://arxiv.org/abs/2311.10947)          | åˆ©ç”¨å¾®è°ƒçš„LLMä½œä¸ºä»£ç†æ¨¡å‹ï¼Œå­¦ä¹ æ¨¡ä»¿å’Œç†è§£ç›®æ ‡æ¨èæ¨¡å‹ã€‚è®¾è®¡è¡Œä¸ºå¯¹é½å’Œæ„å›¾å¯¹é½ã€‚å‰è€…åŠ¨æœºæ˜¯å¦‚æœLLMèƒ½å¯¹é½ç›®æ ‡æ¨¡å‹ï¼Œé‚£ä¹ˆå°±èƒ½å¤Ÿæ¨¡ä»¿ç›®æ ‡æ¨¡å‹çš„é€»è¾‘åšå‡ºé¢„æµ‹ï¼›åè€…åŠ¨æœºæ˜¯å¦‚æœLLMèƒ½å¤Ÿä¿ç•™å…¶å¤šæ­¥æ¨ç†èƒ½åŠ›çš„åŒæ—¶ç†è§£ç›®æ ‡æ¨¡å‹çš„ç¥ç»å…ƒï¼Œåˆ™æœ‰å¯èƒ½é˜æ˜ç›®æ ‡æ¨¡å‹çš„å†³ç­–é€»è¾‘ã€‚ä¸‹æ¸¸ä»»åŠ¡åŒ…æ‹¬next item retrieval, item ranking, interest classification, and history reconstructionä»¥åŠè§£é‡Šç”Ÿæˆã€‚ |
|      N/A      | The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations      |             ChatGPT             |            Frozen            |           EAAMO 2023           |             [[Paper]](https://arxiv.org/abs/2308.02053)             |                    |
|     BIGRec     | A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems                                |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2308.08434)             |                    |
|     KP4SR     | Knowledge Prompt-tuning for Sequential Recommendation                                                           |          T5-small (60M)          |        Full Finetuning        |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2308.08459)             |                    |
|   RecSysLLM   | Leveraging Large Language Models for Pre-trained Recommender Systems                                            |            GLM (10B)            |             LoRA             |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2308.10837)             |                    |
| PODâœ… | Prompt Distillation for Efficient LLM-based Recommendation | T5-small (60M) | Full Finetuning | CIKM 2023 | [[Code]](https://github.com/lileipisces/POD) | å·²æœ‰LLM4Recéœ€è¦å°†ç”¨æˆ·å’Œç‰©å“ä¿¡æ¯åµŒå…¥åˆ°ç»™å®šçš„ç¦»æ•£æ¨¡ç‰ˆä¸­ï¼Œä½†ç”±äºå¤§é‡çš„ä»»åŠ¡ä¿¡æ¯æè¿°å¯èƒ½å¯¼è‡´å¦‚LLMè¯¯è§£ä¸ºå¦ä¸€ä¸ªä»»åŠ¡ï¼Œæˆ–è€…æ©ç›–äº†å…³é”®ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸ºæ¯ä¸ªä»»åŠ¡è®¾è®¡è¿ç»­promptï¼Œè®­ç»ƒæ—¶è¿ç»­å’Œç¦»æ•£promptåŒæ—¶å‡ºç°ï¼Œç›®çš„æ˜¯å°†ä»»åŠ¡çŸ¥è¯†è’¸é¦åˆ°è¿ç»­promptä¸­ï¼ˆæ¨æ–­æ—¶åªä¿ç•™è¿ç»­promptï¼‰ï¼›æ­¤å¤–è®¾è®¡ä»»åŠ¡äº¤æ›¿è®­ç»ƒæ–¹æ³•ï¼Œé˜²æ­¢ä¸åŒä»»åŠ¡å¥å­é•¿åº¦ä¸åŒå¯¼è‡´padæ—¶é—´è¿‡é•¿ã€‚ |
|      N/A      | Evaluating ChatGPT as a Recommender System: A Rigorous Approach                                                 |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2309.03613)             |                    |
|      RaRS      | Retrieval-augmented Recommender System: Enhancing Recommender Systems with Large Language Models                |             ChatGPT             |            Frozen            | RecSys Doctoral Symposium 2023 |    [[Paper]](https://dl.acm.org/doi/abs/10.1145/3604915.3608889)    |                    |
|   JobRecoGPT   | JobRecoGPT -- Explainable job recommendations using LLMs                                                        |               GPT4               |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2309.11805)             |                    |
|     LANCER     | Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling |           GPT2 (110M)           |         Prefix Tuning         |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2309.10435)             |                    |
|    TransRec    | A Multi-facet Paradigm to Bridge Large Language Model and Recommendation                                        |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2310.06491)             |                    |
|    AgentCFâœ…    | AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems                         | text-davinci-003 & gpt-3.5-turbo |            Frozen            |            WWW 2024            |             [[Paper]](https://arxiv.org/abs/2310.09233)             | å°†ç”¨æˆ·å’Œç‰©å“éƒ½ä½œä¸ºagentå»ºæ¨¡åŒè¾¹å…³ç³»å¹¶å®ç°ååŒä¼˜åŒ–å„è‡ªçš„memoryï¼Œreflectionç­‰å†…éƒ¨ç»“æ„ |
|      P4LM      | Factual and Personalized Recommendations using Language Models and Reinforcement Learning                       |             PaLM2-XS             |        Full Finetuning        |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2310.06176)             |                    |
|   InstructMK   | Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model                        |            LLaMA (7B)            |        Full Finetuning        |        CIKM GenRec 2023        |             [[Paper]](https://arxiv.org/abs/2310.16409)             |                    |
|    LightLM    | LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation                             |          T5-small (60M)          |        Full Finetuning        |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2310.17488)             |                    |
|    LlamaRecâœ…    | LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking                                      |           LLaMA2 (7B)           |             QLoRA             |         PGAI@CIKM 2023         |             [[Paper]](https://arxiv.org/abs/2311.02089)             | LLMç”±äºè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆå¯¼è‡´æ¨æ–­å»¶æ—¶é«˜ã€‚æœ¬æ–‡æå‡ºä¸¤é˜¶æ®µæ’åºæ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µæ˜¯ç”¨å°è§„æ¨¡æ¨¡å‹ç”Ÿæˆå€™é€‰ç‰©å“ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯ç”¨LLMæ ¹æ®è¯­è¨€å™¨çš„æ–¹æ³•å°†è¾“å‡ºlogitså˜ä¸ºå€™é€‰ç‰©å“æ¦‚ç‡ã€‚LLMä½¿ç”¨æŒ‡ä»¤æ•°æ®å¾®è°ƒï¼Œåªå¾®è°ƒindex letterå’ŒEOS tokenã€‚è¿™é‡Œindex letteræ˜¯æŒ‡promptä¸­ä¼šå½¢æˆå¦‚"(A) item title"ä¸­çš„Aã€‚ |
|      N/A      | Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study                                |              GPT-4V              |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2311.04199)             |                    |
|      N/A      | Exploring Fine-tuning ChatGPT for News Recommendation                                                           |             ChatGPT             |  gpt-3.5-urbo finetuning API  |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2311.05850)             |                    |
|      N/A      | Do LLMs Implicitly Exhibit User Discrimination in Recommendation? An Empirical Study                            |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Code]](https://github.com/ljy0ustc/LLaRA)             |                    |
|     LC-Rec     | Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation                        |            LLaMA (7B)            |             LoRA             |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2311.09049)             |                    |
|      DOKE      | Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations                          |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2311.10779)             |                    |
|   ControlRec   | ControlRec: Bridging the Semantic Gap between Language Model and Personalized Recommendation                    |          T5-base (223M)          |        Full Finetuning        |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2311.16441)             |                    |
|     LLaRAâœ…     | LLaRA: Aligning Large Language Models with Sequential Recommenders                                              |           LLaMA2 (7B)           |             LoRA             |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2312.02445)             | ä»…åˆ©ç”¨IDæˆ–è€…æ–‡æœ¬ï¼Œæ— æ³•å……åˆ†å‘æŒ¥ä¸–ç•ŒçŸ¥è¯†å’Œè¡Œä¸ºåºåˆ—æ¨¡å¼ã€‚æœ¬æ–‡å°†ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒå¾—åˆ°çš„embeddingå’Œæ–‡æœ¬æè¿°embeddingéƒ½æ”¾åœ¨promptä¸­ï¼›æ­¤å¤–ï¼Œè®¾è®¡è¯¾ç¨‹å­¦ä¹ ä»ç®€å•ä»»åŠ¡ï¼ˆçº¯æ–‡æœ¬ï¼‰åˆ°éš¾ä»»åŠ¡ï¼ˆæ–‡æœ¬+IDï¼‰å®ç°åºåˆ—è¡Œä¸ºçŸ¥è¯†è’¸é¦åˆ°LLM |
|     PO4ISR     | Large Language Models for Intent-Driven Session Recommendations                                                 |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2312.07552)             |                    |
|      DRDT      | DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation                        |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2312.11336)             |                    |
|   RecPrompt   | RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models                        |               GPT4               |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2312.10463)             |                    |
|      LiT5      | Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models            |            T5-XL (3B)            |        Full Finetuning        |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2312.16098)             |                    |
|     STELLA     | Large Language Models are Not Stable Recommender Systems                                                        |             ChatGPT             |            Frozen            |           Arxiv 2023           |             [[Paper]](https://arxiv.org/abs/2312.15746)             |                    |
|   Llama4Rec   | Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation          |           LLaMA2 (7B)           |        Full Finetuning        |           Arxiv 2024           |             [[Paper]](https://arxiv.org/abs/2401.13870)             |                    |
| VSTâœ… | Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models | GPT4<br />LLaVA (13B) | Frozen | Arxiv 2024 | [[Paper]](https://arxiv.org/pdf/2402.08670.pdf) | large vision-language model (LVLM)ç¼ºä¹ç”¨æˆ·åå¥½çŸ¥è¯†ï¼Œéš¾ä»¥è§£å†³åŒ…å«å¤§é‡ç¦»æ•£ã€å™ªå£°ã€å†—ä½™çš„å›¾åƒåºåˆ—ã€‚æœ¬æ–‡æå‡ºVSTæ¨¡å‹é€šè¿‡ä»ç‰©å“çš„å…³é”®å›¾åƒä¸­æå–æ€»ç»“ä¿¡æ¯ï¼ˆåˆ©ç”¨LVLMï¼‰ï¼Œå†å°†è¿™äº›ä¿¡æ¯å’Œç‰©å“titleæ‹¼æ¥ä½œä¸ºç”¨å“æ–‡æœ¬è¡¨ç¤ºã€‚ |

<h4 id="1.3.3">1.3.3 Hybrid Task</h4>

| **Name** | **Paper**                                                                                              | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |                **Link**                | Main Contributions                                                                                                                                                                                           |
| :------------: | :----------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :-------------------: | :------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
|      P5âœ…      | Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5) |          T5-base (223M)          |        Full Finetuning        |      RecSys 2022      |    [[Code]](https://github.com/jeykigung/P5)    | ä½¿ç”¨ç›¸åŒçš„è¯­è¨€å»ºæ¨¡ç›®æ ‡å®ç°ç»Ÿä¸€çš„æ¨èå¼•æ“ï¼ˆåºåˆ—æ¨è+ç›´æ¥æ¨è+è§£é‡Šç”Ÿæˆ+è¯„è®ºç›¸å…³+è¯„åˆ†é¢„æµ‹ï¼‰ï¼Œå®ç°åŸºäºpromptçš„æŒ‡ä»¤æ¨è                                                                                           |
|    M6-Recâœ…    | M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems                             |          M6-base (300M)          |         Option Tuning         |      Arxiv 2022      |   [[Paper]](https://arxiv.org/abs/2205.08084)   | ç”¨æˆ·è¡Œä¸ºè½¬åŒ–ä¸ºçº¯æ–‡æœ¬ï¼Œè®­ç»ƒä»»åŠ¡åŒ…æ‹¬CTR/CVR+è§£é‡Šç”Ÿæˆ+queryç”Ÿæˆ+å¯¹è¯æ¨è+äº§å“ç”Ÿæˆ+æ£€ç´¢ä»»åŠ¡ç­‰ï¼ˆæ— IDï¼‰ã€‚åŒæ—¶è®¾è®¡å¤šä¸ªä¼˜åŒ–æ“ä½œï¼Œå¦‚option tuningç­‰                                                                   |
| InstructRecâœ… | Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach            |         FLAN-T5-XL (3B)         |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2305.07001)   | è®¾è®¡39ç²—ç²’åº¦instruction promptï¼ˆPreference+Intention+Task Form+Contextï¼‰ç”¨äºä¸åŒåœºæ™¯ï¼Œå¹¶åŸºäºæ­¤è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªç»†ç²’åº¦ä¸ªæ€§åŒ–instructionï¼ˆæ— IDï¼‰ï¼Œå°†æ¨èä»»åŠ¡è½¬åŒ–ä¸ºinstruction followingèŒƒå¼ï¼Œå®ç°ç”¨æˆ·è‡ªç”±åœ°è¡¨è¾¾éœ€æ±‚ |
|   ChatGPTâœ…   | Is ChatGPT a Good Recommender? A Preliminary Study                                                           |             ChatGPT             |            Frozen            |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2304.10149)   | æ¢æµ‹ChatGPTåœ¨Aè¯„åˆ†é¢„æµ‹ã€Bç›´æ¥æ¨èã€Cåºåˆ—æ¨èã€Dè§£é‡Šç”Ÿæˆå’ŒEè¯„è®ºæ€»ç»“äº”æ–¹é¢çš„èƒ½åŠ›ã€‚ç»“è®ºæ˜¯Aã€Då’ŒEè¡¨ç°è¾ƒä½³ï¼Œä½†æ˜¯Bå’ŒCè¡¨ç°å·®                                                                                        |
|   ChatGPTâœ…   | Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent                           |             ChatGPT             |            Frozen            |      Arxiv 2023      | [[Code]](https://github.com/sunnweiwei/RankGPT) | å·²æœ‰å·¥ä½œç¼ºä¹å¯¹LLMå¯¹äºæ–‡æ¡£é‡æ‹èƒ½åŠ›çš„æ¢ç´¢ï¼Œæœ¬æ–‡éªŒè¯äº†GPT4ç›¸æ¯”äºSOTAçš„ä¼˜è¶Šæ€§ï¼Œæ­¤å¤–æœ¬æ–‡ä¹Ÿå°†ChatGPTæ’åºèƒ½åŠ›è’¸é¦åˆ°å°æ¨¡å‹ï¼Œå®ç°æ€§èƒ½çš„æå‡                                                                           |
|   ChatGPTâœ…   | Uncovering ChatGPT's Capabilities in Recommender Systems                                                     |             ChatGPT             |            Frozen            |      RecSys 2023      |  [[Code]](https://github.com/rainym00d/LLM4RS)  | æ¢æµ‹ChatGPTåœ¨point-wiseï¼Œpair-wiseå’Œlist-wiseä¸‹çš„æ¨èæ€§èƒ½                                                                                                                                                    |
| BDLMâœ… | Bridging the Information Gap Between Domain-Specific Model and General LLM for Personalized Recommendation | Vicuna (7B) | Full Finetuning | Arxiv 2023 | [[Paper]](https://arxiv.org/abs/2311.03778) | å¤§æ¨¡å‹ä»¥æ–‡æœ¬å½¢å¼è¡¨è¾¾éš¾ä»¥åŒºåˆ†ç›¸ä¼¼ä½†ä»æœ‰å¾®å°åŒºåˆ«çš„å•†å“ï¼Œä¸”éš¾ä»¥è¡¨ç¤ºå¤æ‚çš„ç”¨æˆ·è¡Œä¸ºæ¨¡å¼ï¼›ä¼ ç»Ÿdomain-specificæ¨¡å‹éš¾ä»¥åœ¨æ•°æ®ç¨€ç–åœºæ™¯è¡¨ç°å¥½ã€‚æå‡ºä¿¡æ¯å…±äº«æ¨¡å—ï¼Œç”¨æˆ·/ç‰©å“ID tokenæ‰©å……è¯è¡¨ï¼Œè”åˆæ¨¡å‹è®­ç»ƒç­‰æ–¹æ³•ã€‚ |
|  RecRankerâœ…  | RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation                        |           LLaMA2 (13B)           |        Full Finetuning        |      Arxiv 2023      |   [[Paper]](https://arxiv.org/abs/2312.16018)   | æ··åˆæ’åºï¼ˆpoint+pair+listï¼‰æŒ‡ä»¤æ„å»ºï¼Œpromptä½ç½®æ¶ˆåï¼Œè‡ªé€‚åº”ç”¨æˆ·é‡‡æ ·ï¼ˆé‡è¦æ€§ã€cluster-basedï¼Œé‡å¤æƒ©ç½šï¼‰ï¼Œæ¨æ–­æ—¶ä½¿ç”¨ä¸‰ç§æ’åºä»»åŠ¡çš„ï¼ˆè°ƒæ•´åï¼‰åˆ†æ•°ä¹‹å’Œã€‚                                                         |

</p>
</details>

<details><summary><h3 id="1.4">1.4 LLM for User Interaction</h3></summary>
<p>
<h4 id="1.4.1">1.4.1 Task-oriented User Interaction</h4>
<h4 id="1.4.1">1.4.1 Task-oriented User Interaction</h4>


| **Name** | **Paper**                                                                                                     | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |              **Link**              | Main Contributions |
| :------------: | :------------------------------------------------------------------------------------------------------------------ | :------------------------------: | :---------------------------: | :-------------------: | :--------------------------------------: | ------------------ |
|   TG-ReDial   | Towards Topic-Guided Conversational Recommender System                                                              |  BERT-base (110M) & GPT2 (110M)  |            Unknown            |      COLING 2020      | [[Paper]](https://arxiv.org/abs/2010.04125) |                    |
|      TCP      | Follow Me: Conversation Planning for Target-driven Recommendation Dialogue Systems                                  |         BERT-base (110M)         |        Full Finetuning        |      Arxiv 2022      | [[Paper]](https://arxiv.org/abs/2208.03516) |                    |
|      MESE      | Improving Conversational Recommendation Systems' Quality with Context-Aware Item Meta-Information                   |  DistilBERT (67M) & GPT2 (110M)  |        Full Finetuning        |       ACL 2022       | [[Paper]](https://arxiv.org/abs/2112.08140) |                    |
|    UniMIND    | A Unified Multi-task Learning Framework for Multi-goal Conversational Recommender Systems                           |         BART-base (139M)         |        Full Finetuning        |     ACM TOIS 2023     | [[Paper]](https://arxiv.org/abs/2204.06923) |                    |
|     VRICR     | Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation                            |         BERT-base (110M)         |        Full Finetuning        |       WSDM 2023       | [[Paper]](https://arxiv.org/abs/2212.11868) |                    |
|      KECR      | Explicit Knowledge Graph Reasoning for Conversational Recommendation                                                |  BERT-base (110M) & GPT2 (110M)  |            Frozen            |     ACM TIST 2023     | [[Paper]](https://arxiv.org/abs/2305.00783) |                    |
|      N/A      | Large Language Models as Zero-Shot Conversational Recommenders                                                      |               GPT4               |            Frozen            |       CIKM 2023       | [[Paper]](https://arxiv.org/abs/2308.10053) |                    |
|    MuseChat    | MuseChat: A Conversational Music Recommendation System for Videos                                                   |           Vicuna (7B)           |             LoRA             |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2310.06282) |                    |
|      N/A      | Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue |        Chinese-Alpaca (7B)        |             LoRA             |  EMNLP 2023 Findings  | [[Paper]](https://arxiv.org/abs/2310.14626) |                    |

<h4 id="1.4.2">1.4.2 Open-ended User Interaction</h4>

| **Name**  | **Paper**                                                    |  **LLM Backbone (Largest)**   |  **LLM Tuning Strategy**   | **Publication** |                  **Link**                   | Main Contributions |
| :-------: | :----------------------------------------------------------- | :---------------------------: | :------------------------: | :-------------: | :-----------------------------------------: | ------------------ |
|  BARCOR   | BARCOR: Towards A Unified Framework for Conversational Recommendation Systems |       BART-base (139M)        | Selective-layer Finetuning |   Arxiv 2022    | [[Paper]](https://arxiv.org/abs/2203.14257) |                    |
| RecInDial | RecInDial: A Unified Framework for Conversational Recommendation with Pretrained Language Models |        DialoGPT (110M)        |      Full Finetuning       |    AACL 2022    | [[Paper]](https://arxiv.org/abs/2110.07477) |                    |
|  UniCRS   | Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning |     DialoGPT-small (176M)     |           Frozen           |    KDD 2022     | [[Paper]](https://arxiv.org/abs/2206.09363) |                    |
|   T5-CR   | Multi-Task End-to-End Training Improves Conversational Recommendation |        T5-base (223M)         |      Full Finetuning       |   Arxiv 2023    | [[Paper]](https://arxiv.org/abs/2305.06218) |                    |
|    TtW    | Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation | T5-base (223M) & T5-XXL (11B) |  Full Finetuning & Frozen  |   Arxiv 2023    |                                             |                    |
|    N/A    | Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models |            ChatGPT            |           Frozen           |   EMNLP 2023    | [[Paper]](https://arxiv.org/abs/2305.13112) |                    |

</p>
</details>

<details><summary><h3 id="1.5">1.5 LLM for RS Pipeline Controller</h3></summary>
<p>


| **Name** | **Paper**                                                                                  | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** |              **Link**              | Main Contributions |
| :------------: | :----------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :-------------------: | :--------------------------------------: | ------------------ |
|    Chat-REC    | Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System                  |             ChatGPT             |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2303.14524) |                    |
|     RecLLM     | Leveraging Large Language Models in Conversational Recommender Systems                           |            LLaMA (7B)            |        Full Finetuning        |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2305.07961) |                    |
|      RAH      | RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models |               GPT4               |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2308.09904) |                    |
|    RecMindâœ…    | RecMind: Large Language Model Powered Agent For Recommendation                                   |             ChatGPT             |            Frozen            |      PGAI@CIKM 2023      | [[Paper]](https://arxiv.org/abs/2308.14296) | ç¼ºä¹å…³äºLLMå¦‚ä½•å›åº”ä¸ªæ€§åŒ–æŸ¥è¯¢ï¼ˆå¦‚æ¨èè¯·æ±‚ï¼‰çš„ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºLLMèµ‹èƒ½çš„æ¨èæ™ºèƒ½ä½“ï¼Œé€šè¿‡è®¡åˆ’ï¼Œåˆ©ç”¨å·¥å…·è·å–å¤–éƒ¨çŸ¥è¯†ï¼Œåˆ©ç”¨ç§äººæ•°æ®å®ç°ç²¾å‡†ä¸ªæ€§åŒ–æ¨èã€‚ä¸»è¦æå‡ºç®—æ³•ä¸ºâ€œSelf- Inspiringâ€ï¼Œè€ƒè™‘ä¹‹å‰æ¢ç´¢è¿‡çš„çŠ¶æ€æŒ‡å¯¼ä¸‹ä¸€æ­¥ï¼ˆä¸åŒäºToTå’ŒCoTé—å¼ƒäº†ä¹‹å‰æ¢ç´¢è¿‡çš„åˆ†æ”¯ï¼‰ã€‚åœ¨ä¸‹æ¸¸å¤šä¸ªä»»åŠ¡ï¼ˆè¯„åˆ†é¢„æµ‹ã€åºåˆ—æ¨èã€ç›´æ¥æ¨èã€è§£é‡Šç”Ÿæˆã€è¯„è®ºæ€»ç»“ï¼‰å®ç°ä¸P5ç›¸å½“çš„æ€§èƒ½ã€‚ |
|  InteRecAgent  | Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations          |               GPT4               |            Frozen            |      Arxiv 2023      | [[Paper]](https://arxiv.org/abs/2308.16505) |                    |
|      CORE      | Lending Interaction Wings to Recommender Systems with Conversational Agents                      |               N/A               |              N/A              |       NIPS 2023       | [[Paper]](https://arxiv.org/abs/2310.04230) |                    |

</p>
</details>

<details><summary><h3 id="1.6">1.6 Other Related Papers</h3></summary>
<p>

<h4 id="1.6.1">1.6.1 Related Survey Papers</h4> 


| **Paper**                                                                                                                |        **Publication**        |              **Link**              | Main Contributions |
| :----------------------------------------------------------------------------------------------------------------------------- | :---------------------------------: | :--------------------------------------: | ------------------ |
| Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis                      |             Arixv 2024             | [[Paper]](https://arxiv.org/abs/2401.04997) |                    |
| User Modeling in the Era of Large Language Models: Current Research and Future Directions                                      | IEEE Data Engineering Bulletin 2023 | [[Paper]](https://arxiv.org/abs/2312.11518) |                    |
| A Survey on Large Language Models for Personalized and Explainable Recommendations                                             |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2311.12338) |                    |
| Large Language Models for Generative Recommendation: A Survey and Visionary Discussions                                        |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2309.01157) |                    |
| Large Language Models for Information Retrieval: A Survey                                                                      |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2308.07107) |                    |
| When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities                                  |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2307.16376) |                    |
| Recommender Systems in the Era of Large Language Models (LLMs)                                                                 |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2307.02046) |                    |
| A Survey on Large Language Models for Recommendation                                                                           |             Arxiv 2023             | [[Paper]](https://arxiv.org/abs/2305.19860) |                    |
| Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems |              TACL 2023              | [[Paper]](https://arxiv.org/abs/2302.03735) |                    |
| Self-Supervised Learning for Recommender Systems: A Survey                                                                     |              TKDE 2022              | [[Paper]](https://arxiv.org/abs/2203.15876) |                    |

<h4 id="1.6.2">1.6.2 Other Papers</h4>

| **Paper**                                                                         | **Publication** |                       **Link**                       | Main Contributions |
| :-------------------------------------------------------------------------------------- | :-------------------: | :--------------------------------------------------------: | ------------------ |
| Large Language Model Can Interpret Latent Space of Sequential Recommender               |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2310.20487)          |                    |
| Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2309.01026)          |                    |
| INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning  |      Arxiv 2024      |          [[Paper]](https://arxiv.org/abs/2401.06532)          |                    |
| Evaluation of Synthetic Datasets for Conversational Recommender Systems                 |      Arxiv 2023      |         [[Paper]](https://arxiv.org/abs/2212.08167v1)         |                    |
| Generative Recommendation: Towards Next-generation Recommender Paradigm                 |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2304.03516)          |                    |
| Towards Personalized Prompt-Model Retrieval for Generative Recommendation               |      Arxiv 2023      |          [[Paper]](https://arxiv.org/abs/2308.02205)          |                    |
| Generative Next-Basket Recommendation                                                   |      RecSys 2023      | [[Paper]](https://dl.acm.org/doi/abs/10.1145/3604915.3608823) |                    |

</p>
</details>

<details><summary><h3 id="1.7">1.7 Paper Pending List: to be Added to Our Survey Paper</h3></summary>
<p>


|  **Name**  | **Paper**                                                                                                                         | **LLM Backbone (Largest)** | **LLM Tuning Strategy** |     **Publication**     |                       **Link**                       |
| :---------------: | :-------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------: | :---------------------------: | :----------------------------: | :--------------------------------------------------------: |
|                  | A Large Language Model Enhanced Conversational Recommender System                                                                       |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2308.06212)          |
|                  | User-Centric Conversational Recommendation: Adapting the Need of User with Large Language Models                                        |                                  |                              | RecSys Doctoral Symposium 2023 | [[Paper]](https://dl.acm.org/doi/abs/10.1145/3604915.3608885) |
|                  | Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation                         |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2310.16738)          |
|                  | Enhancing Recommender Systems with Large Language Model Reasoning Graphs                                                                |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2308.10835)          |
|                  | Large Language Model based Long-tail Query Rewriting in Taobao Search                                                                   |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2311.03758)          |
|                  | Knowledge Graphs and Pre-trained Language Models enhanced Representation Learning for Conversational Recommender Systems                |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.10967)          |
|                  | Unlocking the Potential of Large Language Models for Explainable Recommendations                                                        |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.15661)          |
|                  | The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective                                                  |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.15524)          |
|                  | Empowering Few-Shot Recommender Systems with Large Language Models -- Enhanced Representations                                          |                                  |                              |          IEEE Access          |          [[Paper]](https://arxiv.org/abs/2312.13557)          |
|                  | dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models         |                                  |                              |           Arxiv 2023           |          [[Paper]](https://arxiv.org/abs/2312.13264)          |
| Logic-Scaffolding | Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs                                      |           Falcon (40B)           |            Frozen            |           WSDM 2024           |          [[Paper]](https://arxiv.org/abs/2312.14345)          |
|                  | Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.04057)          |
|                  | ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback                                        |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.03605)          |
|                  | Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations in Recommender Systems                                    |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.04474)          |
|                  | Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency                           |                                  |                              |           Arxiv 2024           |          [[Paper]](https://arxiv.org/abs/2401.10545)          |
|                  | LLM4Vis: Explainable Visualization Recommendation using ChatGPT                                                                         |                                  |                              |           EMNLP 2023           |          [[Paper]](https://arxiv.org/abs/2310.07652)          |
|                  | Parameter-Efficient Conversational Recommender System as a Language Processing Task                                                     |                                  |                              |           EACL 2024           |          [[Paper]](https://arxiv.org/abs/2401.14194)          |
| | Data-efficient Fine-tuning for LLM-based Recommendation | | | Arxiv 2024 | [[Paper]](https://arxiv.org/abs/2401.17197) |
| | LoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks | | | Arxiv 2024 |  |
| | PAP-REC: Personalized Automatic Prompt for Recommendation Language Model | | | Arxiv 2024 |  |
| | From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models | | | Arxiv 2024 |  |
| | Uncertainty-Aware Explainable Recommendation with Large Language Models | | | Arxiv 2024 | [[Paper]](https://arxiv.org/abs/2402.03366) |

</p>
</details>

<details><summary><h2 id="2">2. LLM & Graph</h2></summary>
<p>

| **Name** | **Paper** | **LLM Backbone (Largest)** | **LLM Tuning Strategy** | **Publication** | **Link** | Main Contributions |
| :------------: | :-------------- | :------------------------------: | :---------------------------: | :-------------------: | :------------: | :----------------: |
| GraphGPTâœ… | GraphGPT: Graph Instruction Tuning for Large Language Models | Vicuna (7B) | Frozen | Arxiv 2023 | [[Code]](https://github.com/HKUDS/GraphGPT) | å·²æœ‰é¢„è®­ç»ƒå›¾Embeddingä¾èµ–äºä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒï¼Œé™åˆ¶äº†å…¶åœ¨å°‘é‡æ•°æ®æˆ–æ•°æ®ä¸å¯ç”¨çš„åœºæ™¯ã€‚æœ¬æ–‡æå‡ºGraphGPTæ¡†æ¶å¯¹é½LLMå’Œå›¾ç»“æ„çŸ¥è¯†é€šè¿‡ä¸¤é˜¶æ®µçš„æŒ‡ä»¤å¾®è°ƒï¼ŒåŒ…æ‹¬SSLæŒ‡ä»¤ï¼ˆæ–‡æœ¬å’Œå›¾è¡¨ç¤ºåŒ¹é…ï¼‰+å…·ä½“ä»»åŠ¡å›¾æŒ‡ä»¤ï¼ˆèŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ï¼‰ï¼ŒåŒæ—¶åˆ©ç”¨ChatGPTçš„CoTæŠ€æœ¯è’¸é¦å¼€æºLLMï¼›å°†å…·ä½“ä»»åŠ¡æŒ‡ä»¤å’ŒCoTæŒ‡ä»¤æ•°æ®æ··åˆä½œä¸ºæ¨¡å‹å¾®è°ƒæ•°æ®ã€‚ |
| InstructGraphâœ… | InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment | LLaMA (7B) | LoRA | Arxiv 2024 | [[Paper]](https://arxiv.org/pdf/2402.08785.pdf) | èµ‹äºˆLLMå›¾æ¨ç†å’Œå›¾ç”Ÿæˆçš„èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨åå¥½å¯¹é½è§£å†³ç”Ÿæˆå¹»è±¡é—®é¢˜ã€‚ç¬¬ä¸€æ­¥ï¼Œå°†å›¾ç¼–ç ä¸ºcode_likeçš„åŸºæœ¬å˜é‡ï¼Œå¹¶è®¾è®¡å›¾ç»“æ„å»ºæ¨¡ã€å›¾è¯­è¨€å»ºæ¨¡ã€å›¾ç”Ÿæˆå»ºæ¨¡å’Œå›¾æ€ç»´å»ºæ¨¡ä½œä¸ºæŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚ç¬¬äºŒæ­¥ï¼Œä¸ºäº†è§£å†³å›¾æ¨ç†å’Œå›¾ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è±¡ï¼Œåˆ©ç”¨DPOå¯¹é½æ–¹æ³•å‡è½»ã€‚ |

</p>
</details>

<h2 id="3"> 3. Datasets & Benchmarks </h2>

The datasets & benchmarks for LLM-related RS topics should maintain the original semantic/textual features, instead of anonymous feature IDs.

<h3 id="3.1">3.1 Datasets</h3>

| **Dataset** | **RS Scenario** |                                                               **Link**                                                               | Main Contributions |
| :---------------: | :--------------------: | :----------------------------------------------------------------------------------------------------------------------------------------: | ------------------ |
|   Reddit-Movie   | Conversational & Movie | [[Link]](https://github.com/AaronHeee/LLMs-as-Zero-Shot-Conversational-RecSys#large-language-models-as-zero-shot-conversational-recommenders) |                    |
|     Amazon-M2     |       E-commerce       |                                                  [[Link]](https://arxiv.org/abs/2307.09688)                                                  |                    |
|     MovieLens     |         Movie         |                                            [[Link]](https://grouplens.org/datasets/movielens/1m/)                                            |                    |
|      Amazon      |       E-commerce       |                                   [[Link]](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews)                                   |                    |
|   BookCrossing   |          Book          |                                        [[Link]](http://www2.informatik.uni-freiburg.de/~cziegler/BX/)                                        |                    |
|     GoodReads     |          Book          |                                          [[Link]](https://mengtingwan.github.io/data/goodreads.html)                                          |                    |
|       Anime       |         Anime         |                             [[Link]](https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database)                             |                    |
|     PixelRec     |      Short Video      |                                              [[Link]](https://github.com/westlake-repl/PixelRec)                                              |                    |
|      Netflix      |         Movie         |                                                   [[Link]](https://github.com/HKUDS/LLMRec)                                                   |                    |

<h3 id="3.2">3.2 Benchmarks</h3>

|   **Benchmarks**   |                                      **Webcite Link**                                      |             **Paper**             | Main Contributions |
| :----------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------: | ------------------ |
| Amazon-M2 (KDD Cup 2023) | [[Link]](https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendation-challenge) | [[Paper]](https://arxiv.org/abs/2307.09688) |                    |
|          LLMRec          |                           [[Link]](https://github.com/williamliujl/LLMRec)                           | [[Paper]](https://arxiv.org/abs/2308.12241) |                    |
|          OpenP5          |                           [[Link]](https://github.com/agiresearch/OpenP5)                           | [[Paper]](https://arxiv.org/abs/2306.11134) |                    |
|          TABLET          |                             [[Link]](https://dylanslacks.website/Tablet)                             | [[Paper]](https://arxiv.org/abs/2304.13188) |                    |

<h2 id="4">Related Repositories</h2>

|                                                  **Repo Name**                                                  |           **Maintainer**           |           Link           |
| :-------------------------------------------------------------------------------------------------------------------: | :--------------------------------------: | :--------------------------------------: |
|                            [rs-llm-paper-list](https://github.com/wwliu555/rs-llm-paper-list)                            |   [wwliu555](https://github.com/wwliu555)   |      |
| [awesome-recommend-system-pretraining-papers](https://github.com/archersama/awesome-recommend-system-pretraining-papers) | [archersama](https://github.com/archersama) |  |
|                                        [LLM4Rec](https://github.com/WLiK/LLM4Rec)                                        |       [WLiK](https://github.com/WLiK)       |              |
|                       [Awesome-LLM4RS-Papers](https://github.com/nancheng58/Awesome-LLM4RS-Papers)                       | [nancheng58](https://github.com/nancheng58) |  |
|                               [LLM4IR-Survey](https://github.com/RUC-NLPIR/LLM4IR-Survey)                               |  [RUC-NLPIR](https://github.com/RUC-NLPIR)  |    |
| [Awesome-LLM-for-RecSys](https://github.com/CHIANGEL/Awesome-LLM-for-RecSys) | [Jianghao Lin](https://github.com/CHIANGEL) | [[Survey]](https://github.com/CHIANGEL/Awesome-LLM-for-RecSys) |
| [Awesome-LLM-Uncertainty-Reliability-Robustness](https://github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustness) | [jxzhangjhu](https://github.com/jxzhangjhu) |  |
